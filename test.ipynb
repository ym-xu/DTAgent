{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5297acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ec12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/layout.json', 'r', encoding='utf-8') as f:\n",
    "    layout = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0eb31f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/87a24eb7-d010-4cc6-80a7-fb87a2ce7dff_content_list.json', 'r', encoding='utf-8') as f:\n",
    "    content_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c0b7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = []\n",
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        content_img.append(i['img_path'].split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b88bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/layout.json', 'r', encoding='utf-8') as f:\n",
    "    layout = json.load(f)\n",
    "\n",
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/87a24eb7-d010-4cc6-80a7-fb87a2ce7dff_content_list.json', 'r', encoding='utf-8') as f:\n",
    "    content_list = json.load(f)\n",
    "\n",
    "layout_info = dict()\n",
    "def recursive_search(obj, path=\"\"):\n",
    "    \"\"\"Recursively search for image_path keys in nested structures\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            current_path = f\"{path}.{key}\" if path else key\n",
    "            if key == \"image_path\" and isinstance(value, str):\n",
    "                layout_info[value] = obj[\"bbox\"]\n",
    "                # print(f\"Found image_path at {current_path}: {value}\")\n",
    "            else:\n",
    "                recursive_search(value, current_path)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            current_path = f\"{path}[{i}]\"\n",
    "            recursive_search(item, current_path)\n",
    "\n",
    "recursive_search(layout['pdf_info'])\n",
    "\n",
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        i[\"outline\"] = layout_info[i['img_path'].split('/')[1]]\n",
    "len(layout_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65400ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        i[\"outline\"] = layout_info[i['img_path'].split('/')[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2efa575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'MMDetectionRelease 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 0},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection Authors',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 0},\n",
       " {'type': 'text', 'text': '', 'page_idx': 1},\n",
       " {'type': 'text', 'text': '1 Prerequisites 1', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text', 'text': '2 Installation 3', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '2 Installation 32.1 Prepare environment 32.2 Install MMDetection 32.3 Install without GPU support 52.4 Another option: Docker Image 52.5 A from- scratch setup script 52.6 Developing with multiple MMDetection versions 6',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text', 'text': '3 Verification 7', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '4 Benchmark and Model Zoo 9',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '4 Benchmark and Model Zoo 94.1 Mirror sites 94.2 Common settings 94.3 ImageNet Pretrained Models 94.4 Baselines 104.5 Speed benchmark 154.6 Comparison with Detector2 16',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '5 1: Inference and train with existing models and standard datasets 17',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '5.1 Inference with existing models 175.2 Test existing models on standard datasets 205.3 Train predefined models on standard datasets 26',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '6 2: Train with customized datasets 29',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '6.1 Prepare the customized dataset 296.2 Prepare a config 336.3 Train a new model 346.4 Test and inference 34',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '7 3: Train with customized models and standard datasets 35',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '7.1 Prepare the standard dataset 357.2 Prepare your own customized model 367.3 Prepare a config 377.4 Train a new model 407.5 Test and inference 40',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '8 Tutorial 1: Learn about Configs 41',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '8.1 Modify config through script arguments 418.2 Config File Structure 41',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '8.3 Config Name Style 42  8.4 Deprecated train_cfg/test_cfg 42  8.5 An Example of Mask R- CNN 43  8.6 FAQ 51',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '9 Tutorial 2: Customize Datasets 55',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '9.1 Support new data format 55  9.2 Customize datasets by dataset wrappers 60  9.3 Modify Dataset Classes 63  9.4 COCO Panoptic Dataset 64',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '10 Tutorial 3: Customize Data Pipelines 67',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '10.1 Design of Data pipelines 67  10.2 Extend and use custom pipelines 70',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '11 Tutorial 4: Customize Models 71',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text', 'text': '11.1 Develop new components 71', 'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '12 Tutorial 5: Customize Runtime Settings 79',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '12.1 Customize optimization settings 79  12.2 Customize training schedules 81  12.3 Customize workflow 82  12.4 Customize hooks 82',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '13 Tutorial 6: Customize Losses 87',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '13.1 Computation pipeline of a loss 87  13.2 Set sampling method (step 1) 87  13.3 Tweaking loss 88  13.4 Weighting loss (step 3) 89',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '14 Tutorial 7: Finetuning Models 91',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '14.1 Inherit base configs 91  14.2 Modify head 91  14.3 Modify dataset 92  14.4 Modify training schedule 92  14.5 Use pre- trained model 93',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '15 Tutorial 8: Pytorch to ONNX (Experimental) 95',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '15.1 How to convert models from Pytorch to ONNX 95  15.2 How to evaluate the exported models 97  15.3 List of supported models exportable to ONNX 98  15.4 The Parameters of Non- Maximum Suppression in ONNX Export 99  15.5 Reminders 99  15.6 FAQs 99',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '16 Tutorial 9: ONNX to TensorRT (Experimental) 101',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '16.1 How to convert models from ONNX to TensorRT 101  16.2 How to evaluate the exported models 102  16.3 List of supported models convertible to TensorRT 103  16.4 Reminders 103  16.5 FAQs 103',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '17 Tutorial 10: Weight initialization 105',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '17.1 Description 105  17.2 Initialize parameters 105',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text', 'text': '17.3 Usage of init_cfg 107', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '18 Log Analysis 109', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '19 Result Analysis 111', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '20 Visualization 113', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '20.1 Visualize Datasets 113', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '20.2 Visualize Models 113  20.3 Visualize Predictions 113',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '21 Error Analysis 115', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '22 Model Serving 117', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '22.1 1. Convert model from MMDetection to TorchServe 117',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '22.2 2. Build mmdet- serve docker image 117',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '22.3 3. Run mmdet- serve 117', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '22.4 4. Test deployment 118', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '23 Model Complexity 121', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '24 Model conversion 123', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.1 MMDetection model to ONNX (experimental) 123',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.2 MMDetection 1. x model to MMDetection 2. x 123',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.3 RegNet model to MMDetection 123',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.4 Detectron ResNet to Pytorch 124',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.5 Prepare a model for publishing 124',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '25 Dataset Conversion 125', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '26 Benchmark 127', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '26.1 Robust Detection Benchmark 127',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '26.2 FPS Benchmark 127', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '27 Miscellaneous 129', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '27.1 Evaluating a metric 129  27.2 Print the entire config 129',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '28 Hyper- parameter Optimization 131',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '28.1 YOLO Anchor Optimization 131', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29 Conventions 133', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29.1 Loss 133', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29.2 Empty Proposals 133', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29.3 Coco Panoptic Dataset 134', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '30 Compatibility of MMDetection 2. x 135',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.1 MMDetection 2.18.0 135', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.2 MMDetection 2.14.0 135', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.3 MMDetection 2.12.0 135', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '30.4 Compatibility with MMDetection 1. x 136',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.5 pycocotools compatibility 138', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '31 Projects based on MMDetection 139',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '31.1 Projects as an extension 139', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '31.2 Projects of papers 139', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '32 Changelog 141', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '32.1 v2.18.0 (27/10/2021) 141 32.2 v2.17.0 (28/9/2021) 142 32.3 v2.16.0 (30/8/2021) 144 32.4 v2.15.1 (11/8/2021) 145 32.5 v2.15.0 (02/8/2021) 146 32.6 v2.14.0 (29/6/2021) 147 32.7 v2.13.0 (01/6/2021) 148 32.8 v2.12.0 (01/5/2021) 150 32.9 v2.11.0 (01/4/2021) 151 32.10 v2.10.0 (01/03/2021) 152 32.11 v2.9.0 (01/02/2021) 153 32.12 v2.8.0 (04/01/2021) 154 32.13 v2.7.0 (30/11/2020) 156 32.14 v2.6.0 (1/11/2020) 157 32.15 v2.5.0 (5/10/2020) 158 32.16 v2.4.0 (5/9/2020) 159 32.17 v2.3.0 (5/8/2020) 161 32.18 v2.2.0 (1/7/2020) 162 32.19 v2.1.0 (8/6/2020) 163 32.20 v2.0.0 (6/5/2020) 165 32.21 v1.1.0 (24/2/2020) 166 32.22 v1.0.0 (30/1/2020) 167 32.23 v1.0rc1 (13/12/2019) 168 32.24 v1.0rc0 (27/07/2019) 171 32.25 v0.6.0 (14/04/2019) 171 32.26 v0.6rc0(06/02/2019) 171 32.27 v0.5.7 (06/02/2019) 171 32.28 v0.5.6 (17/01/2019) 171 32.29 v0.5.5 (22/12/2018) 171 32.30 v0.5.4 (27/11/2018) 172 32.31 v0.5.3 (26/11/2018) 172 32.32 v0.5.2 (21/10/2018) 172 32.33 v0.5.1 (20/10/2018) 172',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '33 Frequently Asked Questions 173',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '33.1 MMCV Installation 173 33.2 PyTorch/CUDA Environment 173 33.3 Training 174 33.4 Evaluation 175',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': '34 English 177', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text', 'text': '35 179', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text', 'text': '36 mmdet.apis 181', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text', 'text': '37 mmdet.core 183', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '37.1 anchor 183 37.2 bbox 191 37.3 export 208 37.4 mask 211 37.5 evaluation 219 37.6 post_processing 221 37.7 utils 224',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': '38 mmdet.datasets 227', 'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': '38.1 datasets 227  38.2 pipelines 239  38.3 samplers 256  38.4 api_wrappers 257',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text', 'text': '39 mmdet.models 259', 'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': '39.1 detectors 259  39.2 backbones 273  39.3 necks 291  39.4 dense-heads 301  39.5 roi-heads 383  39.6 losses 412  39.7 utils 425',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text', 'text': '40 mmdet.utils 437', 'page_idx': 6},\n",
       " {'type': 'text', 'text': '41 Indices and tables 439', 'page_idx': 6},\n",
       " {'type': 'text', 'text': 'Python Module Index 441', 'page_idx': 6},\n",
       " {'type': 'text', 'text': 'Index 443', 'page_idx': 6},\n",
       " {'type': 'text', 'text': 'vi', 'page_idx': 7},\n",
       " {'type': 'text', 'text': 'PREREQUISITES', 'text_level': 1, 'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': '- Linux or macOS (Windows is in experimental support)- Python 3.6+- PyTorch 1.3+- CUDA 9.2+ (If you build PyTorch from source, CUDA 9.0 is also compatible)- GCC 5+- MMCV',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Compatible MMDetection and MMCV versions are shown as below. Please install the correct version of MMCV to avoid installation issues.',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: You need to run pip uninstall mmcv first if you have mmcv installed. If mmcv and mmcv- full are both installed, there will be ModuleNotFoundError.',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 9},\n",
       " {'type': 'text', 'text': 'INSTALLATION', 'text_level': 1, 'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '2.1 Prepare environment',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '1. Create a conda virtual environment and activate it.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda create - n openmmlab python  $= 3.7$  - y conda activate openmmlab',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '2. Install PyTorch and torchvision following the official instructions, e.g.,',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda install pytorch torchvision - c pytorch',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: Make sure that your compilation CUDA version and runtime CUDA version match. You can check the supported CUDA version for precompiled packages on the PyTorch website.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'E.g. 1 If you have CUDA 10.1 installed under /usr/local/cuda and would like to install PyTorch 1.5, you need to install the prebuilt PyTorch with CUDA 10.1.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda install pytorch cudatoolkit  $= 10.1$  torchvision - c pytorch',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'E.g. 2 If you have CUDA 9.2 installed under /usr/local/cuda and would like to install PyTorch 1.3.1. you need to install the prebuilt PyTorch with CUDA 9.2.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda install pytorch  $= 1.3.1$  cudatoolkit  $= 9.2$  torchvision  $= 0.4.2$  - c pytorch',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'If you build PyTorch from source instead of installing the prebuilt package, you can use more CUDA versions such as 9.0.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '2.2 Install MMDetection',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'It is recommended to install MMDetection with MM, which automatically handles the dependencies of OpenMMLab projects, including mmcv and other python packages.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install openmim mim install mmdet',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Or you can still install MMDetection manually:',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '1. Install mmcv-full.```pythonpip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/{cu_version}/ -ftorch_version}/index.html```(continues on next page)',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'Please replace {cu_version} and {torch_version} in the url to your desired one. For example, to install the latest mmcv- full with CUDA 11.0 and PyTorch 1.7.0, use the following command:',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install mmcv- full - f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/  $\\\\rightarrow$  index.html',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'See here for different versions of MMCV compatible to different PyTorch and CUDA versions.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'Optionally you can compile mmcv from source if you need to develop both mmcv and mmdet. Refer to the guide for details.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': '2. Install MMDetection.', 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'You can simply install mmdetection with the following command:',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': 'pip install mmdet', 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'or clone the repository and then install it:',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'git clone https://github.com/open- mmlab/mmdetection.git cd mmdetection pip install - r requirements/build.txt pip install - v - e . # or \"python setup.py develop\"',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': '3. Install extra dependencies for Instaboost, Panoptic Segmentation, LVIS dataset, or Albumentations.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': '```python# for instaboostpip install instaboostfast# for panoptic segmentationpip install git+https://github.com/cocodataset/panopticapi.git# for LVIS datasetpip install git+https://github.com/lvis- dataset/lvis- api.git# for albumentationspip install albumentations>=0.3.2 - - no- binary imgaug,albumentations```',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'a. When specifying -e or develop, MMDetection is installed on dev mode, any local modifications made to the code will take effect without reinstallation.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'b. If you would like to use opencv-python-headless instead of opencv-python, you can install it before installing MMCV.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'c. Some dependencies are optional. Simply running pip install -v -e . will only install the minimum runtime requirements. To use optional dependencies like albumentations and imagecorruptions either install them manually with pip install -r requirements/optional.txt or specify desired extras when calling pip (e.g. pip install -v -e . [optional]). Valid keys for the extras field are: all, tests, build, and optional.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'd. If you would like to use albumentations, we suggest using pip install albumentations>=0.3.2 - - no-binary imgaug,albumentations. If you simply use pip install albumentations>=0.3.2, it will install opencv-python-headless simultaneously (even though you have already installed opencv-python). We should not allow opencv-python and opencv-python-headless installed at the same time, because it might cause unexpected issues. Please refer to official documentation for more details.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': '2.3 Install without GPU support',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': \"MMDetection can be built for CPU only environment (where CUDA isn't available).\",\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'In CPU mode you can run the demo/webcam_demo.py for example. However some functionality is gone in this mode:',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Deformable Convolution Modulated Deformable Convolution ROI pooling Deformable ROI pooling CARAFE: Content- Aware ReAssembly of FFeatures SyncBatchNorm CrissCrossAttention: Criss- Cross Attention MaskedConv2d Temporal Interlace Shift nms_cuda sigmoid_focal_loss_cuda bbox_overlapss',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'If you try to run inference with a model containing above ops, an error will be raised. The following table lists affected algorithms.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Notice: MMDetection does not support training with CPU for now.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': '2.4 Another option: Docker Image',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide a Dockerfile to build an image. Ensure that you are using docker version  $> = 19.03$',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'build an image with PyTorch 1.6, CUDA 10.1 docker build - t mmdetection docker/',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text', 'text': 'Run it with', 'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'docker run - - gpus all - - shm- size  $= 8g$  - it - v {DATA_DIR}: /mmdetection/data mmdetection',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': '2.5 A from-scratch setup script',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming that you already have CUDA 10.1 installed, here is a full script for setting up MMDetection with conda.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'conda create - n openmmlab python  $= 3$  .7 - y conda activate openmmlab conda install pytorch  $= = 1$  .6.0 torchvision  $= = 0$  .7.0 cudatoolkit  $= 10$  .1 - c pytorch - y # install the latest mmcv pip install mmcv- full - f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index. html (continues on next page)',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'install mmdetection git clone https://github.com/open- mmlab/mmdetection.git cd mmdetection pip install - r requirements/build.txt pip install - v - e.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': '2.6 Developing with multiple MMDetection versions',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'The train and test scripts already modify the PYTHONPATH to ensure the script use the MMDetection in the current directory.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'To use the default MMDetection installed in the environment rather than that you are working with, you can remove the following line in those scripts',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text', 'text': 'VERIFICATION', 'text_level': 1, 'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': 'To verify whether MMDetection is installed correctly, we can run the following sample code to initialize a detector and inference a demo image.',\n",
       "  'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': \"from mmdet.apis import init_detector, inference_detector config_file  $=$  'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' # download the checkpoint from model zoo and put it in checkpoints/ # url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_  $\\\\leftrightarrow$  1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth checkpoint_file  $=$  'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth' device  $=$  'cuda:0' # init a detector model  $=$  initDetector(config_file, checkpoint_file, device  $\\\\coloneqq$  device) # inference the demo image inference_detector(model, 'demo/demo.jpg')\",\n",
       "  'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': 'The above code is supposed to run successfully upon you finish the installation.',\n",
       "  'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': 'BENCHMARK AND MODEL ZOO',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text', 'text': '4.1 Mirror sites', 'text_level': 1, 'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'We only use aiyun to maintain the model zoo since MMDetection V2.0. The model zoo of V1. x has been deprecated.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '4.2 Common settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '- All models were trained on coco_2017_train, and tested on the coco_2017_val.- We use distributed training.- All pytorch-style pretrained backbones on ImageNet are from PyTorch model zoo, caffe-style pretrained backbones are converted from the newly released model from detectron2.- For fair comparison with other codebases, we report the GPU memory as the maximum value of torch.cuda.max_memory_allocated() for all 8 GPUs. Note that this value is usually less than what nvidia-smi shows.- We report the inference time as the total time of network forwarding and post-processing, excluding the data loading time. Results are obtained with the script benchmark.py which computes the average time on 2000 images.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '4.3 ImageNet Pretrained Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'It is common to initialize from backbone models pre- trained on ImageNet classification task. All pre- trained model links can be found at open_mmlab. According to img_norm_cfg and source of weight, we can divide all the ImageNet pre- trained model weights into some cases:',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '- TorchVision: Corresponding to torchvision weight, including ResNet50, ResNet101. The img_norm_cfg is dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True).- Pycis: Corresponding to pycis weight, including RegNetX. The img_norm_cfg is dict(mean=[103.530, 116.280, 123.675], std=[57.375, 57.12, 58.395], to_rgb=False).- MSRA styles: Corresponding to MSRA weights, including ResNet50_Caffe and ResNet101_Caffe. The img_norm_cfg is dict(mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False).- Caffe2 styles: Currently only contains ResNext101_32x8d. The img_norm_cfg is dict(mean=[103.530, 116.280, 123.675], std=[57.375, 57.120, 58.395], to_rgb=False).',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Other styles: E.g SSD which corresponds to img_norm_cfg is dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True) and YOLOv3 which corresponds to img_norm_cfg is dict(mean=[0, 0, 0], std=[255. , 255. , 255. ], to_rgb=True).',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'The detailed table of the commonly used backbone models in MMDetection is listed below :',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4 Baselines', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.1 RPN', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text', 'text': 'Please refer to RPN for details.', 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.2 Faster R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Faster R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.3 Mask R-CNN', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Mask R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.4 Fast R-CNN (with pre-computed proposals)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Fast R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.5 RetinaNet', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to RetinaNet for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.6 Cascade R-CNN and Cascade Mask R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Cascade R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.7 Hybrid Task Cascade (HTC)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': 'Please refer to HTC for details.', 'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.8 SSD', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text', 'text': 'Please refer to SSD for details.', 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.9 Group Normalization (GN)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Group Normalization for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.10 Weight Standardization',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Weight Standardization for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.11 Deformable Convolution v2',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Deformable Convolutional Networks for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.12 CARAFE: Content-Aware ReAssembly of FEatures',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CARAFE for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.13 Instaboost',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Instaboost for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.14 Libra R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Libra R- CNN for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.15 Guided Anchoring',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Guided Anchoring for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text', 'text': '4.4.16 FCOS', 'text_level': 1, 'page_idx': 18},\n",
       " {'type': 'text', 'text': 'Please refer to FCOS for details.', 'page_idx': 18},\n",
       " {'type': 'text', 'text': '4.4.17 FoveaBox', 'text_level': 1, 'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to FoveaBox for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text', 'text': '4.4.18 RepPoints', 'text_level': 1, 'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to RepPoints for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.19 FreeAnchor',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to FreeAnchor for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.20 Grid R-CNN (plus)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Grid R- CNN for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.21 GHM', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text', 'text': 'Please refer to GHM for details.', 'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.22 GCNet', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to GCNet for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.23 HRNet', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to HRNet for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.24 Mask Scoring R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Mask Scoring R- CNN for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.25 Train from Scratch',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Rethinking ImageNet Pre- training for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.26 NAS-FPN', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to NAS- FPN for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.27 ATSS', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text', 'text': 'Please refer to ATSS for details.', 'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.28 FSAF', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text', 'text': 'Please refer to FSAF for details.', 'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.29 RegNetX', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to RegNet for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.30 Res2Net', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Res2Net for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.31 GRoIE', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to GRoIE for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.32 Dynamic R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Dynamic R- CNN for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.33 PointRend', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to PointRend for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.34 DetectorRS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to DetectorRS for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.35 Generalized Focal Loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Generalized Focal Loss for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.36 CornerNet', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CornerNet for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.37 YOLOv3', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to YOLOv3 for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.38 PAA', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text', 'text': 'Please refer to PAA for details.', 'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.39 SABL', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text', 'text': 'Please refer to SABL for details.', 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.40 CentripetalNet',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CentripetalNet for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.41 ResNeSt', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to ResNeSt for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.42 DETR', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text', 'text': 'Please refer to DETR for details.', 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.43 Deformable DETR',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Deformable DETR for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.44 AutoAssign',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to AutoAssign for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.45 YOLOF', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to YOLOF for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.46 Seesaw Loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Seesaw Loss for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.47 CenterNet', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CenterNet for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.48 YOLOX', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to YOLOX for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.49 PVT', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text', 'text': 'Please refer to PVT for details.', 'page_idx': 22},\n",
       " {'type': 'text', 'text': '4.4.50 SOLO', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text', 'text': 'Please refer to SOLO for details.', 'page_idx': 22},\n",
       " {'type': 'text', 'text': '4.4.51 QueryInst', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to QueryInst for details.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.52 Other datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We also benchmark some methods on PASCAL VOC, Cityscapes and WIDER FACE.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.53 Pre-trained Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.53 Pre- trained ModelsWe also train Faster R- CNN and Mask R- CNN using ResNet- 50 and RegNetX- 3.2G with multi- scale training and longer schedules. These models serve as strong pre- trained models for downstream tasks for convenience.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.5 Speed benchmark',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.5.1 Training Speed benchmark',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide analyze_logs.py to get average time of iteration in training. You can find examples in Log Analysis.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We compare the training speed of Mask R- CNN with some other popular frameworks (The data is copied from detectron2). For mmdetection, we benchmark with mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1. py, which should have the same setting with mask_rcnn_R_50_FPN_noaug_1x.yaml of detectron2. We also provide the checkpoint and training log for reference. The throughput is computed as the average throughput in iterations 100- 500 to skip GPU warmup time.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.5.2 Inference Speed Benchmark',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide benchmark.py to benchmark the inference latency. The script benchmarks the model with 2000 images and calculates the average time ignoring first 5 times. You can change the output log interval (defaults: 50) by setting LOG- INTERVAL.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'python toools/benchmark.py \\\\ ${CONFIG}$ {CHECKPOINT}[- - log- interval \\\\$[LOG- INTERVAL]] [- - fuse- conv- bn]',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'The latency of all models in our model zoo is benchmarked without setting fuse- conv- bn, you can get a lower latency by setting it.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.6 Comparison with Detector2',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': 'We compare mmdetection with Detector2 in terms of speed and performance. We use the commit id 185c27e(30/4/2020) of detectron. For fair comparison, we install and run both frameworks on the same machine.',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text', 'text': '4.6.1 Hardware', 'text_level': 1, 'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '- 8 NVIDIA Tesla V100 (32G) GPUs- Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.2 Software environment',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '- Python 3.7- PyTorch 1.4- CUDA 10.1- CUDNN 7.6.03- NCCL 2.4.08',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.3 Performance',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.4 Training Speed',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': 'The training speed is measured with s/iter. The lower, the better.',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.5 Inference Speed',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': 'The inference speed is measured with fps (img/s) on a single GPU, the higher, the better. To be consistent with Detectron2, we report the pure inference speed (without the time of data loading). For Mask R- CNN, we exclude the time of RLE encoding in post- processing. We also include the officially reported speed in the parentheses, which is slightly higher than the results tested on our server due to differences of hardwares.',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.6 Training memory',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '1: INFERENCE AND TRAIN WITH EXISTING MODELS AND STANDARD DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection provides hundreds of existing and existing detection models in Model Zoo), and supports multiple standard datasets, including Pascal VOC, COCO, CityScapes, LVIS, etc. This note will show how to perform common tasks on these existing models and standard datasets, including:',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'Use existing models to inference on given images. Test existing models on standard datasets. Train predefined models on standard datasets.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': '5.1 Inference with existing models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'By inference, we mean using trained models to detect objects on images. In MMDetection, a model is defined by a configuration file and existing model parameters are save in a checkpoint file.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'To start with, we recommend Faster RCNN with this configuration file and this checkpoint file. It is recommended to download the checkpoint file to checkpoints directory.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': '5.1.1 High-level APIs for inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection provide high- level Python APIs for inference on images. Here is an example of building the model and inference on given images or videos.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': \"from mmdet.apis import initDetector, inferenceDetector import mmcv # Specify the path to model config and checkpoint file config_file  $=$  'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' checkpoint_file  $=$  'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth' # build the model from a config file and a checkpoint file model  $=$  initDetector(config_file, checkpoint_file, device  $=$  cuda:0') # test a single image and show the results img  $=$  'test.jpg' # or img  $=$  mmcv.imread(img), which will only load it once result  $=$  inferenceDetector(model, img) # visualize the results in a new window model.show_result(img, result)\",\n",
       "  'page_idx': 24},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': \"or save the visualization results to image files model.show_result(img, result, out_file  $\\\\equiv$  'result.jpg') # test a video and show the results video  $=$  mmcv.VideoReader('video.mp4') for frame in video: result  $=$  inference_detector(model, frame) model.show_result(frame, result, wait_time  $= 1$\",\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'A notebook demo can be found in demo/inference_demo.ipynb.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: inference_detector only supports single- image inference for now.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': '5.1.2 Asynchronous interface - supported for Python 3.7+',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'For Python  $3.7+$  , MMDetection also supports async interfaces. By utilizing CUDA streams, it allows not to block CPU on GPU bound inference code and enables better CPU/GPU utilization for single- threaded application. Inference can be done concurrently either between different input data samples or between different models of some inference pipeline.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'See tests/async_benchmark.py to compare the speed of synchronous and asynchronous interfaces.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': \"import asyncio import torch from mmdet.apis import init_detector, async_inference_detector from mmdet.utils.contextmanagers import concurrent async def main(): config_file  $=$  'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' checkpoint_file  $=$  'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth' device  $=$  'cuda:0' model  $=$  initDetector(config_file, checkpoint  $=$  checkpoint_file, device  $=$  device) # queue is used for concurrent inference of multiple images streamqueue  $=$  asyncio.Queue() # queue size defines concurrency level streamqueue_size  $= 3$  for _ in range(streamqueue_size): streamqueue.put_nowait(torch.cuda.Stream(device  $=$  device)) # test a single image and show the results img  $=$  'test.jpg' # or img  $=$  mmcv.inread(img), which will only load it once async with concurrent(streamqueue): result  $=$  await async_inference_detector(model, img) # visualize the results in a new window model.show_result(img, result) # or save the visualization results to image files model.show_result(img, result, out_file  $=$  result.jpg')\",\n",
       "  'page_idx': 25},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 26},\n",
       " {'type': 'text', 'text': '5.1.3 Demos', 'text_level': 1, 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'We also provide three demo scripts, implemented with high- level APIs and supporting functionality codes. Source codes are available here.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Image demo', 'text_level': 1, 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'This script performs inference on a single image.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/e6d0b39ec58bce120f37a9425a06f8f03ed952d0e569c7fea4d33e88de3cdc1f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>python demo/image_demo.py</td><td>\\\\</td></tr><tr><td>$ {IMAGE_FILE} \\\\</td><td></td></tr><tr><td>$ {CONFIG_FILE} \\\\</td><td></td></tr><tr><td>$ {CHECKPOINT_FILE} \\\\</td><td></td></tr><tr><td>[--device]{GPU_ID} \\\\</td><td></td></tr><tr><td>[--score-thr]{SCORE_THR} ]</td><td></td></tr></table>\\n\\n\\n</figure>',\n",
       "  'page_idx': 26,\n",
       "  'outline': [69, 250, 543, 328]},\n",
       " {'type': 'text', 'text': 'Examples:', 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/image_demo.py demo/demo.jpg \\\\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\ - - device cpu',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Webcam demo', 'text_level': 1, 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'This is a live demo from a webcam.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/webcam_demo.py \\\\ \\\\\\\\({CONFIG_FILE} \\\\ \\\\\\\\){CHECKPOINT_FILE} \\\\ \\\\\\\\({GPU_ID} \\\\ \\\\\\\\({camera - id}\\\\){CAMERA- ID} \\\\ \\\\\\\\({score - thr}\\\\){SCORE_THR} ]',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Examples:', 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/webcam_demo.py \\\\ \\\\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Video demo', 'text_level': 1, 'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'This script performs inference on a video.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/video_demo.py \\\\\\\\\\\\({VIDEO_FILE}\\\\)CONFIG_FILE\\\\({CHECKPOINT_FILE}\\\\)- - device}\\\\({GPU_ID}\\\\)\\\\[-- score - th \\\\){SCORE THR}\\\\[-- out \\\\){OUT_FILE}\\\\]- - show\\\\[- - wait - time \\\\){WAIT_TIME}',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text', 'text': 'Examples:', 'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/video_demo.py demo/demo.mp4 \\\\configigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\- - out result.mp4',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': '5.2 Test existing models on standard datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': \"To evaluate a model's accuracy, one usually tests the model on some standard datasets. MMDetection supports multiple public datasets including COCO, Pascal VOC, CityScapes, and more. This section will show how to test existing models on supported datasets.\",\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.1 Prepare datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'Public datasets like Pascal VOC or mirror and COCO are available from official websites or mirrors. Note: In the detection task, Pascal VOC 2012 is an extension of Pascal VOC 2007 without overlap, and we usually use them together. It is recommended to download and extract the dataset somewhere outside the project directory and symlink the dataset root to  $\\\\mathfrak{S}$  MMDETECTION/data as below. If your folder structure is different, you may need to change the corresponding paths in config files.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/ad9c1df455c01d871b0a3e0d05a7a0ad0316692add45118b9141403152b3e279.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 27,\n",
       "  'outline': [67, 514, 543, 712]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 27},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/70ba71c7b94a8656df7fbdd69e7f51dbe6061d2218b0f6ab7349d2ef5802c46b.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 84, 543, 150]},\n",
       " {'type': 'text',\n",
       "  'text': 'Some models require additional COCO- stuff datasets, such as HTC, DetectoRS and SCNet, you can download and unzip then move to the coco folder. The directory should be like this.',\n",
       "  'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/015a6e28a7221a9e77123ee0fa0399f958bf04d0bf3c128b0fcaa48b23784458.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 190, 543, 293]},\n",
       " {'type': 'text',\n",
       "  'text': 'Panoptic segmentation models like PanopticFPN require additional COCO Panoptic datasets, you can download and unzip then move to the coco annotation folder. The directory should be like this.',\n",
       "  'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/7c29cab430344af1ab97e97bafaec3cb824e4b3b4dd06fb19c7cc3e4952da12e.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 330, 543, 468]},\n",
       " {'type': 'text',\n",
       "  'text': 'The cityscapes annotations need to be converted into the coco format using tools/dataset_converters/cityscapes.py:',\n",
       "  'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/70efa7e3b477764b3373f407470301859325b127d4d20163f978002d2faf2e51.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 507, 543, 586]},\n",
       " {'type': 'text', 'text': 'TODO: CHANGE TO THE NEW PATH', 'page_idx': 28},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.2 Test existing models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide testing scripts for evaluating an existing model on the whole dataset (COCO, PASCAL VOC, Cityscapes, etc.). The following testing environments are supported:',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'single GPU single node multiple GPUs multiple nodes',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Choose the proper script to perform testing depending on the testing environment.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'single- gpu testing python tools/test.py \\\\ \\\\\\\\({CONFIG_FILE} \\\\ \\\\\\\\){CHECKPOINT_FILE} \\\\ [--out \\\\\\\\({RESULT_FILE} \\\\ [--eval \\\\\\\\){EVAL_METRICS}} \\\\ [--show] # multi- gpu testing bash tools/dist_test.sh \\\\ \\\\\\\\({CONFIG_FILE} \\\\ \\\\\\\\){CHECKPOINT_FILE} \\\\ \\\\\\\\({GPU_NUM} \\\\ [--out \\\\\\\\){RESULT_FILE} \\\\ [--eval \\\\\\\\({EVAL_METRICS}}',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"tools/dist_test. sh also supports multi- node testing, but relies on PyTorch's launch utility.\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text', 'text': 'Optional arguments:', 'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"- RESULT_FILE: Filename of the output results in pickle format. If not specified, the results will not be saved to a file.- EVAL_METRICS: Items to be evaluated on the results. Allowed values depend on the dataset, e.g., proposal_fast, proposal, bbox, segm are available for COCO, mAP, recall for PASCAL VOC. Cityscapes could be evaluated by cityscapes as well as all COCO metrics.- --show: If specified, detection results will be plotted on the images and shown in a new window. It is only applicable to single GPU testing and used for debugging and visualization. Please make sure that GUI is available in your environment. Otherwise, you may encounter an error like cannot connect to X server.- --show-dir: If specified, detection results will be plotted on the images and saved to the specified directory. It is only applicable to single GPU testing and used for debugging and visualization. You do NOT need a GUI available in your environment for using this option.- --show-score-thr: If specified, detections with scores below this threshold will be removed.- --cfg-options: if specified, the key-value pair optional cfg will be merged into config file.- --eval-options: if specified, the key-value pair optional eval cfg will be kwargs for dataset.evaluate() function, it's only for evaluation\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text', 'text': '5.2.3 Examples', 'text_level': 1, 'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming that you have already downloaded the checkpoints to the directory checkpoints/.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '1. Test Faster R-CNN and visualize the results. Press any key for the next image. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py \\\\ config/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\ - - show',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '2. Test Faster R-CNN and save the painted images for future visualization. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py \\\\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\ - - show- dir faster_rcnn_r50_fpn_1x_recults',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '3. Test Faster R-CNN on PASCAL VOC (without saving the test results) and evaluate the mAP. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py \\\\ configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_voc0712_20200624- c9895d40. pth \\\\ - - eval mAP',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '4. Test Mask R-CNN with 8 GPUs, and evaluate the bbox and mask AP. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\ 8 \\\\ - - out results.pkl \\\\ - - eval bbox segm',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '5. Test Mask R-CNN with 8 GPUs, and evaluate the classwise bbox and mask AP. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\ 8 \\\\ - - out results.pkl \\\\ - - eval bbox segm \\\\ - - options \"classwise=True\"',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '6. Test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files for submitting to the official evaluation server. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '5.2. Test existing models on standard datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 30},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/a94edb4aadcf4812cfc1d2e104fe1a71a02d9b044499e6dc4b03e41947c7ba50.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>8 \\\\</td></tr><tr><td>--format-only\\\\</td></tr><tr><td>--options &quot;jsonfile_prefix=./mask_rcnn_test-dev_results&quot;</td></tr></table>',\n",
       "  'page_idx': 31,\n",
       "  'outline': [91, 84, 542, 124]},\n",
       " {'type': 'text',\n",
       "  'text': 'This command generates two JSON files mask_rcnn_test- dev_results. bbox.json and mask_rcnn_test- dev_results.segm.json.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': '7. Test Mask R-CNN on Cityscapes test with 8 GPUs, and generate txt and png files for submitting to the official evaluation server. Config and checkpoint files are available here.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227- afe51d5a.pth \\\\ 8 \\\\ - - format- only \\\\ - - options \"txtfile_prefix  $=$  ./mask_rcnn_cityscapes_test_results\"',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'The generated png and txt would be under ./mask_rcnn_cityscapes_test_results directory.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.4 Test without Ground Truth Annotations',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection supports to test models without ground- truth annotations using CocoDataset. If your dataset format is not in COCO format, please convert them to COCO format. For example, if your dataset format is VOC, you can directly convert it to COCO format by the script in tools. If your dataset format is Cityscapes, you can directly convert it to COCO format by the script in tools. The rest of the formats can be converted using this script.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/dataset_converters/images2coco.py \\\\ \\\\\\\\({IMG_PATH} \\\\ \\\\\\\\){CLASSES} \\\\ \\\\\\\\({OUT} \\\\ [ - - exclude - extensions]',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text', 'text': 'arguments', 'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': \"- IMG_PATH: The root path of images.- CLASSES: The text file with a list of categories.- OUT: The output annotation json file name. The save dir is in the same directory as IMG_PATH.- exclude-extensions: The suffix of images to be excluded, such as 'png' and 'bmp'.\",\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'After the conversion is complete, you can use the following command to test',\n",
       "  'page_idx': 31},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/fcc3da535e6975e30933ade2d428ca2f746e575fb8375fa3b7d82972ea7d046f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td># single-gpu testing\\npython tools/test.py \\\\ \\n{{CONFIG_FILE} \\\\ \\n{{CHECKPOINT_FILE} \\\\ \\n--format-only \\\\ \\n--options {{JSONFILE_PREFIX} \\\\ \\n[--show]}\\n# multi-gpu testing\\nbash tools/dist_test.sh \\\\ \\n{{CONFIG_FILE} \\\\}</td></tr></table>',\n",
       "  'page_idx': 31,\n",
       "  'outline': [67, 576, 543, 714]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 31},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': '${\\\\mathcal{S}}\\\\left\\\\{\\\\begin{array}{rl}\\\\end{array}\\\\right.$  CHECKPOINT_FILE} \\\\${GPU_NUM} \\\\- - format- only \\\\- - options \\\\${JSONFILE_PREFIX} \\\\[-- show\\\\]',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming that the checkpoints in the model zoo have been downloaded to the directory checkpoints/, we can test Mask R- CNN on COCO test- dev with 8 GPUs, and generate JSON files using the following command.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ config /mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\ 8 \\\\ - format- only \\\\ - - options \"jsonfile_prefix=./mask_rcnn_test- dev_results\"',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'This command generates two JSON files mask_rcnn_test- dev_results.bbox.json and mask_rcnn_test- dev_results.segm.json.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.5 Batch Inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection supports inference with a single image or batched images in test mode. By default, we use single- image inference and you can use batch inference by modifying samples_per_gpu in the config of test data. You can do that either by modifying the config as below.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'data  $=$  dict(traitn=dict(...), val=dict(...), test=dict(samples_per_gpu=2, ...)',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Or you can set it through - - cfg- options as - - cfg- options data.test.samples_per_gpu=2',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.6 Deprecated ImageToTensor',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"In test mode, ImageToTensor pipeline is deprecated, it's replaced by DefaultFormatBundle that recommended to manually replace it in the test data pipeline in your config file. examples:\",\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"use ImageToTensor (deprecated) pipelines  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict( type  $\\\\coloneqq$  'MultiScaleFlipAug', img_scale  $=$  (1333, 800), flip  $\\\\coloneqq$  False, transforms  $\\\\coloneqq$  [ dict(type  $\\\\coloneqq$  'Resize', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip'), dict(type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [0, 0, 0], std=[1, 1, 1]), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  'ImageToTensor', keys=['img']), dict(type  $\\\\coloneqq$  'Collect', keys=['img'], ]) ]\",\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"manually replace ImageToTensor to DefaultFormatBundle (recommended) pipelines  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict type  $\\\\coloneqq$  'MultiScaleFlipAug', img_scale  $\\\\coloneqq$  (1333, 800), flip  $\\\\coloneqq$  False, transforms  $\\\\coloneqq$  [ dict(type  $\\\\coloneqq$  'Resize', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip'), dict(type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [0, 0, 0], std=[1, 1, 1]), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), dict(type  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img'], ] ]\",\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': '5.3 Train predefined models on standard datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection also provides out- of- the- box tools for training detection models. This section will show how to train predefined models (under configs) on standard datasets i.e. COCO.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Important: The default learning rate in config files is for 8 GPUs and 2 img/gpu (batch size  $= 8^{*}2 = 16$  . According to the linear scaling rule, you need to set the learning rate proportional to the batch size if you use different GPUs or images per GPU, e.g.,  $\\\\mathbf{lr} = \\\\emptyset$  .01 for 4 GPUs \\\\* 2 imgs/gpu and  $\\\\mathbf{lr} = \\\\emptyset$  .08 for 16 GPUs \\\\* 4 imgs/gpu.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.1 Prepare datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Training requires preparing datasets too. See section Prepare datasets above for details.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: Currently, the config files under configs/cityscapes use COCO pretrained weights to initialize. You could download the existing models in advance if the network connection is unavailable or slow. Otherwise, it would cause errors at the beginning of training.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.2 Training on a single GPU',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide tools/train.py to launch training jobs on a single GPU. The basic usage is as follows.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/dbff7adc695fbd067625874a2b8fdfdf83cab8ef45283e3db0021466683fae7b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>python tools/train.py\\n  {{CONFIC_DIR:E} \\\\ \\n[optional arguments]}</td></tr></table>',\n",
       "  'page_idx': 33,\n",
       "  'outline': [69, 571, 543, 612]},\n",
       " {'type': 'text',\n",
       "  'text': 'During training, log files and checkpoints will be saved to the working directory, which is specified by work_dir in the config file or via CLI argument - - work- dir.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'By default, the model is evaluated on the validation set every epoch, the evaluation interval can be specified in the config file as shown below.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'evaluate the model every 12 epoch. evaluation  $=$  dict(interval  $= 12$',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'This tool accepts several optional arguments, including:',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': \"- --no-validate (not suggested): Disable evaluation during training.- --work-dir  ${{WORK_DIR}}$ : Override the working directory.- --resume-from  ${{CHECKPOINT_FILE}}$ : Resume from a previous checkpoint file.- --options 'Key-value': Overrides other settings in the used config.\",\n",
       "  'page_idx': 34},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Difference between resume- from and load- from:',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'resume- from loads both the model weights and optimizer status, and the epoch is also inherited from the specified checkpoint. It is usually used for resuming the training process that is interrupted accidentally. Load- from only loads the model weights and the training epoch starts from 0. It is usually used for finetuning.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.3 Training on multiple GPUs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide tools/dist_train.sh to launch training on multiple GPUs. The basic usage is as follows.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'bash ./tools/dist_train.sh  ${{CONFIG_FILE}}\\\\ \\\\backslash$ ${{GPU_NUM}}\\\\ \\\\backslash$  [optional arguments]',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Optional arguments remain the same as stated above.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Launch multiple jobs simultaneously',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'If you would like to launch multiple jobs on a single machine, e.g., 2 jobs of 4- GPU training on a machine with 8 GPUs, you need to specify different ports (29500 by default) for each job to avoid communication conflict.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'If you use dist_train.sh to launch training jobs, you can set the port in commands.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'CUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh  ${{CONFIG_FILE}}\\\\ 4$  CUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh  ${{CONFIG_FILE}}\\\\ 4$',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.4 Training on multiple nodes',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': \"MMDetection relies on torch. distributed package for distributed training. Thus, as a basic usage, one can launch distributed training via PyTorch's launch utility.\",\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.5 Manage jobs with Slurm',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Slurm is a good job scheduling system for computing clusters. On a cluster managed by Slurm, you can use slurm_train.sh to spawn training jobs. It supports both single- node and multi- node training.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text', 'text': 'The basic usage is as follows.', 'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '$[GPUS =$ \\\\{\\\\text{GPUS}\\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\}',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Below is an example of using 16 GPUs to train Mask R- CNN on a Slurm partition named dev, and set the work- dir to some shared file systems.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'GPUS=16 ./tools/slurm_train.sh dev mask_r50_ix configs/mask_rcnn_r50_fpn_1x_coco.py /nfs/  $\\\\rightarrow$  xxxx/mask_rcnn_r50_fpn_1x',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'You can check the source code to review full arguments and environment variables.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'When using Slurm, the port option need to be set in one of the following ways:',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '1. Set the port through - - options. This is more recommended since it does not change the original configs.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': \"CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh \\\\(\\\\)\\\\{PARTITION\\\\}\\\\(\\\\)\\\\{JOB_NAME\\\\}\\\\(\\\\)\\\\rightarrow\\\\(config1. py\\\\)\\\\{\\\\(WORK_DIR\\\\}\\\\(- - options 'dist_params.port\\\\)=29500\\\\(\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,5,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\(\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(5,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathcal{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\)\",\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '2. Modify the config files to set different communication ports.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': \"In config1. py, set dist_params = dict(backend='nccl', port=29500)\",\n",
       "  'page_idx': 35},\n",
       " {'type': 'text', 'text': '', 'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': \"In config2. py, set dist_params = dict(backend='nccl', port=29501)\",\n",
       "  'page_idx': 35},\n",
       " {'type': 'text', 'text': '', 'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Then you can launch two jobs with config1. py and config2. py.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh \\\\(\\\\)\\\\{PARTITION\\\\}\\\\(\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\ (5,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(5,6,7\\\\mathcal{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(5,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(5,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\)',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '2: TRAIN WITH CUSTOMIZED DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, you will know how to inference, test, and train predefined models with customized datasets. We use the balloon dataset as an example to describe the whole process.The',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text', 'text': 'The basic steps are as below:', 'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '1. Prepare the customized dataset2. Prepare a config3. Train, test, inference models on the customized dataset.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '6.1 Prepare the customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'There are three ways to support a new dataset in MMDetection:',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '1. reorganize the dataset into COCO format.2. reorganize the dataset into a middle format.3. implement a new dataset.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'Usually we recommend to use the first two methods which are usually easier than the third.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, we give an example for converting the data into COCO format.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: MMDetection only supports evaluating mask AP of dataset in COCO format for now. So for instance segmentation task users should convert the data into coco format.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '6.1.1 COCO annotation format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'The necessary keys of COCO format for instance segmentation is as below, for the complete details, please refer here.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/ba8442b856ebb73437c5390d8e3c3097de7495769f2ccd761ccb6bc60ea219c0.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>{\\n    &quot;images&quot;: [image],\\n    &quot;annotations&quot;: [annotation],\\n    &quot;categories&quot;: [category]\\n}</td></tr><tr><td>image = {\\n    &quot;id&quot;: int,\\n    &quot;width&quot;: int,\\n    &quot;height&quot;: int,</td></tr></table>',\n",
       "  'page_idx': 36,\n",
       "  'outline': [67, 574, 543, 712]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '\"file_name\": str, } annotation  $\\\\begin{array}{rl}{=}&{\\\\left\\\\{ \\\\begin{array}{ll} \\\\end{array} \\\\right.}\\\\end{array}$  \"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1, } categories  $\\\\begin{array}{rl}{=}&{\\\\left\\\\{ \\\\begin{array}{ll} \\\\end{array} \\\\right.}\\\\end{array}$  \"id\": int, \"name\": str, \"supercategory\": str, }]',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume we use the balloon dataset. After downloading the data, we need to implement a function to convert the annotation format into the COCO format. Then we can use implemented COCODataset to load the data and perform training and evaluation.',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'If you take a look at the dataset, you will find the dataset format is as below:',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': \"['base64_img_data': '', 'file_attributes': {}, 'filename': '34020010494_e5cb88e1c4_k.jpg', 'fileref': '', 'regions': {'0': {'region_attributes': {}, 'shape_attributes': {'all_points_x': [1020, 1000, 994, 1003, 1023, 1050, 1089, 1134, 1190, 1265, 1321, 1361, 1403, 1428, 1442, 1445, 1441, 1427, 1400, 1361, 1316, 1269, 1228,\",\n",
       "  'page_idx': 37},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/bc5b5cb48a30e123cd0eb2b2428b73c74734efa74fd73e685e6bf9ad0883752c.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>1198,</td></tr><tr><td>1207,</td></tr><tr><td>1210,</td></tr><tr><td>1190,</td></tr><tr><td>1177,</td></tr><tr><td>1172,</td></tr><tr><td>1174,</td></tr><tr><td>1170,</td></tr><tr><td>1153,</td></tr><tr><td>1127,</td></tr><tr><td>1104,</td></tr><tr><td>1061,</td></tr><tr><td>1032,</td></tr><tr><td>1020,</td></tr><tr><td>&#x27;all_points_y&#x27;: [963,</td></tr><tr><td>899,</td></tr><tr><td>841,</td></tr><tr><td>787,</td></tr><tr><td>738,</td></tr><tr><td>700,</td></tr><tr><td>663,</td></tr><tr><td>638,</td></tr><tr><td>621,</td></tr><tr><td>619,</td></tr><tr><td>643,</td></tr><tr><td>672,</td></tr><tr><td>720,</td></tr><tr><td>765,</td></tr><tr><td>800,</td></tr><tr><td>860,</td></tr><tr><td>896,</td></tr><tr><td>942,</td></tr><tr><td>990,</td></tr><tr><td>1035,</td></tr><tr><td>1079,</td></tr><tr><td>1112,</td></tr><tr><td>1129,</td></tr><tr><td>1134,</td></tr><tr><td>1144,</td></tr><tr><td>1153,</td></tr><tr><td>1166,</td></tr><tr><td>1166,</td></tr><tr><td>1150,</td></tr><tr><td>1136,</td></tr><tr><td>1129,</td></tr><tr><td>1122,</td></tr><tr><td>1112,</td></tr><tr><td>1084,</td></tr><tr><td>1037,</td></tr><tr><td>989,</td></tr><tr><td>963],</td></tr><tr><td>&#x27;name&#x27;: &#x27;polygon&#x27;{{}},</td></tr></table>',\n",
       "  'page_idx': 38,\n",
       "  'outline': [67, 82, 546, 712]},\n",
       " {'type': 'text',\n",
       "  'text': \"The annotation is a JSON file where each key indicates an image's all annotations. The code to convert the balloon dataset into coco format is as below.\",\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': \"import os.path as osp def convert_balloon_to_coco(ann_file, out_file, image_prefix): data_infos  $=$  mmcv.load(ann_file) annotations  $=$  [] images  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\\\\end{array}\\\\right]}\\\\end{array}$  obj_count - 0 for idx, v in enumerate(mmcv.track_iter_progress(data_infos.values())): filename  $=$  v['filename'] img_path  $=$  osp.join(image_prefix, filename) height, width  $=$  mmcv.imread(img_path).shape[:2] images.append(dict(  $\\\\scriptstyle {\\\\dot{\\\\mathbf{1}}} = \\\\dot{\\\\mathbf{1}}\\\\mathbf{d}\\\\mathbf{x}$  file_name  $=$  filename, height  $=$  height, width  $=$  width)) bboxes  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\\\\end{array}\\\\right]}\\\\end{array}$  labels  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\\\\end{array}\\\\right]}\\\\end{array}$  masks  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll} \\\\end{array}\\\\right]}\\\\end{array}$  for - , obj in v['regions'].items(): assert not obj['region_attributes'] obj  $=$  obj['shape_attributes'] px  $=$  obj['all_points_x'] py  $=$  obj['all_points_y'] poly  $=$  [(x + 0.5, y + 0.5) for x, y in zip(px, py)] poly  $=$  [p for x in poly for p in x] x_min, y_min, x_max, y_max = ( min(px), min(py), max(px), max(py)) data anno  $=$  dict( image_id  $=$  idx, id  $=$  obj_count, category_id  $= 0$  bbox  $=$  [x_min, y_min, x_max - x_min, y_max - y_min], area  $=$  (x_max - x_min) \\\\* (y_max - y_min), segmentation  $=$  [poly], iscrowd  $= 0$  annotations.append(data anno) obj_count  $+ = 1$  coco_format_json  $=$  dict( images  $=$  images,\",\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Using the function above, users can successfully convert the annotation file into json format, then we can use CocoDataset to train and evaluate the model.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': '6.2 Prepare a config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'The second step is to prepare a config thus the dataset could be successfully loaded. Assume that we want to use Mask R- CNN with FPN, the config to train the detector on balloon dataset is as below. Assume the config is under directory configs/balloon/ and named as mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py, the config is as below.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': \"The new config inherits a base config to highlight the necessary modification _base_  $=$  'mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_coco.py' # We also need to change the num_classes in head to match the dataset's annotation model  $=$  dict( roi_head  $=$  dict( bbox_head  $=$  dict(num_classes  $= 1$  1 mask_head  $=$  dict(num_classes  $= 1$  1) # Modify dataset related settings dataset_type  $=$  'COCODataset classes  $=$  ('balloon',) data  $=$  dict( train  $=$  dict( img_prefix  $=$  'balloon/train/' classes  $=$  classes, ann_file  $=$  'balloon/train/annotation_coco.json'), val  $=$  dict( img_prefix  $=$  'balloon/val/' classes  $=$  classes, ann_file  $=$  'balloon/val/annotation_coco.json'), test  $=$  dict( img_prefix  $=$  'balloon/val/' classes  $=$  classes, ann_file  $=$  'balloon/val/annotation_coco.json')) # We can use the pre- trained Mask RCNN model to obtain higher performance load_from  $=$  'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain- poly_3x_coco_bbox_mAP- 0.408.  $\\\\hookrightarrow$  segm_mAP- 0.37_20200504_163245- 42aa3d00. pth\",\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': '6.3 Train a new model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'To train a model with the new config, you can simply run',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/train.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': '6.4 Test and inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'To test the trained model, you can simply run python tools/test.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py work_dirs/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py/latest.pth - - eval bbox_ segm',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': '', 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': '3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, you will know how to train, test and inference your own customized models under standard datasets. We use the cityscapes dataset to train a customized Cascade Mask R- CNN R50 model as an example to demonstrate the whole process, which using AugFPN to replace the default FPN as neck, and add Rotate or Translate as training- time auto augmentation.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text', 'text': 'The basic steps are as below:', 'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': '1. Prepare the standard dataset  \\n2. Prepare your own customized model  \\n3. Prepare a config  \\n4. Train, test, and inference models on the standard dataset.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': '7.1 Prepare the standard dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, as we use the standard cityscapes dataset as an example.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'It is recommended to symlink the dataset root to $MMDETECTION/data. If your folder structure is different, you may need to change the corresponding paths in config files.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/c85bc2a8c1e1d6ed6f5b4f94da866d6033782be48fb212176e2278b635bf56c3.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 42,\n",
       "  'outline': [67, 467, 544, 701]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'The cityscapes annotations have to be converted into the coco format using tools/dataset_converters/cityscapes.py:',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install cityscapesscriptspython tools/dataset_converters/cityscapes.py ./data/cityscapes - - nproc 8 - - out- dir ./data/cityscapes/annotations',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Currently the config files in cityscapes use COCO pre- trained weights to initialize. You could download the pre- trained models in advance if network is unavailable or slow, otherwise it would cause errors at the beginning of training.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': '7.2 Prepare your own customized model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': '7.2 Prepare your own customized modelThe second step is to use your own module or training setting. Assume that we want to implement a new neck called AugFPN to replace with the default FPN under the existing detector Cascade Mask R- CNN R50. The following implementsAugFPN under MMDetection.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': '7.2.1 1. Define a new neck (e.g. AugFPN)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Firstly create a new file mmdet/models/necks/augfpn.py.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c43e20e1493bc2dab0a74f6522c2a9287699fe28d587916616807063474614cc.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from .builder import NECKS</td></tr><tr><td>@NECKS.register_module()</td></tr><tr><td>class AugFPN(nn.Module):</td></tr><tr><td>def __init__(self, \\n    in_channels, \\n    out_channels, \\n    num_outs, \\n    start_level=0, \\n    end_level=-1, \\n    add_extra_conv=False):</td></tr><tr><td>pass</td></tr><tr><td>def forward(self, inputs):</td></tr><tr><td># implementation is ignored</td></tr><tr><td>pass</td></tr></table>',\n",
       "  'page_idx': 43,\n",
       "  'outline': [67, 384, 544, 596]},\n",
       " {'type': 'text',\n",
       "  'text': '7.2.2 2. Import the module',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'You can either add the following line to mmdet/models/necks/__init__.py,',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text', 'text': 'from .augfpn import AugFPN', 'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': \"or alternatively add custom_imports  $=$  dict( imports  $=$  ['mmdet.models.necks.augfpn.py'] allow_failed_imports  $=$  False)\",\n",
       "  'page_idx': 44},\n",
       " {'type': 'text', 'text': '', 'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'to the config file and avoid modifying the original code.',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': '7.2.3 3. Modify the config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': \"neck=dict( type  $\\\\coloneqq$  'AugFPN', in_channels  $=$  [256, 512, 1024, 2048], out_channels  $= 256$  num_outs  $= 5$\",\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages about customize your own models (e.g. implement a new backbone, head, loss, etc) and runtime training settings (e.g. define a new optimizer, use gradient clip, customize training schedules and hooks, etc), please refer to the guideline Customize Models and Customize Runtime Settings respectively.',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': '7.3 Prepare a config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'The third step is to prepare a config for your own training setting. Assume that we want to add AugFPN and Rotate or Translate augmentation to existing Cascade Mask R- CNN R50 to train the cityscapes dataset, and assume the config is under directory configs/cityscapes/ and named as cascade_mask_rcnn_r50_augfpn_autoaug_10e_cityscapes.py, the config is as below.',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': \"The new config inherits the base configs to highlight the necessary modification _base_ = [ '._base_/models/cascade_mask_rcnn_r50_fpn.py', '._base_/datasets/cityscapes_instance.py', '../base_/default_runtime.py'] model = dict( # set None to avoid loading ImageNet pretrained backbone, # instead here we set `load_from` to load from COCO pretrained detectors. backbone=dict(init_cfg=None), # replace neck from defaultly `FPN` to our new implemented module `AugFPN` neck=dict( type='AugFPN', in_channels=[256, 512, 1024, 2048], out_channels=256, num_outs=5), # We also need to change the num_classes in head from 80 to 8, to match the # cityscapes dataset's annotation. This modification involves `bbox_head` and `mask_head`.\",\n",
       "  'page_idx': 44},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/0af98f27eebb7d89b4c822400bf5668cb0512944beba4e7f574b5cad596309f6.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 45,\n",
       "  'outline': [67, 82, 495, 712]},\n",
       " {'type': 'text',\n",
       "  'text': \"use sigmoid=False, loss_weight=1.0), loss_bbox=dict(type='SmoothLoss', beta=1.0, loss_weight=1.0)) ], mask_head=dict( type=FCNMaskHead', num_conv=4, in_channels=256, conv_out_channels=256, # change the number of classes from defaultlY COcO to cityscapes num_classes=8, loss_mask=dict( type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))) # over- write `train_pipeline` for new added `AutoAugment` training setting img_norm_cfg = dict( mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True) train_pipeline = [ dict(type='LoadImageFromFile'), dict(type='LoadAnnotations', with_bbox=True, with_mask=True), dict( type='AutoAugment', policies=[ [dict( type='Rotate', level=5, img_fill_val=(124, 116, 104), prob=0.5, scale=1) ], [dict(type='Rotate', level=7, img_fill_val=(124, 116, 104)), dict( type='Translate', level=5, prob=0.5, img_fill_val=(124, 116, 104)) ], ]), dict( type='Resize', img_scale=[(2048, 800), (2048, 1024)], keep_ratio=True), dict(type='RandomFlip', flip_ratio=0.5), dict(type='Normalize', **img_norm_cfg), dict(type='Pad', size_divisor=32), dict(type='DefaultFormatBundle'), dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']), ] # set batch_size per gpu, and set new training pipeline data = dict( samples_per_gpu=1, workers_per_gpu=3, # over- write `pipeline` with new training pipeline setting\",\n",
       "  'page_idx': 46},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': \"train=dict(dataset=dict(pipeline=train_pipeline))) # Set optimizer optimizer  $=$  dict(type  $\\\\coloneqq$  'SGD',  $\\\\mathtt{lr} = \\\\mathtt{0}$  .01, momentum  $= 0$  .9, weight_decay  $= 0$  .0001) optimizer_config  $=$  dict(grad clip  $\\\\equiv$  None) # Set customized learning policy lr_config  $=$  dict( policy  $\\\\coloneqq$  'step', warmup  $=$  'linear', warmup_iter  $= 500$  warmup_ratio  $= 0$  .001, step  $\\\\coloneqq$  [8]) runner  $=$  dict(type  $=$  'EpochBasedRunner', max_epochs  $= 10$  # We can use the COcO pretrained Cascade Mask R- CNN R50 model for more stable.  $\\\\hookrightarrow$  performance initialization load_from  $=$  'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_  $\\\\hookrightarrow$  rcnn_r50_fpn_lx_coco/cascade_mask_rcnn_r50_fpn_lx_coco_20200203- 9d4dcb24. pth'\",\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': '7.4 Train a new model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'To train a model with the new config, you can simply run',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/train.py configs/cityscapes/cascade_mask_rcnn_r50_augfpn_autoaug_10e_  $\\\\hookrightarrow$  cityscapes.py',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': '7.5 Test and inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'To test the trained model, you can simply run',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py configs/cityscapes/cascade_mask_rcnn_r50_augfpn_autoaug_10e_  $\\\\hookrightarrow$  cityscapes.py work_dirs/cascade_mask_rcnn_r50_augfpn_autoaug_10e_cityscapes.py/latest.  $\\\\hookrightarrow$  pth - - eval bbox segm',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 1: LEARN ABOUT CONFIGS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'We incorporate modular and inheritance design into our config system, which is convenient to conduct various experiments. If you wish to inspect the config file, you may run python tools/misc/print_config.py /PATH/TO/ CONFIG to see the complete config.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '8.1 Modify config through script arguments',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'When submitting jobs using \"tools/train.py\" or \"tools/test.py\", you may specify - - cfg- options to in- place modify the config.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '- Update config keys of dict chains. The config options can be specified following the order of the dict keys in the original config. For example, \\n--cfg-options model.backbone.norm_eval=False changes the all BN modules in model backbones to train mode.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '- Update keys inside a list of configs.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': \"Some config dicts are composed as a list in your config. For example, the training pipeline data.train. pipeline is normally a list e.g. [dict(type  $\\\\equiv$  'LoadImageFromFile'), ...]. If you want to change 'LoadImageFromFile' to 'LoadImageFromWebcam' in the pipeline, you may specify - - cfg- options data.train.pipeline.0. type  $\\\\equiv$  LoadImageFromFileWebcam.\",\n",
       "  'page_idx': 48},\n",
       " {'type': 'text', 'text': '- Update values of list/tuples.', 'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'If the value to be updated is a list or a tuple. For example, the config file normally sets workflow  $\\\\equiv$  [\\'train\\', 1)]. If you want to change this key, you may specify - - cfg- options workflow  $\\\\equiv$  \"[(train,1), (val,1)]\". Note that the quotation mark \" is necessary to support list/tuple data types, and that NO white space is allowed inside the quotation marks in the specified value.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '8.2 Config File Structure',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'There are 4 basic component types under config/_base_ dataset, model, schedule, default_runtime. Many methods could be easily constructed with one of each like Faster R- CNN, Mask R- CNN, Cascade R- CNN, RPN, SSD. The configs that are composed by components from _base_ are called primitive.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'For all configs under the same folder, it is recommended to have only one primitive config. All other configs should inherit from the primitive config. In this way, the maximum of inheritance level is 3.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'For easy understanding, we recommend contributors to inherit from existing methods. For example, if some modification is made base on Faster R- CNN, user may first inherit the basic Faster R- CNN structure by specifying _base_ = ../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py, then modify the necessary fields in the config files.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'If you are building an entirely new method that does not share the structure with any of the existing methods, you may create a folder xxx_rcnn under configs,',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to mmcw for detailed documentation.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '8.3 Config Name Style',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'We follow the below style to name config files. Contributors are advised to follow the same style.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '{model}_5model setting]{backbone}{neck}_norm setting}{misc}{gpu x batch_per_gpu}  $\\\\rightarrow$  {schedule}{dataset}',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'xxx} is required field and [yyy] is optional.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '- {model}: model type like faster_rcnn, mask_rcnn, etc.- [model setting]: specific setting for some model, like without_semantic for htc, moment for reppoints, etc.- {backbone}: backbone type like r50 (ResNet-50), x101 (ResNeXt-101).- {neck}: neck type like fpn, pafpn, nasfpn, c4.- [norm_setting]: bn (Batch Normalization) is used unless specified, other norm layer type could be gn (Group Normalization), syncbn (Synchronized Batch Normalization). gn-head/gn-neck indicates GN is applied in head/neck only, while gn-all means GN is applied in the entire model, e.g. backbone, neck, head.- [misc]: miscellaneous setting/plugins of model, e.g. dconv, gcb, attention, albu, mstrain.- [gpu x batch_per_gpu]: GPUs and samples per GPU, 8xz is used by default.- {schedule}: training schedule, options are 1x, 2x, 20e, etc. 1x and 2x means 12 epochs and 24 epochs respectively. 20e is adopted in cascade models, which denotes 20 epochs. For 1x/2x, initial learning rate decays by a factor of 10 at the 8/16th and 11/22th epochs. For 20e, initial learning rate decays by a factor of 10 at the 16th and 19th epochs.- {dataset}: dataset like coco, cityscapes, voc_0712, wider_face.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '8.4 Deprecated train_cfg/test_cfg',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'The train_cfg and test_cfg are deprecated in config file, please specify them in the model config. The original config structure is as below.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '```c# deprecatedmodel = dict(    type=...,    ...)train_cfg=dict(...)test_cfg=dict(...)```',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'The migration example is as below.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '```python# recommendedmodel = dict(    type=...,    ...    train_cfg=dict(...),    test_cfg=dict(...),)```',\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': '8.5 An Example of Mask R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': 'To help the users have a basic idea of a complete config and the modules in a modern detection system, we make brief comments on the config of Mask R- CNN using ResNet50 and FPN as the following. For more detailed usage and the corresponding alternative for each module, please refer to the API documentation.',\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': \"```pythonmodel = dict(    type='MaskRCNN',    # The name of detector    backbone=dict(    # The config of backbone        type='ResNet',    # The type of the backbone, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/backbones/resnet.py#L308 for more details.        depth=50,    # The depth of backbone, usually it is 50 or 101 for ResNet and.    ResNet backbones:        num stages=4,    # Number of stages of the backbone.        out_indices=(0, 1, 2, 3),    # The index of output feature maps produced in each.    stages        frozen stages=1,    # The weights in the first 1 stage are frozen        norm_cfg=dict(    # The config of normalization layers.        type='BN',    # Type of norm layer, usually it is BN or GN        requires_grad=True),    # Whether to train the gamma and beta in BN        norm_eval=True,    # Whether to freeze the statistics in BN        style='pytorch'    # The style of backbone, 'pytorch' means that stride 2 layers.    are in 3x3 conv, 'caffe' means stride 2 layers are in 1x1 conv.        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),    # The ImageNet pretrained backbone to be loaded        neck=dict(    type='FPN',    # The neck of detector is FPN. We also support 'NASFPN', 'PAFPN', etc. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/neck/sfpn.py#L10 for more details.        in_channels=[256, 512, 1024, 2048],    # The input channels, this is consistent.        with the output channels of backbone        out_channels=256,    # The output channels of each level of the pyramid feature map        num outs=5),    # The number of output scales        rpn_head=dict(    type='RPNHead',    # The type of RPN head is 'RPNHead', we also support 'GARPNHead', etc. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/dense_heads/ron_head.py#L12 for more details.        in_channels=256,    # The input channels of each input feature map, this is consistent with the output channels of neck        feat_channels=256,    # Feature channels of convolutional layers in the head.        anchor_generator=dict(    # The config of anchor generator        type='AnchorGenerator',    # Most of methods use AnchorGenerator, SSD.    Detectors uses `SSDAnchorGenerator`. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/core/anchor/anchor_generator.py#L10 for more details. )```\",\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': \"(continued from previous page) scales  $\\\\coloneqq$  [8], # Basic scale of the anchor, the area of the anchor in one.  $\\\\rightharpoonup$  position of a feature map will be scale \\\\* base_sizes ratios  $\\\\coloneqq$  [0.5, 1.0, 2.0], # The ratio between height and width. strides  $\\\\coloneqq$  [4, 8, 16, 32, 64]), # The strides of the anchor generator. This is.  $\\\\rightharpoonup$  consistent with the FPN feature strides. The strides will be taken as base_sizes if.  $\\\\rightharpoonup$  base_sizes is not. bboxCoder  $\\\\coloneqq$  dict( # Config of box coder to encode and decode the boxes during.  $\\\\rightharpoonup$  training and testing type  $\\\\coloneqq$  'DeltaXYwHBBoxCoder', # Type of box coder. 'DeltaXYwHBBoxCoder' is.  $\\\\rightharpoonup$  applied for most of methods. Refer to https://github.com/open- mmlab/mmdetection/blob/  $\\\\rightharpoonup$  master/mmdet/core/bbox/coder/delta_xywh_bboxCoder.py#L9 for more details. target_means  $\\\\coloneqq$  [0.0, 0.0, 0.0, 0.0], # The target means used to encode and.  $\\\\rightharpoonup$  decode boxes target_std  $\\\\coloneqq$  [1.0, 1.0, 1.0, 1.0]], # The standard variance used to encode.  $\\\\rightharpoonup$  and decode boxes loss_cls  $\\\\coloneqq$  dict( # Config of loss function for the classification branch type  $\\\\coloneqq$  'CrossEntropyLoss', # Type of loss for classification branch, we also.  $\\\\rightharpoonup$  support FocalLoss etc. use sigmoid  $\\\\coloneqq$  True, # RPN usually perform two- class classification, so it.  $\\\\rightharpoonup$  usually uses sigmoid function. loss_weight  $\\\\coloneqq$  1.0), # Loss weight of the classification branch. loss_bbox  $\\\\coloneqq$  dict( # Config of loss function for the regression branch. type  $\\\\coloneqq$  'l1Loss', # Type of loss, we also support many IoU Losses and smooth.  $\\\\rightharpoonup$  L1- loss, etc. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/  $\\\\rightharpoonup$  models/losses/smooth_11_loss.py#L56 for implementation. loss_weight  $\\\\coloneqq$  1.0), # Loss weight of the regression branch. roi_head  $\\\\coloneqq$  dict( # RoIHead encapsulates the second stage of two- stage/cascade.  $\\\\rightharpoonup$  detectors. type  $\\\\coloneqq$  'StandardRoIHead', # Type of the RoI head. Refer to https://github.com/  $\\\\rightharpoonup$  open- mmlab/mmdetection/blob/master/mmdet/models/roi heads/standard_roi head.py#L10 for.  $\\\\rightharpoonup$  implementation. bbox ROI extractor  $\\\\coloneqq$  dict( # RoI feature extractor for bbox regression. type  $\\\\coloneqq$  'SingleRoIExtractor', # Type of the RoI feature extractor, most of.  $\\\\rightharpoonup$  methods uses SingleRoIExtractor. Refer to https://github.com/open- mmlab/mmdetection/  $\\\\rightharpoonup$  blob/master/mmdet/models/roi heads/roi extractors/single_level.py#L10 for details. roi_layer  $\\\\coloneqq$  dict( # Config of RoI Layer type  $\\\\coloneqq$  'RoIAlign', # Type of RoI Layer, DeformRoIPoolingPack and.  $\\\\rightharpoonup$  ModulatedDeformRoIPoolingPack are also supported. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/ops/roi_align/roi_align.py#L79 for details. output_size  $\\\\coloneqq$  7, # The output size of feature maps. sampling_ratio  $\\\\coloneqq$  0), # Sampling ratio when extracting the RoI features.  $\\\\theta_{\\\\perp}$ $\\\\rightharpoonup$  means adaptive ratio. out_channels  $= 256$  , # output channels of the extracted feature. featmap_strides  $\\\\coloneqq$  [4, 8, 16, 32]), # Strides of multi- scale feature maps. It.  $\\\\rightharpoonup$  should be consistent to the architecture of the backbone. bbox_head  $\\\\coloneqq$  dict( # Config of box head in the RoIHead. type  $\\\\coloneqq$  'Shared2FCBBoxHead', # Type of the bbox head, Refer to https://github. com/open- mmlab/mmdetection/blob/master/mmdet/models/roi heads/bbox heads/convfc_bbox_  $\\\\rightharpoonup$  head.py#L177 for implementation details. in_channels  $= 256$  , # Input channels for bbox head. This is consistent with.  $\\\\rightharpoonup$  the out_channels in roi extractor fc_out_channels  $= 1024$  , # Output feature channels of FC layers.\",\n",
       "  'page_idx': 51},\n",
       " {'type': 'text',\n",
       "  'text': \"roilfeat_size  $= 7$  , # Size of RoI features num_classes  $= 80$  , # Number of classes for classification bbok coder  $\\\\coloneqq$  dict( # Box coder used in the second stage. type  $\\\\coloneqq$  'DeltaXYwHBoxCoder', # Type of box coder. 'DeltaXYwHBoxCoder' is. applied for most of methods. target_means  $=$  [0.0, 0.0, 0.0, 0.0], # Means used to encode and decode box target stds  $=$  [0.1, 0.1, 0.2, 0.2]), # Standard variance for encoding and. decoding. It is smaller since the boxes are more accurate. [0.1, 0.1, 0.2, 0.2] is a. conventional setting. reg_class_agnostic  $=$  False, # Whether the regression is class agnostic. loss cls  $\\\\coloneqq$  dict( # Config of loss function for the classification branch. type  $\\\\coloneqq$  'CrossEntropyLoss', # Type of loss for classification branch, we. also support FocalLoss etc. use_sigmoid  $\\\\coloneqq$  False, # Whether to use sigmoid. loss_weight  $= 1.0$  ), # Loss weight of the classification branch. loss_bbox  $\\\\coloneqq$  dict( # Config of loss function for the regression branch. type  $\\\\coloneqq$  'LlLoss', # Type of loss, we also support many IoU losses and. smooth L1- loss, etc. loss_weight  $= 1.0$  ), # Loss weight of the regression branch. mask ROI extractor  $\\\\coloneqq$  dict( # RoI feature extractor for mask generation. type  $\\\\coloneqq$  'SingleRoIExtractor', # Type of the RoI feature extractor, most of. methods uses SingleRoIExtractor. roi_layer  $\\\\coloneqq$  dict( # Config of RoI Layer that extracts features for instance. segmentation type  $\\\\coloneqq$  'RoIAlign', # Type of RoI Layer, DeformRoIPoolingPack and. ModulatedDeformRoIPoolingPack are also supported. output_size  $= 14$  , # The output size of feature maps. sampling_ratio  $= 0$  ), # Sampling ratio when extracting the RoI features. out_channels  $= 256$  , # Output channels of the extracted feature. featmap_strides  $=$  [4, 8, 16, 32]), # Strides of multi- scale feature maps. mask_head  $\\\\coloneqq$  dict( # Mask prediction head. type  $\\\\coloneqq$  'FCNMaskHead', # Type of mask head, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/roi heads/mask heads/fcn_mask_head.py#L21. for implementation details. num_conv  $s = 4$  , # Number of convolutional layers in mask head. in_channels  $= 256$  , # Input channels, should be consistent with the output. channels of mask roi extractor. conv_out_channels  $= 256$  , # Output channels of the convolutional layer. num_classes  $= 80$  , # Number of class to be segmented. loss_mask  $\\\\coloneqq$  dict( # Config of loss function for the mask branch. type  $\\\\coloneqq$  'CrossEntropyLoss', # Type of loss used for segmentation use_mask  $\\\\coloneqq$  True, # Whether to only train the mask in the correct class. loss_weight  $= 1.0$  )) # Loss weight of mask branch. train_cfg  $=$  dict( # Config of training hyperparameters for rpn and rconn. rpn  $\\\\coloneqq$  dict( # Training config of rpn. assigner  $\\\\coloneqq$  dict( # Config of assigner type  $\\\\coloneqq$  'MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for. many common detectors. Refer to https://github.com/open- mmlab/mmdetection/blob/master/ mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details. pos_iou_thr  $= 0.7$  , # IoU  $> =$  threshold 0.7 will be taken as positive. samples neg_iou_thr  $= 0.3$  , # IoU  $<$  threshold 0.3 will be taken as negative samples\",\n",
       "  'page_idx': 52},\n",
       " {'type': 'text',\n",
       "  'text': \"min_pos_iou  $= 0.3$  , # The minimal IoU threshold to take boxes as positive. samples match_low_quality  $=$  True, # Wether to match the boxes under low quality.  $\\\\leftrightarrow$  (see API doc for more details). ignore_iof_thr  $\\\\scriptstyle = - 1$  ), # IoF threshold for ignoring bboxes sampler=dict( # Config of positive/negative sampler type  $\\\\coloneqq$  'RandomSampler', # Type of sampler, PseudoSampler and other.  $\\\\leftrightarrow$  samplers are also supported. Refer to https://github.com/open- mmlab/mmdetection/blob/  $\\\\leftrightarrow$  master/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details. num  $= 256$  , # Number of samples. pos_fraction  $= 0.5$  , # The ratio of positive samples in the total samples. neg_pos_ub  $\\\\scriptstyle = - 1$  , # The upper bound of negative samples based on the.  $\\\\leftrightarrow$  number of positive samples. add_gt_as_proposals  $=$  False), # Whether add GT as proposals after.  $\\\\leftrightarrow$  sampling. allowed_border  $\\\\scriptstyle = - 1$  , # The border allowed after padding for valid anchors. pos_weight  $\\\\scriptstyle = - 1$  , # The weight of positive samples during training. debug  $\\\\coloneqq$  False), # Whether to set the debug mode. rpn_proposal  $=$  dict( # The config to generate proposals during training nms_across_levels  $=$  False, # Whether to do NMS for boxes across levels. Only.  $\\\\leftrightarrow$  work in GARPNHead', naive rpn does not support do nms cross levels. nms_prc  $= 2000$  , # The number of boxes before NMS nms_post  $= 1000$  , # The number of boxes to be kept by NMS, Only work in.  $\\\\leftrightarrow$  GARPNHead'. max_per_img  $= 1000$  , # The number of boxes to be kept after NMS. nms- dict( # Config of NMS type  $\\\\coloneqq$  'nms', # Type of NMS iou_threshold  $= 0.7$  # NMS threshold ), min_bbox_size  $= 0$  ), # The allowed minimal box size rcnn=dict( # The config for the roi heads. assigner  $=$  dict( # Config of assigner for second stage, this is different for.  $\\\\leftrightarrow$  that in rpn type  $\\\\coloneqq$  'MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for.  $\\\\leftrightarrow$  all roi heads for now. Refer to https://github.com/open- mmlab/mmdetection/blob/master/  $\\\\leftrightarrow$  mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details. pos_iou_thr  $= 0.5$  , # IoU  $> =$  threshold 0.5 will be taken as positive.  $\\\\leftrightarrow$  samples neg_iou_thr  $= 0.5$  , # IoU  $<$  threshold 0.5 will be taken as negative samples min_pos_iou  $= 0.5$  , # The minimal IoU threshold to take boxes as positive.  $\\\\leftrightarrow$  samples match_low_quality  $=$  False, # Whether to match the boxes under low quality.  $\\\\leftrightarrow$  (see API doc for more details). ignore_iof_thr  $\\\\scriptstyle = - 1$  ), # IoF threshold for ignoring bboxes sampler=dict( type  $\\\\coloneqq$  'RandomSampler', # Type of sampler, PseudoSampler and other.  $\\\\leftrightarrow$  samplers are also supported. Refer to https://github.com/open- mmlab/mmdetection/blob/  $\\\\leftrightarrow$  master/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details. num  $= 512$  , # Number of samples pos_fraction  $= 0.25$  , # The ratio of positive samples in the total samples. neg_pos_ub  $\\\\scriptstyle = - 1$  , # The upper bound of negative samples based on the.  $\\\\leftrightarrow$  number of positive samples.\",\n",
       "  'page_idx': 53},\n",
       " {'type': 'text',\n",
       "  'text': \"add_gt_as_proposals  $=$  True ), # Whether add GT as proposals after sampling. mask_size  $= 28$  , # Size of mask pos_weight  $\\\\coloneqq - 1$  , # The weight of positive samples during training. debug  $\\\\equiv$  False)) # Whether to set the debug mode test_cfg  $=$  dict( # Config for testing hyperparameters for rpn and rcnn. rpn  $\\\\equiv$  dict( # The config to generate proposals during testing nms_lacross_levels  $\\\\equiv$  False, # Whether to do NMS for boxes across levels. Only. work in `GARPNHead`, naive rpn does not support do nms cross levels. nms_pre  $= 1000$  , # The number of boxes before NMS nms_post  $= 1000$  , # The number of boxes to be kept by NMS, Only work in.  $\\\\leftrightarrow$  GARPNHead'. max_per_img  $= 1000$  , # The number of boxes to be kept after NMS. nms=dict( # Config of NMS type  $\\\\equiv$  'nms', #Type of NMS iou_threshold  $= 0.7$  # NMS threshold ), min_bbox_size  $= 0$  ), # The allowed minimal box size rcnn  $\\\\equiv$  dict( # The config for the roi heads. score thr  $= 0.05$  , # Threshold to filter out boxes nms=dict( # Config of NMS in the second stage type  $\\\\equiv$  'nms', # Type of NMS iou thr  $= 0.5$  ), # NMS threshold max_per_img  $= 100$  , # Max number of detections of each image mask_thr_binary  $= 0.5$  ) # Threshold of mask prediction dataset_type  $=$  'CocoDataset' # Dataset type, this will be used to define the dataset data_root  $=$  'data/coco/' # Root path of data img_norm_cfg  $=$  dict( # Image normalization config to normalize the input images mean  $=$  [123.675, 116.28, 103.53], # Mean values used to pre- training the pre- trained.  $\\\\leftrightarrow$  backbone models std  $=$  [58.395, 57.12, 57.375], # Standard variance used to pre- training the pretrained backbone models to_rg  $\\\\beta =$  True ) # The channel orders of image used to pre- training the pre- trained backbone models train_pipeline  $=$  [ # Training pipeline dict(type  $=$  'LoadImageFromFile'), # First pipeline to load images from file path dict( type  $=$  'LoadAnnotations', # Second pipeline to load annotations for current image with_bbox=True, # Whether to use bounding box, True for detection with_mask  $\\\\equiv$  True, # Whether to use instance mask, True for instance segmentation poly2mask  $\\\\equiv$  False), # Whether to convert the polygon mask to instance mask, set.  $\\\\leftrightarrow$  False for acceleration and to save memory dict( type  $=$  'Resize', # Augmentation pipeline that resize the images and their annotations img_scale  $=$  (1333, 800), # The largest scale of image keep_ratio  $\\\\equiv$  True ), # whether to keep the ratio between height and width. dict( type  $=$  'RandomFlip', # Augmentation pipeline that flip the images and their.  $\\\\leftrightarrow$  annotations flip_ratio  $= 0.5$  ), # The ratio or probability to flip\",\n",
       "  'page_idx': 54},\n",
       " {'type': 'text',\n",
       "  'text': \"dict( type  $\\\\coloneqq$  'Normalize', # Augmentation pipeline that normalize the input images mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], # These keys are the same of img_norm_cfg since_ the std=[58.395, 57.12, 57.375], # keys of img_norm_cfg are used here as arguments to_rgb  $\\\\coloneqq$  True), dict( type  $\\\\coloneqq$  'Pad', # Padding config size_divisor  $= 32$  ), # The number the padded images should be divisible dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), # Default format bundle to gather data in the_ pipeline dict( type  $\\\\coloneqq$  'Collect', # Pipeline that decides which keys in the data should be passed_ to the detector keys  $\\\\coloneqq$  ['img', 'gt_bboxes', 'gt_labels', 'gt_masks']) ] test_pipeline  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), # First pipeline to load images from file path dict( type  $\\\\coloneqq$  'MultiScaleFlipAug', # An encapsulation that encapsulates the testing_ augmentations img_scale  $\\\\coloneqq$  (1333, 800), # Decides the largest scale for testing, used for the_ Resize pipeline flip  $\\\\coloneqq$  False, # Whether to flip images during testing transforms  $=$  [ dict(type  $\\\\coloneqq$  'Resize', # Use resize augmentation keep_ratio  $\\\\coloneqq$  True), # Whether to keep the ratio between height and width, the img_scale set here will be suppressed by the img_scale set above. dict(type  $\\\\coloneqq$  'RandomFlip'), # Thought RandomFlip is added in pipeline, it is_ not used because flip  $\\\\coloneqq$  False dict( type  $\\\\coloneqq$  'Normalize', # Normalization config, the values are from img_norm_ cfg mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True), dict( type  $\\\\coloneqq$  'Pad', # Padding config to pad images divisible by 32. size_divisor  $= 32$  ), dict( type  $\\\\coloneqq$  'ImageToTensor', # convert image to tensor keys  $\\\\coloneqq$  ['img']), dict( type  $\\\\coloneqq$  'Collect', # Collect pipeline that collect necessary keys for_ testing. keys  $\\\\coloneqq$  ['img']) ]) ] data  $=$  dict( samples_per_gpu  $= 2$  , # Batch size of a single GPU workers_per_gpu  $= 2$  , # Worker to pre- fetch data for each single GPU train  $\\\\coloneqq$  dict( # Train dataset config\",\n",
       "  'page_idx': 55},\n",
       " {'type': 'text',\n",
       "  'text': \"type  $\\\\coloneqq$  'CocoDataset', # Type of dataset, refer to https://github.com/open- mmlab/ _mmdetection/blob/master/mmdet/datasets/coco.py#L19 for details. ann_file  $\\\\coloneqq$  'data/coco/annotations/instances_train2017. json', # Path of annotation file img_prefix  $\\\\coloneqq$  'data/coco/train2017/' # Prefix of image path pipeline  $\\\\coloneqq$  # pipeline, this is passed by the train_pipeline created before. dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict( type  $\\\\coloneqq$  'LoadAnnotations', with_bbox  $\\\\coloneqq$  True, with_mask  $\\\\coloneqq$  True, poly2mask  $\\\\coloneqq$  False), dict(type  $\\\\coloneqq$  'Resize', img_scale  $\\\\coloneqq$  (1333, 800), keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip', flip_ratio  $= 0.5$  ), dict( type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  ), dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), dict( type  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img', 'gt_bboxes', 'gt_labels', 'gt_masks']) ], val=dict( # Validation dataset config type  $\\\\coloneqq$  'CocoDataset', ann_file  $\\\\coloneqq$  'data/coco/annotations/instances_val2017. json', img_prefix  $\\\\coloneqq$  'data/coco/val2017/', pipeline  $\\\\coloneqq$  [ # Pipeline is passed by test_pipeline created before dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict( type  $\\\\coloneqq$  'MultiScaleFlipAug', img_scale  $\\\\coloneqq$  (1333, 800), flip=False, transforms=[ dict(type  $\\\\coloneqq$  'Resize', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip'), dict( type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  ), disttype  $\\\\coloneqq$  'ImageToTensor', keys  $\\\\coloneqq$  ['img'], disttype  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img']) ] ) test=dict( # Test dataset config, modify the ann_file for test- dev/test submission type  $\\\\coloneqq$  'CocoDataset', ann_file  $\\\\coloneqq$  'data/coco/annotations/instances_val2017. json', img_prefix  $\\\\coloneqq$  'data/coco/val2017/',\",\n",
       "  'page_idx': 56},\n",
       " {'type': 'text',\n",
       "  'text': \"```pythonswitch (continued from previous page)pipeline=[ # Pipeline is passed by test_pipeline created before    dict(type='LoadImageFromFile'),    dict(        type='MultiScaleFlipAug',        img_scale=(1333, 800),        flip=False,        transforms=[            dict(type='Resize', keep_ratio=True),            dict(type='RandomFlip'),            dict(                type='Normalize',                mean=[123.675, 116.28, 103.53],                std=[58.395, 57.12, 57.375],                to_rgb=True),            dict(type='Pad', size_divisor=32),            dict(type='ImageToTensor', keys=['img']),            dict(type='Collect', keys=['img'])        ])    ],    samples_per_gpu=2  # Batch size of a single GPU used in testing)evaluation = dict( # The config to build the evaluation hook, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/core/evaluation/eval_hooks.py#L7 for more_details.interval=1, # Evaluation interval    metric=['bbox', 'segm'])  # Metrics used during evaluationoptimizer = dict( # Config used to build optimizer, support all the optimizers inPyTorch whose arguments are also the same as those in PyTorch    type='SGD',  # Type of optimizers, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/core/optimizer/default_constructor.py#L13 for more_details    lr=0.02,  # Learning rate of optimizers, see detail usages of the parameters in the_documentation of PyTorch    momentum=0.9,  # Momentum        weight_decay=0.0001)  # Weight decay of SGDoptimizer_config = dict( # Config used to build the optimizer hook, refer to https://github.com/open- mmlab/mmcv/blob/master/mmcv/runner/hooks/optimizer.py#L8 forimplementation details.    grad_clip=None)  # Most of the methods do not use gradient cliplr_config = dict( # Learning rate scheduler config used to register LrUpdater hook    policy='step',  # The policy of scheduler, also support CosineAnnealing, Cyclic, etc.    Refer to details of supported LrUpdater from https://github.com/open- mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py#L9. warmup='linear',  # The warmup policy, also support `exp` and `constant`.    warmup_iters=500,  # The number of iterations for warmup    warmup_ratio=    0.001,  # The ratio of the starting learning rate used for warmup    step=[8, 11])  # Steps to decay the learning raterunner = dict(type='EpochBasedRunner',  # Type of runner to use (i.e. IterBasedRunner or_EpochBasedRunner)    max_epochs=12)  # Runner that runs the workflow in total max_epochs. For    IterBasedRunner_use `max_itors`  ```\",\n",
       "  'page_idx': 57},\n",
       " {'type': 'text',\n",
       "  'text': \"checkpoint_config  $=$  dict( # Config to set the checkpoint hook, Refer to https://github. com/open- mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation. interval  $= 1$  # The save interval is 1 log_config  $=$  dict( # config to register logger hook interval  $= 50$  # Interval to print the log hooks=[ # dict(type  $\\\\coloneqq$  'TensorboardLoggerHook') # The Tensorboard logger is also supported dict(type  $\\\\coloneqq$  'TextLoggerHook') ]) # The logger used to record the training process. dist_params  $=$  dict(backend  $\\\\equiv$  'ncc1') # Parameters to setup distributed training, the port. can also be set. log_level  $=$  'INFO' # The level of logging. load_from  $=$  None # load models as a pre- trained model from a given path. This will not. resume training. resume_from  $=$  None # Resume checkpoints from a given path, the training will be resumed.  $\\\\rightharpoonup$  from the epoch when the checkpoint's is saved. workflow  $=$  [('train', 1)] # Workflow for runner. [('train', 1)] means there is only one. workflow and the workflow named 'train' is executed once. The workflow trains the. model by 12 epochs according to the total_epochs. work_dir  $=$  'work_dir' # Directory to save the model checkpoints and logs for the. current experiments.\",\n",
       "  'page_idx': 58},\n",
       " {'type': 'text', 'text': '8.6 FAQ', 'text_level': 1, 'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': '8.6.1 Ignore some fields in the base configs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': 'Sometimes, you may set _delete_=True to ignore some of fields in base configs. You may refer to mmcv for simple illustration.',\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': 'In MMDetection, for example, to change the backbone of Mask R- CNN with the following config.',\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': \"model  $=$  dict( type  $\\\\coloneqq$  'MaskRCNN', pretrained  $=$  'torchvision://resnet50', backbone  $=$  dict( type  $\\\\coloneqq$  'ResNet', depth  $= 50$  num stages  $= 4$  out_indices  $= (0,1,2,3)$  frozen_stages  $= 1$  norm_cfg=dict(type  $= 1\\\\mathrm{BN}$  , requires_grad  $=$  True), norm_eval  $=$  True, style  $=$  'pytorch'), neck  $=$  dict(...), rpn_head  $=$  dict(...), roi_head  $=$  dict(...))\",\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': 'ResNet and HRNet use different keywords to construct.',\n",
       "  'page_idx': 58},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/0afdf850715327fe0701f4af9550bd9485929957f4b7dbf81f1741b5d53cfb84.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>_base_ = &#x27;../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py&#x27;</td></tr><tr><td>model = dict(\\n    pretrained=&#x27;open-mlab://msra/hrnetv2_w32&#x27;,\\n    backbone=dict(\\n        _delete=True,\\n        type=&#x27;HRNet&#x27;,\\n        extra=dict(\\n            stage1=dict(\\n                num_modules=1,\\n                numbranches=1,\\n                block=&#x27;BOTTLENECK&#x27;,\\n                num_blocks=(4, ),\\n                num_channels=(64, )),\\n                stage2=dict(\\n                    num_modules=1,\\n                    numbranches=2,\\n                    block=&#x27;BASIC&#x27;,\\n                    num_blocks=(4, 4),\\n                    num_channels=(32, 64)),\\n                stage3=dict(\\n                    num_modules=4,\\n                    numbranches=3,\\n                    block=&#x27;BASIC&#x27;,\\n                    num_blocks=(4, 4, 4),\\n                    num_channels=(32, 64, 128)),\\n                stage4=dict(\\n                    num_modules=3,\\n                    numbranches=4,\\n                    block=&#x27;BASIC&#x27;,\\n                    num_blocks=(4, 4, 4, 4),\\n                    num_channels=(32, 64, 128, 256))&#x27;)</td></tr><tr><td>neck=dict(...)</td></tr></table>',\n",
       "  'page_idx': 59,\n",
       "  'outline': [67, 73, 543, 465]},\n",
       " {'type': 'text',\n",
       "  'text': 'The _delete_=True would replace all old keys in backbone field with new keys.',\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': '8.6.2 Use intermediate variables in configs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': \"Some intermediate variables are used in the configs files, like train_pipeline/test_pipeline in datasets. It's worth noting that when modifying intermediate variables in the children configs, user need to pass the intermediate variables into corresponding fields again. For example, we would like to use multi scale strategy to train a Mask R- CNN. train_pipeline/test_pipeline are intermediate variable we would like modify.\",\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': \"base_  $=$  '../mask_rcnn_r50_fpn_1x_coco.py' img_norm_cfg  $=$  dict( mean  $\\\\equiv$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\equiv$  True) train_pipeline  $=$  [ dict(type  $=$  'LoadImageFromFile'), dict(type  $=$  'LoadAnnotations', with_bbox  $\\\\equiv$  True, with_mask  $\\\\equiv$  True), dict( type  $=$  'Resize', img_scale  $=$  [(1333, 640), (1333, 672), (1333, 704), (1333, 736), (1333, 768), (1333, 800)]\",\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': 'multiscale_mode  $\\\\coloneqq$  \"value\", keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  \\'RandomFlip\\', flip_ratio  $= 0$  .5), dict(type  $\\\\coloneqq$  \\'Normalize\\', \\\\*\\\\*img_norm_cfg), dict(type  $\\\\coloneqq$  \\'Pad\\', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  \\'DefaultFormatBundle\\'), dict(type  $\\\\coloneqq$  \\'Collect\\', keys  $\\\\coloneqq$  [\\'img\\', \\'gt_bboxes\\', \\'gt_labels\\', \\'gt_masks\\']), ] test_pipeline  $=$  [ dict(type  $\\\\coloneqq$  \\'LoadImageFromFile\\'), dict( type  $\\\\coloneqq$  \\'MultiScaleFlipAug\\', img_scale  $=$  (1333, 800), flip  $\\\\coloneqq$  False, transforms  $\\\\coloneqq$  [ dict(type  $\\\\coloneqq$  \\'Resize\\', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  \\'RandomFlip\\'), dict(type  $\\\\coloneqq$  \\'Normalize\\', \\\\*\\\\*img_norm_cfg), dict(type  $\\\\coloneqq$  \\'Pad\\', size_divisor  $= 32$  1 dict(type  $\\\\coloneqq$  \\'ImageToTensor\\', keys  $\\\\coloneqq$  [\\'img\\']), dict(type  $\\\\coloneqq$  \\'Collect\\', keys  $\\\\coloneqq$  [\\'img\\']), ] ] data  $=$  dict( train  $\\\\coloneqq$  dict(pipeline  $\\\\coloneqq$  train_pipeline), val  $\\\\coloneqq$  dict(pipeline  $\\\\coloneqq$  test_pipeline), test  $\\\\coloneqq$  dict(pipeline  $\\\\coloneqq$  test_pipeline))',\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': 'We first define the new train_pipeline/test_pipeline and pass them into data.',\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': 'Similarly, if we would like to switch from SyncBN to BN or MMSyncBN, we need to substitute every norm_cfg in the config.',\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': \"base_ = './mask_rcnn_r50_fpn_1x_coco.py'  norm_cfg = dict(type='BN', requires_grad=True)  model = dict(    backbone=dict(norm_cfg=norm_cfg),    neck=dict(norm_cfg=norm_cfg),    ...)\",\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 8. Tutorial 1: Learn about Configs',\n",
       "  'page_idx': 61},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 2: CUSTOMIZE DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': '9.1 Support new data format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': \"To support a new data format, you can either convert them to existing formats (COCO format or PASCAL format) or directly convert them to the middle format. You could also choose to convert them offline (before training by a script) or online (implement a new dataset and do the conversion at training). In MMDetection, we recommend to convert the data into COCO formats and do the conversion offline, thus you only need to modify the config's data annotation paths and classes after the conversion of your data.\",\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': '9.1.1 Reorganize new data formats to existing format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': 'The simplest way is to convert your dataset to existing dataset formats (COCO or PASCAL VOC).',\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': 'The annotation json files in COCO format has the following necessary keys:',\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': \"'images': [ { 'file_name': 'COCO_val2014_000000001268. jpg', 'height': 427, 'width': 640, 'id': 1268 }, ... ], 'annotations': [ { 'segmentation': [[192.81, 247.09, ... 219.03, 249.06]], # if you have mask labels 'area': 1035.749, 'iscrowd': 0, 'image_id': 1268, 'bbox': [192.81, 224.8, 74.73, 33.43], 'category_id': 16, 'id': 42986 }, ...\",\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': \"], 'categories': [ {id': 0, 'name': 'car'}, ]\",\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'There are three necessary keys in the json file:',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '- images: contains a list of images with their information like file_name, height, width, and id.- annotations: contains the list of instance annotations.- categories: contains the list of categories names and their ID.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'After the data pre- processing, there are two steps for users to train the customized new dataset with existing format (e.g. COCO format):',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '1. Modify the config file for using the customized dataset.2. Check the annotations of the customized dataset.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'Here we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format to train an existing Cascade Mask R- CNN R50- FPN detector.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '1. Modify the config file for using the customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'There are two aspects involved in the modification of config file:',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '1. The data field. Specifically, you need to explicitly add the classes fields in data.train, data.val and data.test.2. The num_classes field in the model part. Explicitly over-write all the num_classes from default value (e.g. 80 in COCO) to your classes number.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text', 'text': 'In configs/my_custom_config.py:', 'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': \"the new config inherits the base configs to highlight the necessary modification _base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py' # 1. dataset settings dataset_type  $=$  'CocoDataset' classes  $=$  ('a','b','c','d','e') data  $=$  dict( samples_per_gpu  $= 2$  workers_per_gpu  $= 2$  train  $=$  dict( type  $=$  dataset_type, # explicitly add your class names to the field classes classes  $=$  classes, ann_file  $=$  'path/to/your/train/annotation_data', img_prefix  $=$  'path/to/your/train/image_data'), val  $=$  dict( type  $=$  dataset_type, # explicitly add your class names to the field classes classes  $=$  classes,\",\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': \"(continued from previous page)ann_file='path/to/your/val/annotation_data',img_prefix='path/to/your/val/image_data'),test=dict(type=dataset_type, # explicitly add your class names to the field `classes` classes=classes, ann_file='path/to/your/test/annotation_data', img_prefix='path/to/your/test/image_data'))# 2. model settings# explicitly over- write all the `num_classes` field from default 80 to 5. model = dict(roi_head=dict(    bbox_head=[        dict(            type='Shared2FCBBoxHead',            # explicitly over- write all the `num_classes` field from default 80 to 5.            num_classes=5),            dict(            type='Shared2FCBBoxHead',            # explicitly over- write all the `num_classes` field from default 80 to 5.            num_classes=5),            dict(            type='Shared2FCBBoxHead',            # explicitly over- write all the `num_classes` field from default 80 to 5.            num_classes=5)],# explicitly over- write all the `num_classes` field from default 80 to 5. mask_head=dict(num_classes=5)))\",\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': '2. Check the annotations of the customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming your customized dataset is COCO format, make sure you have the correct annotations in the customized dataset:',\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': '1. The length for categories field in annotations should exactly equal the tuple length of classes fields in your config, meaning the number of classes (e.g. 5 in this example).2. The classes fields in your config file should have exactly the same elements and the same order with the name in categories of annotations. MMDetection automatically maps the uncontinuous id in categories to the continuous label indices, so the string order of name in categories field affects the order of label indices. Meanwhile, the string order of classes in config affects the label text during visualization of predicted bounding boxes.3. The category_id in annotations field should be valid, i.e., all values in category_id should belong to id in categories.',\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': 'Here is a valid example of annotations:',\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': \"'annotations': [{    'segmentation': [[192.81,\",\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': \"247.09, 219.03, 249.06], # if you have mask labels 'area': 1035.749, 'iscrowd': 0, 'image_id': 1268, 'bbox': [192.81, 224.8, 74.73, 33.43], 'category_id': 16, 'id': 42986 }, 2, 1, # MMDetection automatically maps the uncontinuous `id` to the continuous label indices. 'categories': ['id': 1, 'name': 'a'}, {'id': 3, 'name': 'b'}, {'id': 4, 'name': 'c'}, {'id': 16, 'name': 'd'}, {'id': 17, 'name': 'e'}, ]\",\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': 'We use this way to support CityScapes dataset. The script is in cityscapes.py and we also provide the finetuning configs.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text', 'text': 'Note', 'text_level': 1, 'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': '1. For instance segmentation datasets, MMDetection only supports evaluating mask AP of dataset in COCO format for now.  \\n2. It is recommended to convert the data offline before training, thus you can still use CocoDataset and only need to modify the path of annotations and the training classes.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': '9.1.2 Reorganize new data format to middle format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': 'It is also fine if you do not want to convert the annotation format to COCO or PASCAL format. Actually, we define a simple annotation format and all existing datasets are processed to be compatible with it, either online or offline.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': 'The annotation of a dataset is a list of dict, each dict corresponds to an image. There are 3 field filename (relative path), width, height for testing, and an additional field ann for training. ann is also a dict containing at least 2 fields: bboxes and labels, both of which are numpy arrays. Some datasets may provide annotations like crowd/difficult/ignored bboxes, we use bboxes_ignore and labels_ignore to cover them.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text', 'text': 'Here is an example.', 'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': \"[ { 'filename': 'a.jpg', 'width': 1280, 'height': 720, 'ann': [ 'bboxes': <np.ndarray, float32> (n, 4), 'labels': <np.ndarray, int64> (n, ), 'bboxes_ignore': <np.ndarray, float32> (k, 4), 'labels_ignore': <np.ndarray, int64> (k, ) (optional field) }\",\n",
       "  'page_idx': 65},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/20a012b0d697295b57eefc0c33354c563af7ba33d3735ad7cf80752c0da351cb.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>3,</td></tr><tr><td>...</td></tr><tr><td>]</td></tr></table>',\n",
       "  'page_idx': 66,\n",
       "  'outline': [67, 83, 543, 126]},\n",
       " {'type': 'text',\n",
       "  'text': 'There are two ways to work with custom datasets.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'text', 'text': 'online conversion', 'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': 'You can write a new Dataset class inherited from CustomDataset, and overwrite two methods load_annotations(self, ann_file) and get_ann_info(self, idx), like CocoDataset and VOCDataset.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'text', 'text': 'offline conversion', 'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': 'You can convert the annotation format to the expected format above and save it to a pickle or json file, like pascal_voc.py. Then you can simply use CustomDataset.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': '9.1.3 An example of customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume the annotation is in a new format in text files. The bounding boxes annotations are stored in text file annotation.txt as the following',\n",
       "  'page_idx': 66},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c8e6dbace0eeb3b576a6cc13798520020ca329c9f262f85c69a257d10a5f1c0d.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>#</td></tr><tr><td>0000001.jpg</td></tr><tr><td>1280 720</td></tr><tr><td>2</td></tr><tr><td>10 20 40 60 1</td></tr><tr><td>20 40 50 60 2</td></tr><tr><td>#</td></tr><tr><td>0000002.jpg</td></tr><tr><td>1280 720</td></tr><tr><td>3</td></tr><tr><td>50 20 40 60 2</td></tr><tr><td>20 40 30 45 2</td></tr><tr><td>30 40 50 60 3</td></tr></table>',\n",
       "  'page_idx': 66,\n",
       "  'outline': [69, 331, 543, 493]},\n",
       " {'type': 'text',\n",
       "  'text': 'We can create a new dataset in mmdet/datasets/my_dataset.py to load the data.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/5c3debd4d34e51fe0dddd8ca00bcc2030d5c9762986b1c36db6c506b661613e9.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': \"<table><tr><td>import mmcv\\nimport numpy as np</td></tr></table>\\n\\nfrom .builder import DATASETS\\nfrom .custom import CustomDataset\\n\\n@DATASETS.register_module()\\nclass MyDataset(CustomDataset):\\n    CLASSES = ('person', 'bicycle', 'car', 'motorcycle')\\n    def load_annotations(self, ann_file):\\n        ann_list = mmcv.list_from_file(ann_file)<nl>\",\n",
       "  'page_idx': 66,\n",
       "  'outline': [69, 521, 544, 705]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': \"data_infos = [] for i, ann_line in enumerate(ann_list): if ann_line  $! =$  '#': continue img_shape  $=$  ann_list[i  $^+$  2].split(' ') width  $=$  int(img_shape[0]) height  $=$  int(img_shape[1]) bbox_number  $=$  int(ann_list[i + 3]) anns  $=$  ann_line.split(' ') bboxes  $=$  [] labels  $=$  [] for anns in ann_list[i  $^+$  4:i  $^+$  4+ bbox_number]: bboxes.append([float(ann) for ann in anns[:4]]) labels.append(int(anns[4])) data_infos.append( dict( filename  $=$  ann_list[i + 1], width  $\\\\equiv$  width, height  $=$  height, ann  $\\\\equiv$  dict( bboxes  $=$  np.array(bboxes).astype(np.float32), labels  $=$  np.array(labels).astype(np.int64)) ) return data_infos def get_ann_info(self, idx): return self.data_infos[idx]['ann']\",\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': 'Then in the config, to use MyDataset you can modify the config as the following',\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset_A_train = dict( type='MyDataset', ann_file = 'image_list.txt', pipeline=train_pipeline)\",\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': '9.2 Customize datasets by dataset wrappers',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection also supports many dataset wrappers to mix the dataset or modify the dataset distribution for training. Currently it supports to three dataset wrappers as below:',\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': '- RepeatDataset: simply repeat the whole dataset.- ClassBalancedDataset: repeat dataset in a class balanced manner.- ConcatDataset: concat datasets.',\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': '9.2.1 Repeat dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"We use RepeatDataset as wrapper to repeat the dataset. For example, suppose the original dataset is Dataset_A, to repeat it, the config looks like the following dataset_A_train  $=$  dict( type  $=$  'RepeatDataset', times  $= \\\\mathbb{N}$  dataset=dict( # This is the original config of Dataset_A type  $=$  'Dataset_A', pipeline  $=$  train_pipeline ) )\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text', 'text': '', 'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '9.2.2 Class balanced dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"We use ClassBalancedDataset as wrapper to repeat the dataset based on category frequency. The dataset to repeat needs to instantiate function self. get_cat_ids(idx) to support ClassBalancedDataset. For example, to repeat Dataset_A with oversample_thr  $= 1e - 3$  , the config looks like the following dataset_A_train  $=$  dict( type  $=$  'ClassBalancedDataset', oversample_thr  $= 1e - 3$  dataset=dict( # This is the original config of Dataset_A type  $=$  'Dataset_A', pipeline  $=$  train_pipeline ) )\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text', 'text': '', 'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'You may refer to source code for details.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '9.2.3 Concatenate dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'There are three ways to concatenate the dataset.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '1. If the datasets you want to concatenate are in the same type with different annotation files, you can concatenate the dataset config like the following.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset_A_train  $=$  dict( type  $=$  'Dataset_A', annn_file  $=$  ['anno_file_1', 'anno_file_2'], pipeline  $=$  train_pipeline )\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'If the concatenated dataset is used for test or evaluation, this manner supports to evaluate each dataset separately. To test the concatenated datasets as a whole, you can set separate_eval=False as below.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset_A_train  $=$  dict( type  $=$  'Dataset_A', annn_file  $=$  ['anno_file_1', 'anno_file_2'],\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '9.2. Customize datasets by dataset wrappers',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'separate_eval=False, pipeline=train_pipeline )',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': '2. In case the dataset you want to concatenate is different, you can concatenate the dataset configs like the following.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c1ab06f38869679405cab029e0c97b0fc5fb0ee74dcf3d39979523c7f5c655af.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>dataset_A_train = dict()\\ndataset_B_train = dict()\\ndata = dict(\\n  imgs_per_gpu=2,\\n  workers_per_gpu=2,\\n  train = [\\n    dataset_A_train,\\n    dataset_B_train\\n  ],\\n  val = dataset_A_val,\\n  test = dataset_A_test\\n)</td></tr></table>',\n",
       "  'page_idx': 69,\n",
       "  'outline': [93, 153, 543, 315]},\n",
       " {'type': 'text',\n",
       "  'text': 'If the concatenated dataset is used for test or evaluation, this manner also supports to evaluate each dataset separately.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': '3. We also support to define ConcatDataset explicitly as the following.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/bd4098ccd76c47a3caa96f65e58499483d3c0cc2a55a4119dbb4624d55c5f62b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>dataset_A_val = dict()\\ndataset_B_val = dict()\\ndata = dict(\\n  imgs_per_gpu=2,\\n  workers_per_gpu=2,\\n  train=dataset_A_train,\\n  val=dict(\\n    type=&#x27;ConcatDataset&#x27;,\\n    datasets=[dataset_A_val, dataset_B_val],\\n    separate_eval=False))</td></tr></table>',\n",
       "  'page_idx': 69,\n",
       "  'outline': [93, 373, 543, 510]},\n",
       " {'type': 'text',\n",
       "  'text': 'This manner allows users to evaluate all the datasets as a single one by setting separate_eval=False.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': '1. The option separate_eval=False assumes the datasets use self.data_infos during evaluation. Therefore, COCO datasets do not support this behavior since COCO datasets do not fully rely on self.data_infos for evaluation. Combining different types of datasets and evaluating them as a whole is not tested thus is not suggested. \\n2. Evaluating ClassBalancedDataset and RepeatDataset is not supported thus evaluating concatenated datasets of these types is also not supported.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': 'A more complex example that repeats Dataset_A and Dataset_B by N and M times, respectively, and then concatenates the repeated datasets is as the following.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/61bc0ee014640ff134f49ebaeda0ffb16bae1904e1f10febfd2e7a59f0a7c173.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>dataset_A_train = dict(\\n  type=&#x27;RepeatDataset&#x27;,\\n  times=N,</td></tr></table>',\n",
       "  'page_idx': 69,\n",
       "  'outline': [69, 670, 543, 709]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset=dict( type  $=$  'Dataset_A', pipeline  $=$  train_pipeline ) ) dataset_A_val  $=$  dict( pipeline  $=$  test_pipeline ) dataset_A_test  $=$  dict( pipeline  $=$  test_pipeline ) dataset_B_train  $=$  dict( type  $=$  'RepeatDataset', times  $= \\\\mathbb{M}$  dataset  $=$  dict( type  $=$  'Dataset_B', pipeline  $=$  train_pipeline ) ) data  $=$  dict( imgs_per_gpu  $= 2$  , workers_per_gpu  $= 2$  train  $=$  [ dataset_A_train, dataset_B_train ], val  $=$  dataset_A_val, test  $=$  dataset_A_test )\",\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': '9.3 Modify Dataset Classes',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': 'With existing dataset types, we can modify the class names of them to train subset of the annotations. For example, if you want to train only three classes of the current dataset, you can modify the classes of dataset. The dataset will filter out the ground truth boxes of other classes automatically.',\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': \"classes  $=$  ('person', 'bicycle', 'car') data  $=$  dict( train  $=$  dict(classes  $=$  classes), val  $=$  dict(classes  $=$  classes), test  $=$  dict(classes  $=$  classes))\",\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection V2.0 also supports to read the classes from a file, which is common in real applications. For example, assume the classes.txt contains the name of classes as the following.',\n",
       "  'page_idx': 70},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/d9dc99487a7f725296447ae6d2e54f8fe9857ec99369cc3f139c754dcd03068a.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>person</td></tr><tr><td>bicycle</td></tr><tr><td>car</td></tr></table>',\n",
       "  'page_idx': 71,\n",
       "  'outline': [69, 76, 543, 118]},\n",
       " {'type': 'text',\n",
       "  'text': 'Users can set the classes as a file path, the dataset will load it and convert it to a list automatically.',\n",
       "  'page_idx': 71},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3bf05e44743abd0ee3239213cd3e835d3822260e00f7f006c6e2d1afda3f9e32.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>classes = &#x27;path/to/classes.txt&#x27;</td></tr><tr><td>data = dict(</td></tr><tr><td>train=dict(classes=classes),</td></tr><tr><td>val=dict(classes=classes),</td></tr><tr><td>test=dict(classes=classes))</td></tr></table>',\n",
       "  'page_idx': 71,\n",
       "  'outline': [69, 145, 543, 212]},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 71},\n",
       " {'type': 'text',\n",
       "  'text': 'Note:- Before MMDetection v2.5.0, the dataset will filter out the empty GT images automatically if the classes are set and there is no way to disable that through config. This is an undesirable behavior and introduces confusion because if the classes are not set, the dataset only filter the empty GT images when filter_empty_gt=True and test_mode=False. After MMDetection v2.5.0, we decouple the image filtering process and the classes modification, i.e., the dataset will only filter empty GT images when filter_empty_gt=True and test_mode=False, no matter whether the classes are set. Thus, setting the classes only influences the annotations of classes used for training and users could decide whether to filter empty GT images by themselves.- Since the middle format only has box labels and does not contain the class names, when using CustomDataset, users cannot filter out the empty GT images through configs but only do this offline.- Please remember to modify the num_classes in the head when specifying classes in dataset. We implemented NumClassCheckHook to check whether the numbers are consistent since v2.9.0(after PR#4508).- The features for setting dataset classes and dataset filtering will be refactored to be more user- friendly in the future (depends on the progress).',\n",
       "  'page_idx': 71},\n",
       " {'type': 'text',\n",
       "  'text': '9.4 COCO Panoptic Dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 71},\n",
       " {'type': 'text',\n",
       "  'text': 'Now we support COCO Panoptic Dataset, the format of panoptic annotations is different from COCO format. Both the foreground and the background will exist in the annotation file. The annotation json files in COCO Panoptic format has the following necessary keys:',\n",
       "  'page_idx': 71},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/e79b5d426f3ad37c701fc9542afd8493d99c956516fb2969eeb4fd997a1254db.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>&#x27;images&#x27;: [\\n    {\\n        &#x27;file_name&#x27;: &#x27;0000000001268.jpg&#x27;,\\n        &#x27;height&#x27;: 427,\\n        &#x27;width&#x27;: 640,\\n        &#x27;id&#x27;: 1268\\n    },\\n    ...</td></tr><tr><td>&#x27;]</td></tr><tr><td>&#x27;annotations&#x27;: [\\n    {\\n        &#x27;filename&#x27;: &#x27;0000000001268.jpg&#x27;,\\n        &#x27;image_id&#x27;: 1268,\\n        &#x27;segments_info&#x27;: [\\n            {\\n                &#x27;image_id&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n            }\\n        &#x27;image_width&#x27;:\\n        &#x27;image_height&#x27;:\\n        &#x27;image_width&#x27;:\\n        &#x27;image_height&#x27;:\\n        &#x27;image_width&#x27;:\\n        &#x27;image_height&#x27;:\\n        &#x27;image_width&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:</td></tr></table>',\n",
       "  'page_idx': 71,\n",
       "  'outline': [67, 510, 544, 708]},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': \"'id':8345037, # One- to- one correspondence with the id in the annotation. map. 'category_id': 51, 'iscrowd': 0, 'bbox': (x1, y1, w, h), # The bbox of the background is the outer.  $\\\\rightarrow$  rectangle of its mask. 'area': 24315 }, 1 } 1 'categories': [ # including both foreground categories and background categories {'id': 0, 'name': 'person'}, 1\",\n",
       "  'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': 'Moreover, the seg_prefix must be set to the path of the panoptic annotation images.',\n",
       "  'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': \"data  $=$  dict( type  $\\\\coloneqq$  'CocoPanopticDataset', train  $=$  dict( seg_prefix  $=$  'path/to/your/train/panoptic/image_annotation_data' ), val  $=$  dict( seg_prefix  $=$  'path/to/your/train/panoptic/image_annotation_data' ) )\",\n",
       "  'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 9. Tutorial 2: Customize Datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 73},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 3: CUSTOMIZE DATA PIPELINES',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': '10.1 Design of Data pipelines',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': \"Following typical conventions, we use Dataset and DataLoader for data loading with multiple workers. Dataset returns a dict of data items corresponding the arguments of models' forward method. Since the data in object detection may not be the same size (image size, gt bbox size, etc.), we introduce a new DataContainer type in MMCV to help collect and distribute data of different size. See here for more details.\",\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'The data preparation pipeline and the dataset is decomposed. Usually a dataset defines how to process the annotations and a data pipeline defines all the steps to prepare a data dict. A pipeline consists of a sequence of operations. Each operation takes a dict as input and also output a dict for the next transform.',\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'We present a classical pipeline in the following figure. The blue blocks are pipeline operations. With the pipeline going on, each operator can add new keys (marked as green) to the result dict or update the existing keys (marked as orange).',\n",
       "  'page_idx': 74},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cafc4b1c111689ef52c80e4e47a15a2cae0193513315cb0aec2168e66b29353b.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 74,\n",
       "  'outline': [70, 381, 574, 530]},\n",
       " {'type': 'text', 'text': 'figure', 'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'The operations are categorized into data loading, pre- processing, formatting and test- time augmentation.',\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'Here is a pipeline example for Faster R- CNN.',\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': \"img_norm_cfg  $=$  dict(mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std  $\\\\equiv$  [58.395, 57.12, 57.375], to_rgb  $\\\\equiv$  True) train_pipeline  $=$  [ dict(type  $=$  'LoadImageFromFile'), dict(type  $=$  'LoadAnnotations', with_bbox  $\\\\equiv$  True), dict(type  $=$  'Resize', img_scale  $=$  (1333, 800), keep_ratio  $\\\\equiv$  True), dict(type  $=$  'RandomFlip', flip_ratio  $= 0$  .5), dict(type  $=$  'Normalize', \\\\*\\\\*img_norm_cfg), dict(type  $=$  'Pad', size_divisor  $= 32$  - dict(type  $=$  'DefaultFormatBundle'),\",\n",
       "  'page_idx': 74},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': \"dict(type  $=$  'Collect', keys  $=$  ['img', 'gt_bboxes', 'gt_labels']), ] test_pipeline  $=$  [ dict(type  $=$  'LoadImageFromFile'), dict( type  $=$  'MultiScaleFlipAug', img_scale  $=$  (1333, 800), flip  $\\\\equiv$  False, transforms  $=$  [ dict(type  $=$  'Resize', keep_ratio  $=$  True), dict(type  $=$  'RandomFlip'), dict(type  $=$  'Normalize', \\\\*\\\\*img_norm_cfg), dict(type  $=$  'Pad', size_divisor  $= 32$  1 dict(type  $=$  'ImageToTensor', keys  $=$  ['img']), dict(type  $=$  'Collect', keys  $=$  ['img']), ] ]\",\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': 'For each operation, we list the related dict fields that are added/updated/removed.',\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.1 Data loading',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 75},\n",
       " {'type': 'text', 'text': 'LoadImageFromFile', 'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '- add: img, img_shape, ori_shapeLoadAnnotations- add: gt_bboxes, gt_bboxes_ignore, gt_labels, gt_masks, gt_semks, gt_semantic_seg, bbox_fields, mask_fieldsLoadProposals- add: proposals',\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.2 Pre-processing',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 75},\n",
       " {'type': 'text', 'text': 'Resize', 'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '- add: scale, scale_idx, pad_shape, scale_factor, keep_ratio- update: img, img_shape, *bbox_fields, *mask_fields, *seg_fieldsRandomFlip- add: flip- update: img, *bbox_fields, *mask_fields, *seg_fieldsPad- add: pad_fixed_size, pad_size_divisor- update: img, pad_shape, *mask_fields, *seg_fieldsRandomCrop- update: img, pad_shape, gt_bboxes, gt_labels, gt_masks, *bbox_fieldsNormalize',\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '- add: img_norm_cfg- update: imgSegRescale- update: gt_semantic_segPhotoMetricDistortion- update: imgExpand- update: img, gt_bboxesMinIoURandomCrop- update: img, gt_bboxes, gt_labelsCorrupt- update: img',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.3 Formatting',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 76},\n",
       " {'type': 'text', 'text': 'ToTensor', 'text_level': 1, 'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '- update: specified by keys.ImageToTensor- update: specified by keys.',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text', 'text': 'Transpose', 'text_level': 1, 'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '- update: specified by keys.ToDataContainer- update: specified by fields.',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': 'DefaultFormatBundle',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '- update: img, proposals, gt_bboxes, gt_bboxes_ignore, gt_labels, gt_masks, gt_semantic_segCollect- add: img_meta (the keys of img_meta is specified by meta_keys)- remove: all other keys except for those specified by keys',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.4 Test time augmentation',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 76},\n",
       " {'type': 'text', 'text': 'MultiScaleFlipAug', 'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '10.2 Extend and use custom pipelines',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': '1. Write a new pipeline in a file, e.g., in my_pipeline.py. It takes a dict as input and returns a dict.',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': 'import random from mmdet.datasets import PIPELINES @PIPELINES register_module() class MyTransform: \"Add your transform Args: p (float): Probability of shifts.Default 0.5. def__init__(self,  $\\\\scriptstyle{\\\\vec{p} = \\\\mathbb{0},5}$  : self.  $\\\\textbf{p} = \\\\textbf{p}$  def__call__(self, results): if random.random()  $\\\\gimel$  self.p: results[\\'dummy\\']  $=$  True return results',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': '2. Import and use the pipeline in your config file. Make sure the import is relative to where your train script is located.',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': \"custom_imports  $=$  dict imports  $\\\\coloneqq$  ['path.to_my_pipeline'], allow_failed_imports  $\\\\coloneqq$  False) img_norm_cfg  $=$  dict( mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True) train_pipeline  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict(type  $\\\\coloneqq$  'LoadAnnotations', with_bbox  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'Resize', img_scale=(1333, 800), keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip', flip_ratio  $= 0.5$  - dict(type  $\\\\coloneqq$  'Normalize', \\\\*\\\\*img_norm_cfg), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  'MyTransform',  $\\\\scriptstyle{\\\\vec{p} = \\\\mathbb{0},2}$  - dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), dict(type  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img', 'gt_bboxes', 'gt_labels']), ]\",\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': '3. Visualize the output of your augmentation pipeline',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': 'To visualize the output of your augmentation pipeline, tools/misc/browse_dataset.py can help the user to browse a detection dataset (both images and bounding box annotations) visually, or save the image to a designated directory. More details can refer to useful_tools',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 4: CUSTOMIZE MODELS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'We basically categorize model components into 5 types.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'backbone: usually an FCN network to extract feature maps, e.g., ResNet, MobileNet. neck: the component between backbones and heads, e.g., FPN, PAPPN. head: the component for specific tasks, e.g., bbox prediction and mask prediction. roi extractor: the part for extracting RoI features from feature maps, e.g., RoI Align. loss: the component in head for calculating losses, e.g., FocalLoss, L1Loss, and GHMLoss.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '11.1 Develop new components',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.1 Add a new backbone',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'Here we show how to develop new components with an example of MobileNet.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define a new backbone (e.g. MobileNet)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'Create a new file mmdet/models/backbones/mobilenet. py.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch.nn as nn from .builder import BACKBONES @BACKBONES.register_module() class MobileNet(nn.Module): def __init__(self, angl, arg2): pass def forward(self, x): # should return a tuple pass',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '2. Import the module',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': 'You can either add the following line to mmdet/models/backbones/__init__.py',\n",
       "  'page_idx': 79},\n",
       " {'type': 'text', 'text': 'from .mobilenet import MobileNet', 'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': 'or alternatively add to the config file to avoid modifying the original code.',\n",
       "  'page_idx': 79},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/735bf766349aaa73c8e301984f80effdbc7e2b9657d3d5923408bcb422faf937.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_imports = dict(</td></tr><tr><td>imports=[&#x27;mmdet.models.backends.mobilenet&#x27;],\\nallow_failed_imports=False)</td></tr></table>',\n",
       "  'page_idx': 79,\n",
       "  'outline': [69, 162, 543, 204]},\n",
       " {'type': 'text', 'text': '', 'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': '3. Use the backbone in your config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/08968e25327e282c42608377e3eeb2c9e821af7339e9c484602d38078b933cb2.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>model = dict(</td></tr><tr><td>...</td></tr><tr><td>backbone=dict(</td></tr><tr><td>type=&#x27;MobileNet&#x27;,</td></tr><tr><td>arg1=xxx,</td></tr><tr><td>arg2=xxx),</td></tr><tr><td>...</td></tr></table>',\n",
       "  'page_idx': 79,\n",
       "  'outline': [69, 268, 543, 360]},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.2 Add new necks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define a neck (e.g. PAFPN)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': 'Create a new file mmdet/models/necks/pafpn.py.',\n",
       "  'page_idx': 79},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/63037358f3a66af07db17701f325523f5c4d134496c4a82cc0bc5829b4c48f00.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from .builder import NECKS</td></tr><tr><td>@NECKS.register_module()</td></tr><tr><td>class PAFPN(nn.Module):</td></tr><tr><td>def __init__(self, \\n        in_channels, \\n        out_channels, \\n        num_outs, \\n        start_level=0, \\n        end_level=-1, \\n        add_extra_conv=False):</td></tr><tr><td>pass</td></tr><tr><td>def forward(self, inputs):</td></tr><tr><td># implementation is ignored</td></tr><tr><td>pass</td></tr></table>',\n",
       "  'page_idx': 79,\n",
       "  'outline': [67, 452, 544, 662]},\n",
       " {'type': 'text',\n",
       "  'text': '2. Import the module',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'You can either add the following line to mmdet/models/necks/__init__.py,',\n",
       "  'page_idx': 80},\n",
       " {'type': 'text', 'text': 'from .pafpn import PAFPN', 'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'or alternatively add to the config file and avoid modifying the original code.',\n",
       "  'page_idx': 80},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/bc46fd6327d1b2f260225f5f109fc837fdcf2dba682883faec6e50cd25147bdc.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_imports = dict(</td></tr><tr><td>imports=[&#x27;mmdet.models.necks.pafpn.py&#x27;],\\nallow_failed_imports=False)</td></tr></table>',\n",
       "  'page_idx': 80,\n",
       "  'outline': [69, 162, 543, 202]},\n",
       " {'type': 'text', 'text': '', 'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': '3. Modify the config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': \"neck=dict( type  $\\\\coloneqq$  'PAFPN', in_channels  $=$  [256, 512, 1024, 2048], out_channels  $= 256$  num_outs  $= 5$\",\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.3 Add new heads',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'Here we show how to develop a new head with the example of Double Head R- CNN as the following.',\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'First, add a new bbox head in mmdet/models/roi_heads/bbox_heads/double_bbox_head.py. Double Head RCNN implements a new bbox head for object detection. To implement a bbox head, basically we need to implement three functions of the new module as the following.',\n",
       "  'page_idx': 80},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/d646ec2ee72e1bbe5701a4a8abad0302ea0d728bf252e437216ac6a6b479e6e6.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from mmdet.models.builder import HEADS\\nfrom .bbox_head import BBoxHead</td></tr></table>\\n\\n@HEADS.register_module()\\nclass DoubleConvFCBBoxHead(BBoxHead):\\n    r\"\"\"BBox head used in Double-Head R-CNN\\n\\n    /-&gt; cls\\n    /-&gt; shared convs -&gt; \\n    /-&gt; reg\\n    roi features\\n    /-&gt; cls\\n    /-&gt; shared fc   -&gt; \\n    /-&gt; reg\\n    \"\"\" # noqa: W605\\n\\ndef __init__(self,\\n    num_conv=0,\\n    num_fcs=0,\\n    conv_out_channels=1024,\\n    fc_out_channels=1024,\\n    conv_cfg=None,<nl>',\n",
       "  'page_idx': 80,\n",
       "  'outline': [67, 444, 543, 712]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 80},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/49e8f405080173e77228aa824c94842e3125ff2a10de6ac8df5381355461ead0.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>norm_cfg=dict(type=&#x27;BN&#x27;),\\n    **kwargs):\\nkwargs.setdefault(&#x27;with_avg_pool&#x27;, True)\\nsuper(DoubleConvFCBBoxHead, self).__init__(**kwargs)</td></tr><tr><td>def forward(self, x_cls, x_reg):</td></tr></table>',\n",
       "  'page_idx': 81,\n",
       "  'outline': [67, 78, 543, 175]},\n",
       " {'type': 'text',\n",
       "  'text': 'Second, implement a new RoI Head if it is necessary. We plan to inherit the new DoubleHeadRoIHead from StandardRoIHead. We can find that a StandardRoIHead already implements the following functions.',\n",
       "  'page_idx': 81},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/b5775a03a57e203b60b2cf278c77eaf33d25fe3970550f90925037cf1e41b6f3.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>import torch</td></tr><tr><td>from mmdet.core import bbox2result, bbox2roi, build_assigner, build sampler</td></tr><tr><td>from .builder import HEADS, build_head, build_roi_extractor</td></tr><tr><td>from .base_roi_head import BaseRoIHead</td></tr><tr><td>from .test_mixins import BBoxTestMixin, MaskTestMixin</td></tr><tr><td>@HEADS.register_module()</td></tr><tr><td>class StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\\n    &quot;&quot;&quot;Simplest base roi head including one bbox head and one mask head.\\n    &quot;&quot;&quot;\\n    def init_assigner_sampler(self):\\n    def init_bbox_head(self, bbox_roi_extractor, bbox_head):\\n    def init_mask_head(self, mask_roi_extractor, mask_head):\\n    def forward_dummy(self, x, proposals):\\n    def forward_train(self,\\n        x,\\n        img_metas,\\n        proposal_list,\\n        gt_bboxes,\\n        gt_labels,\\n        gt_bboxes_ignore=None,\\n        gt_masks=None):\\n    def _bbox_forward(self, x, rois):\\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\\n        img_metas):\\n    def _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\\n        img_metas):\\n    def _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None):</td></tr></table>',\n",
       "  'page_idx': 81,\n",
       "  'outline': [69, 213, 543, 709]},\n",
       " {'type': 'text',\n",
       "  'text': 'def simple_test(self, x, proposal_list, img_metas, proposals=None, rescale=False):    \"\"\"Test without augmentation.\"\"\"',\n",
       "  'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': \"Double Head's modification is mainly in the bbox_forward logic, and it inherits other logics from the StandardRoIHead. In the mmdet/models/roi_heads/double_roi_head.py, we implement the new RoI Head as the following:\",\n",
       "  'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': 'from ..builder import HEADS from .standard_roi_head import StandardRoIHead @HEADS.register_module() class DoubleHeadRoIHead(StandardRoIHead): \"\"\"RoI head for Double Head RCNN https://arxiv.ora/abs/1904.06493 def __init__(self, req_roi_scale_factor, **kwargs): super(DoubleHeadRoIHead, self).__init__(**kwargs) self.reg_roi_scale_factor = req_roi_scale_factor def _bbox_forward(self, x, rois): bbox_cls_feats = self.bbox_roi_extractor(x[:self.bbox_roi_extractor.num_inputs], rois) bbox_reg_feats = self.bbox_roi_extractor.num_inputs], x[:self.bbox_roi_extractor.num_inputs], rois, roi_scale_factor=self.reg_roi_scale_factor) if self.with_shared_head: bbox_cls_feats = self.shared_head(bbox_cls_feats) bbox_reg_feats = self.shared_head(bbox_reg_feats) cls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats) bbox_results = dict( cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_cls_feats) return bbox_results',\n",
       "  'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': 'Last, the users need to add the module in mmdet/models/bbox_heads/__init__.py and mmdet/models/roi_heads/__init__.py thus the corresponding registry could find and load them.',\n",
       "  'page_idx': 82},\n",
       " {'type': 'text', 'text': 'Alternatively, the users can add', 'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': \"custom_imports=dict(    imports=['mmdet.models.roi_heads.double_roi_head', 'mmdet.models.bbox_heads.double_  $\\\\rightarrow$  bbox_head'])\",\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'to the config file and achieve the same goal.',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'The config file of Double Head R- CNN is as the following',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': \"base_  $=$  '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' model  $=$  dict( roi_head  $=$  dict( type  $=$  'DoubleHeadRoIHead', reg_roi_scale_factor  $= 1,3$  bbox_head  $=$  dict( delete  $=$  True, type  $=$  'DoubleConvFCBBoxHead', num_conv  $= 4$  num_fcs  $= 2$  in_channels  $= 256$  conv_out_channels  $= 1024$  fc_out_channels  $= 1024$  roi_feat_size  $= 7$  num_classes  $= 80$  bboxcoder  $=$  dict( type  $=$  'DeltaXYwHBBoxCoder', target_means  $=$  [0. , 0. , 0. ., target_std  $=$  [0.1, 0.1, 0.2, 0.2]), reg_class_agnostic  $=$  False, loss_cls  $=$  dict( type  $=$  'CrossEntropyLoss', use_sigmoid  $=$  False, loss_weight  $= 2.0$  loss bbox  $=$  dict(type  $=$  'SmoothLLoss', beta  $= 1.0$  , loss_weight  $= 2.0$  )\",\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'Since MMDetection 2.0, the config system supports to inherit configs such that the users can focus on the modification. The Double Head R- CNN mainly uses a new DoubleHeadRoIHead and a new DoubleConvFCBBoxHead, the arguments are set according to the __init__ function of each module.',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.4 Add new loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume you want to add a new loss as MyLoss, for bounding box regression. To add a new loss function, the users need implement it in mmdet/models/losses/my_loss.py. The decorator weighted_loss enable the loss to be weighted for each element.',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch import torch.nn as nn from ..builder import LOSSES from .utils import weighted_loss @weighted_loss def my_loss(pred, target): assert pred_size()  $= =$  target.size() and target.numel() > 0 loss  $=$  torch.abs(pred - target) return loss',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': '@LOSSES.register_module() class MyLoss(nn.Module):',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': \"def __init__(self, reduction  $\\\\coloneqq$  'mean', loss_weight  $\\\\coloneqq \\\\mathbb{1}$  .0): super(MyLoss, self).__init__() self.reduction  $=$  reduction self.loss_weight  $=$  loss_weight def forward(self, pred, target, weight  $\\\\coloneqq$  None, avg_factor  $\\\\coloneqq$  None, reduction\\toverride  $\\\\coloneqq$  None): assert reduction override in (None, 'none', 'mean', 'sum') reduction  $=$  ( reduction override if reduction override else self.reduction) loss_bbox  $=$  self.loss_weight \\\\* my_loss( pred, target, weight, reduction  $\\\\coloneqq$  reduction, avg_factor  $\\\\coloneqq$  avg_factor) return loss_bbox\",\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'Then the users need to add it in the mmdet/models/losses/__init__.py.',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'from .my_loss import MyLoss, my_loss',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': \"Alternatively, you can add custom_imports=dict( imports=['mmdet.models.losses.my_loss'])\",\n",
       "  'page_idx': 84},\n",
       " {'type': 'text', 'text': '', 'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'to the config file and achieve the same goal.',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'To use it, modify the loss_xxx field. Since MyLoss is for regression, you need to modify the loss_bbox field in the head.',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': \"loss_bbox=dict(type='MyLoss', loss_weight=1.0))\",\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 11. Tutorial 4: Customize Models',\n",
       "  'page_idx': 85},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '12.1 Customize optimization settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.1 Customize optimizer supported by Pytorch',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'We already support to use all the optimizers implemented by PyTorch, and the only modification is to change the optimizer field of config files. For example, if you want to use ADAM (note that the performance could drop a lot), the modification could be as the following.',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': \"optimizer  $=$  dict(type  $=$  'Adam',  $\\\\mathbf{lr} = \\\\mathbb{0}$  .0003, weight_decay  $= 0$  .0001)\",\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'To modify the learning rate of the model, the users only need to modify the lr in the config of optimizer. The users can directly set arguments following the API doc of PyTorch.',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.2 Customize self-implemented optimizer',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define a new optimizer',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'A customized optimizer could be defined as following.',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume you want to add a optimizer named MyOptimizer, which has arguments a, b, and c. You need to create a new directory named mmdet/core/optimizer. And then implement the new optimizer in a file, e.g., in mmdet/core/ optimizer/my_optimizer.py:',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'from .registry import OPTIMIZERS from torch.optim import Optimizer @OPTIMIZERS.register_module() class MyOptimizer(Optimizer): def init__self,a,b,c)',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '2. Add the optimizer to registry',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'To find the above module defined above, this module should be imported into the main namespace at first. There are two options to achieve it.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Modify mmdet/core/optimizer/__init__.py to import it.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'The newly defined module should be imported in mmdet/core/optimizer/__init__.py so that the registry will find the new module and add it:',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'from .my_optimizer import MyOptimizer',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': \"Use custom_imports in the config to manually import it custom_imports  $=$  dict imports  $=$  ['mmdet.core.optimizer.my_optimizer'], allow_failed_  $\\\\rightarrow$  imports  $=$  False)\",\n",
       "  'page_idx': 87},\n",
       " {'type': 'text', 'text': '', 'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'The module mmdet.core.optimizer.my_optimizer will be imported at the beginning of the program and the class MyOptimizer is then automatically registered. Note that only the package containing the class MyOptimizer should be imported. mmdet.core.optimizer.my_optimizer.MyOptimizer cannot be imported directly.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Actually users can use a totally different file directory structure using this importing method, as long as the module root can be located in PYTHONPATH.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': '3. Specify the optimizer in the config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Then you can use MyOptimizer in optimizer field of config files. In the configs, the optimizers are defined by the field optimizer like the following:',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': \"optimizer  $=$  dict(type  $=$  'SGD',  $\\\\mathtt{lr} = \\\\mathtt{0}$  .02, momentum  $= 0$  .9, weight_decay  $= 0$  .00001)\",\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': \"To use your own optimizer, the field can be changed to optimizer  $=$  dict(type  $=$  'MyOptimizer',  $\\\\exists = \\\\exists$  value, b=b_value,  $\\\\mathtt{c} = \\\\mathtt{c}$  value)\",\n",
       "  'page_idx': 87},\n",
       " {'type': 'text', 'text': '', 'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.3 Customize optimizer constructor',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Some models may have some parameter- specific settings for optimization, e.g. weight decay for BatchNorm layers. The users can do those fine- grained parameter tuning through customizing optimizer constructor:',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'from mmcv.utils import build_from_cfg from mmcv.runner.optimizer import OPTIMIZER_BUILDERS, OPTIMIZERS from mmdet.utils import get_root_logger from .my_optimizer import MyOptimizer @OPTIMIZER_BUILDERS.register_module() class MyOptimizerConstructor(object): def _init__(self, optimizer_cfg, paramwise_cfg=None): def _call__(self, model):',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'The default optimizer constructor is implemented here, which could also serve as a template for new optimizer constructor.',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.4 Additional settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'Tricks not implemented by the optimizer should be implemented through optimizer constructor (e.g., set parameterwise learning rates) or hooks. We list some common settings that could stabilize the training or accelerate the training. Feel free to create PR, issue for more settings.',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': '- Use gradient clip to stabilize training: Some models need gradient clip to clip the gradients to stabilize the training process. An example is as below:',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'If your config inherits the base config which already sets the optimizer_config, you might need _delete_=True to override the unnecessary settings. See the config documentation for more details.',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': \"- Use momentum schedule to accelerate model convergence: We support momentum scheduler to modify model's momentum according to learning rate, which could make the model converge in a faster way. Momentum scheduler is usually used with LR scheduler, for example, the following config is used in 3D detection to accelerate convergence. For more details, please refer to the implementation of CyclicLRUpdater and CyclicMomentumUpdater.\",\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': \"lr_config = dict(policy='cyclic', target_ratio=(10, 1e- 4), cyclic_times=1, step_ratio_up=0.4, ) momentum_config = dict(policy='cyclic', target_ratio=(0.85 / 0.95, 1), cyclic_times=1, step_ratio_up=0.4, )\",\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': '12.2 Customize training schedules',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'By default we use step learning rate with 1x schedule, this calls StepLRHook in MMCV. We support many other learning rate schedule here, such as CosineAnnealing and Poly schedule. Here are some examples',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': \"- Poly schedule: lr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=False)\",\n",
       "  'page_idx': 88},\n",
       " {'type': 'text', 'text': '- ConsineAnnealing schedule:', 'page_idx': 88},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/08bca5394a4d7bdfd1fd80e2ce829e9617df705a9959633d810386f7694a9456.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>lr_config = dict(\\n  policy=&#x27;CosineAnnealing&#x27;,\\n  warmup=&#x27;linear&#x27;,\\n  warmup_liters=10000,\\n  warmup_ratio=1.0 / 10,\\n  min_lr_ratio=1e-5)</td></tr></table>',\n",
       "  'page_idx': 89,\n",
       "  'outline': [93, 76, 543, 154]},\n",
       " {'type': 'text',\n",
       "  'text': '12.3 Customize workflow',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': \"Workflow is a list of (phase, epochs) to specify the running order and epochs. By default it is set to be workflow  $=$  [('train', 1)]\",\n",
       "  'page_idx': 89},\n",
       " {'type': 'text', 'text': '', 'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': 'which means running 1 epoch for training. Sometimes user may want to check some metrics (e.g. loss, accuracy) about the model on the validate set. In such case, we can set the workflow as so that 1 epoch for training and 1 epoch for validation will be run iteratively.',\n",
       "  'page_idx': 89},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/65545831fee3f3a119b54b34777c6e869236b51b4d25552966a81f0039509dfc.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>[&#x27;train&#x27;, &#x27;1&#x27;, &#x27;val&#x27;, &#x27;1&#x27;]</td></tr></table>',\n",
       "  'page_idx': 89,\n",
       "  'outline': [69, 289, 543, 306]},\n",
       " {'type': 'text', 'text': '', 'page_idx': 89},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': \"1. The parameters of model will not be updated during val epoch. \\n2. Keyword total_epochs in the config only controls the number of training epochs and will not affect the validation workflow. \\n3. Workflows [('train', 1), ('val', 1)] and [('train', 1)] will not change the behavior of EvalHook because EvalHook is called by after_train_epoch and validation workflow only affect hooks that are called through after_val_epoch. Therefore, the only difference between [('train', 1), ('val', 1)] and [('train', 1)] is that the runner will calculate losses on validation set after each training epoch.\",\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': '12.4 Customize hooks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': '12.4.1 Customize self-implemented hooks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': '1. Implement a new hook',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': 'There are some occasions when the users might need to implement a new hook. MMDetection supports customized hooks in training (#3395) since v2.3.0. Thus the users could implement a hook directly in mmdet or their mmdet- based codebases and use the hook by only modifying the config in training. Before v2.3.0, the users need to modify the code to get the hook registered before training starts. Here we give an example of creating a new hook in mmdet and using it in training.',\n",
       "  'page_idx': 89},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/200a5e00bf236d9a4e7b8b5b51b704ca7747e13333ede4cc5d83223dedccfd0b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from mmcv.runner import HOOKS, Hook</td></tr><tr><td>@HOOKS.register_module()</td></tr><tr><td>class MyHook(Hook):</td></tr><tr><td>def __init__(self, a, b):</td></tr></table>',\n",
       "  'page_idx': 89,\n",
       "  'outline': [69, 622, 543, 722]},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/a7be00f85e243bc5af3bb0e7e7ac9921d1ab45ca96a2acc587e2767d5564b11e.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>pass</td></tr><tr><td>def before_run(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def after_run(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def before_epoch(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def after_epoch(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def before_iter(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def after_iter(self, runner):</td></tr><tr><td>pass</td></tr></table>',\n",
       "  'page_idx': 90,\n",
       "  'outline': [69, 82, 543, 318]},\n",
       " {'type': 'text',\n",
       "  'text': 'Depending on the functionality of the hook, the users need to specify what the hook will do at each stage of the training in before_run, after_run, before_epoch, after_epoch, before_iter, and after_iter.',\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '2. Register the new hook',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': 'Then we need to make MyHook imported. Assuming the file is in mmdet/core/utils/my_hook.py there are two ways to do that:',\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '- Modify mmdet/core/utils/__init__.py to import it. The newly defined module should be imported in mmdet/core/utils/__init__.py so that the registry will find the new module and add it:',\n",
       "  'page_idx': 90},\n",
       " {'type': 'text', 'text': 'from .my_hook import MyHook', 'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': \"- Use custom_imports in the config to manually import itcustom_imports = dict(imports=['mmdet.core.utils.my_hook'], allow_failed_imports=False)\",\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '3. Modify the config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 90},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3ce5ea53df6a33bde776da8129d3d10415c3f5e1016809c578bc969636f6cede.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_hooks = [</td></tr><tr><td>dict(type=&#x27;MyHook&#x27;, a=a_value, b=b_value)</td></tr></table>',\n",
       "  'page_idx': 90,\n",
       "  'outline': [69, 583, 543, 628]},\n",
       " {'type': 'text',\n",
       "  'text': \"You can also set the priority of the hook by adding key priority to 'NORMAL' or 'HIGHEST' as below\",\n",
       "  'page_idx': 90},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/5e4d33181ff9a26991b0beed3e852a4dd2c75b040010487c406fbf687353d4ab.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_hooks = [</td></tr><tr><td>dict(type=&#x27;MyHook&#x27;, a=a_value, b=b_value, priority=&#x27;NORMAL&#x27;)</td></tr></table>',\n",
       "  'page_idx': 90,\n",
       "  'outline': [69, 654, 543, 696]},\n",
       " {'type': 'text',\n",
       "  'text': \"By default the hook's priority is set as NORMAL during registration.\",\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '12.4.2 Use hooks implemented in MMCV',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'If the hook is already implemented in MMCV, you can directly modify the config to use the hook as below',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': '4. Example: NumClassCheckHook',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'We implement a customized hook named NumClassCheckHook to check whether the num_classes in head matches the length of CLASSES in dataset.We set it in default_runtime.py.',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text', 'text': 'We set it in default_runtime.py.', 'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': \"custom_hooks  $=$  [dict(type  $\\\\coloneqq$  'NumClassCheckHook')]\",\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': '12.4.3 Modify default runtime hooks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'There are some common hooks that are not registered through custom_hooks, they are',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'log_config checkpoint_config evaluation lr_config optimizer_config momentum_config',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': \"In those hooks, only the logger hook has the VERY_LOW priority, others' priority are NORMAL. The above- mentioned tutorials already covers how to modify optimizer_config, momentum_config, and lr_config. Here we reveals how what we can do with log_config, checkpoint_config, and evaluation.\",\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'Checkpoint config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'The MMCV runner will use checkpoint_config to initialize CheckpointHook.',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'checkpoint_config  $=$  dict(interval  $= 1$',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'The users could set max_keep_ckpt s to only save only small number of checkpoints or decide whether to store state dict of optimizer by save_optimizer. More details of the arguments are here',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text', 'text': 'Log config', 'text_level': 1, 'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'The log_config wraps multiple logger hooks and enables to set intervals. Now MMCV supports WandbLoggerHook, MlflowLoggerHook, and TensorboardLoggerHook. The detail usages can be found in the doc.',\n",
       "  'page_idx': 91},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/664f26d0798238c78d0ddb3f720b3bdcdb7375c2e00189cf6e93f11a10e5de6c.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>log_config = dict(\\n    interval=50,\\n    hooks=[\\n        dict(type=&#x27;TextLoggerHook&#x27;),\\n        dict(type=&#x27;TensorboardLoggerHook&#x27;)\\n    ]</td></tr></table>',\n",
       "  'page_idx': 91,\n",
       "  'outline': [69, 619, 543, 697]},\n",
       " {'type': 'text',\n",
       "  'text': 'Evaluation config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 92},\n",
       " {'type': 'text',\n",
       "  'text': 'The config of evaluation will be used to initialize the EvalHook. Except the key interval, other arguments such as metric will be passed to the dataset.evaluate()',\n",
       "  'page_idx': 92},\n",
       " {'type': 'text',\n",
       "  'text': \"evaluation = dict(interval=1, metric='bbox')\",\n",
       "  'page_idx': 92},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 12. Tutorial 5: Customize Runtime Settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 93},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 6: CUSTOMIZE LOSSES',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection provides users with different loss functions. But the default configuration may be not applicable for different datasets or models, so users may want to modify a specific loss to adapt the new situation.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'This tutorial first elaborate the computation pipeline of losses, then give some instructions about how to modify each step. The modification can be categorized as tweaking and weighting.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '13.1 Computation pipeline of a loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'Given the input prediction and target, as well as the weights, a loss function maps the input tensor to the final loss scalar. The mapping can be divided into four steps:',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '1. Set the sampling method to sample positive and negative samples.  \\n2. Get element-wise or sample-wise loss by the loss kernel function.  \\n3. Weighting the loss with a weight tensor element-wise.  \\n4. Reduce the loss tensor to a scalar.  \\n5. Weighting the loss with a scalar.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '13.2 Set sampling method (step 1)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'For some loss functions, sampling strategies are needed to avoid imbalance between positive and negative samples. For example, when using CrossEntropyLoss in RPN head, we need to set RandomSampler in train_cfg',\n",
       "  'page_idx': 94},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3db1ae3fe201591b6d8d5a318edc1bda97e95cfe4b1432f857cd594f2643750a.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>train_cfg=dict(</td></tr><tr><td>rpn=dict(</td></tr><tr><td>sampler=dict(</td></tr><tr><td>type=&#x27;RandomSampler&#x27;,</td></tr><tr><td>num=256,</td></tr><tr><td>pos_fraction=0.5,</td></tr><tr><td>neg_pos_ub=-1,</td></tr><tr><td>add_gt_as_proposals=False))</td></tr></table>',\n",
       "  'page_idx': 94,\n",
       "  'outline': [69, 541, 543, 644]},\n",
       " {'type': 'text',\n",
       "  'text': 'For some other losses which have positive and negative sample balance mechanism such as Focal Loss, GHMC, and QualityFocalLoss, the sampler is no more necessary.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '13.3 Tweaking loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 95},\n",
       " {'type': 'text',\n",
       "  'text': '13.3 Tweaking lossTweaking a loss is more related with step 2, 4, 5, and most modifications can be specified in the config. Here we take Focal Loss (FL) as an example. The following code sniper are the construction method and config of FL respectively, they are actually one to one correspondence.',\n",
       "  'page_idx': 95},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/e24c185d69f9fa4cb7137aff1692ea19753c48d0636a1d7ec0546cf09ee66c98.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>@LOSSES.register_module()\\nclass FocalLoss(nn.Module):\\n    def __init__(self,\\n        use sigmoid=True,\\n        gamma=2.0,\\n        alpha=0.25,\\n        reduction=&#x27;mean&#x27;,\\n        loss_weight=1.0)</td></tr><tr><td>loss_cls=dict(\\n    type=&#x27;FocalLoss&#x27;,\\n    use sigmoid=True,\\n    gamma=2.0,\\n    alpha=0.25,\\n    loss_weight=1.0)</td></tr></table>',\n",
       "  'page_idx': 95,\n",
       "  'outline': [69, 147, 543, 348]},\n",
       " {'type': 'text',\n",
       "  'text': '13.3.1 Tweaking hyper-parameters (step 2)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 95},\n",
       " {'type': 'text',\n",
       "  'text': 'gamma and beta are two hyper- parameters in the Focal Loss. Say if we want to change the value of gamma to be 1.5 and alpha to be 0.5, then we can specify them in the config as follows:',\n",
       "  'page_idx': 95},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/1636e9c2008d82d3d2c43045335babf9aae52bbb98e2a46272ab17ccba23952e.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>loss_cls=dict(</td></tr><tr><td>type=&#x27;FocalLoss&#x27;,</td></tr><tr><td>use sigmoid=True,</td></tr><tr><td>gamma=1.5,</td></tr><tr><td>alpha=0.5,</td></tr><tr><td>loss_weight=1.0)</td></tr></table>',\n",
       "  'page_idx': 95,\n",
       "  'outline': [69, 427, 543, 506]},\n",
       " {'type': 'text',\n",
       "  'text': '13.3.2 Tweaking the way of reduction (step 3)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 95},\n",
       " {'type': 'text',\n",
       "  'text': 'The default way of reduction is mean for FL. Say if we want to change the reduction from mean to sum, we can specify it in the config as follows:',\n",
       "  'page_idx': 95},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/28f7f4ff0747401c4d8dad437d3d67da599b72410db105ceb56e2f2980dca03b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>loss_cls=dict(</td></tr><tr><td>type=&#x27;FocalLoss&#x27;,</td></tr><tr><td>use sigmoid=True,</td></tr><tr><td>gamma=2.0,</td></tr><tr><td>alpha=0.25,</td></tr><tr><td>loss_weight=1.0,</td></tr><tr><td>reduction=&#x27;sum&#x27;)</td></tr></table>',\n",
       "  'page_idx': 95,\n",
       "  'outline': [69, 585, 543, 676]},\n",
       " {'type': 'text',\n",
       "  'text': '13.3.3 Tweaking loss weight (step 5)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': 'The loss weight here is a scalar which controls the weight of different losses in multi- task learning, e.g. classification loss and regression loss. Say if we want to change to loss weight of classification loss to be 0.5, we can specify it in the config as follows:',\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': \"loss_cls=dict( type='FocalLoss', use sigmoid=True, gamma=2.0, alpha=0.25, loss_weight=0.5)\",\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': '13.4 Weighting loss (step 3)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': 'Weighting loss means we re- weight the loss element- wisely. To be more specific, we multiply the loss tensor with a weight tensor which has the same shape. As a result, different entries of the loss can be scaled differently, and so called element- wisely. The loss weight varies across different models and highly context related, but overall there are two kinds of loss weights, label_weights for classification loss and bbox_weights for bbox regression loss. You can find them in the get_target method of the corresponding head. Here we take ATSSHead as an example, which inherit AnchorHead but overwrite its get_targets method which yields different label_weights and bbox_weights.',\n",
       "  'page_idx': 96},\n",
       " {'type': 'text', 'text': 'class ATSSHead(AnchorHead):', 'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': '```pythondef get_targets(self, anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None, gt_labels_list=None, label_channels=1, unmap_outputs=True):```',\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 13. Tutorial 6: Customize Losses',\n",
       "  'page_idx': 97},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 7: FINETUNING MODELS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'Detectors pre- trained on the COCO dataset can serve as a good pre- trained model for other datasets, e.g., CityScapes and KITTI Dataset. This tutorial provides instruction for users to use the models provided in the Model Zoo for other datasets to obtain better performance.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'There are two steps to finetune a model on a new dataset.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': '- Add support for the new dataset following Tutorial 2: Customize Datasets.- Modify the configs as will be discussed in this tutorial.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'Take the finetuning process on Cityscapes Dataset as an example, the users need to modify five parts in the config.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': '14.1 Inherit base configs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'To release the burden and reduce bugs in writing the whole configs, MMDetection V2.0 support inheriting configs from multiple existing configs. To finetune a Mask RCNN model, the new config needs to inherit _base_/models/mask_rcnn_r50_fpn.py to build the basic structure of the model. To use the Cityscapes Dataset, the new config can also simply inherit _base_/datasets/cityscapes_instance.py. For runtime settings such as training schedules, the new config needs to inherit _base_/default_runtime.py. This configs are in the configs directory and the users can also choose to write the whole contents rather than use inheritance.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': \"```python- _base_ = [    .../__base_/models/mask_rcnn_r50_fpn.py',    .../__base_/datasets/cityscapes_instance.py', .../__base_/default_runtime.py']```\",\n",
       "  'page_idx': 98},\n",
       " {'type': 'text', 'text': '14.2 Modify head', 'text_level': 1, 'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'Then the new config needs to modify the head according to the class numbers of the new datasets. By only changing num_classes in the roi_head, the weights of the pre- trained models are mostly reused except the final prediction head.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': \"model  $=$  dict( pretrained  $=$  None, roi_head  $=$  dict( bbox_head  $=$  dict( type  $=$  'Shared2FCBBoxHead', in_channels  $= 256$  fc_out_channels  $= 1024$  roi_feat_size  $= 7$\",\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': \"num_classes  $= 8$  bbox_coder  $=$  dict( type  $=$  'DeltaXYwHBBoxCoder', target_means  $=$  [0. , 0. , 0. , 0. ], target_std  $\\\\mathbf{\\\\dot{s}} =$  [0.1, 0.1, 0.2, 0.2]), reg_class_agnostic  $=$  False, loss_cls  $=$  dict( type  $=$  'CrossEntropyLoss', use_sigmoid  $=$  False, loss_weight  $= 1.0$  ), loss_bbox  $=$  dict(type  $=$  'SmoothLLoss', beta  $= 1.0$  , loss_weight  $= 1.0$  )), mask_head  $=$  dict( type  $=$  'FCNMaskHead', num_conv  $s = 4$  in_channels  $= 256$  conv_out_channels  $= 256$  num_classes  $= 8$  loss_mask  $=$  dict( type  $=$  'CrossEntropyLoss', use_mask  $=$  True, loss_weight  $= 1.0$  ))\",\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': '14.3 Modify dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': 'The users may also need to prepare the dataset and write the configs about dataset. MMDetection V2.0 already support VOC, WIDER FACE, COCO and Cityscapes Dataset.',\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': '14.4 Modify training schedule',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': 'The finetuning hyperparameters vary from the default schedule. It usually requires smaller learning rate and less training epochs',\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': \"optimizer # lr is set for a batch size of 8 optimizer  $=$  dict(type  $=$  'SGD',  $\\\\mathtt{lr} = \\\\mathtt{0}$  .01, momentum  $= 0$  .9, weight_decay  $= 0$  .0001) optimizer_config  $=$  dict(grad clip  $\\\\coloneqq$  None) # learning policy lr_config  $=$  dict( policy  $=$  'step', warmup  $=$  'linear', warmup_iter  $s = 500$  warmup_ratio  $= 0$  .001, step=[7]) # the max_epochs and step in lr_config need specifically tuned for the customized dataset runner  $=$  dict(max_epochs  $= 8$  log_config  $=$  dict(interval  $= 100$\",\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': '14.5 Use pre-trained model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 100},\n",
       " {'type': 'text',\n",
       "  'text': 'To use the pre- trained model, the new config add the link of pre- trained models in the load_from. The users might need to download the model weights before training to avoid the download time during training.',\n",
       "  'page_idx': 100},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 14. Tutorial 7: Finetuning Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 101},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '- Tutorial 8: Pytorch to ONNX (Experimental)  \\n- How to convert models from Pytorch to ONNX      * Prerequisite      * Usage      * Description of all arguments  \\n- How to evaluate the exported models      * Prerequisite      * Usage      * Description of all arguments      * Results and Models  \\n- List of supported models exportable to ONNX  \\n- The Parameters of Non-Maximum Suppression in ONNX Export      - Reminders      - FAQs',\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '15.1 How to convert models from Pytorch to ONNX',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '15.1.1 Prerequisite',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '1. Install the prerequisites following get_started.md/Prepare environment.  \\n2. Build custom operators for ONNX Runtime and install MMCV manually following How to build custom operators for ONNX Runtime  \\n3. Install MMdetection manually following steps 2-3 in get_started.md/Install MMdetection.',\n",
       "  'page_idx': 102},\n",
       " {'type': 'text', 'text': '15.1.2 Usage', 'text_level': 1, 'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/deployment/pytorch2onnx.py \\\\${CONFIG_FILE} \\\\${CHECKPOINT_FILE} \\\\- - output- file \\\\ ${OUTPUT_FILE} \\\\- - input - img}$ {INPUT_IMAGE_PATH} \\\\- - shape \\\\ ${{IMAGE_SHAPE} \\\\- - test - img}$ {TEST_IMAGE_PATH} \\\\- - opset - version \\\\\\\\){OPSET_VERSION} \\\\- - cfg - options \\\\\\\\({CFG_OPTIONS} \\\\- - dynamic - export \\\\- - show \\\\- - verify \\\\- - simplify} \\\\',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': '15.1.3 Description of all arguments',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': 'config : The path of a model config file.',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': '15.1.3 Description of all arguments- config: The path of a model config file.- checkpoint: The path of a model checkpoint file.- - output- file: The path of output ONNX model. If not specified, it will be set to tmp. onnx.- - input- img: The path of an input image for tracing and conversion. By default, it will be set to tests/data/color.jpg.- - shape: The height and width of input tensor to the model. If not specified, it will be set to 8000 1216. - - test- img: The path of an image to verify the exported ONNX model. By default, it will be set to None, meaning it will use - - input- img for verification.- - opset- version: The opset version of ONNX. If not specified, it will be set to 11. - - dynamic- export: Determines whether to export ONNX model with dynamic input and output shapes. If not specified, it will be set to False.- - show: Determines whether to print the architecture of the exported model and whether to show detection outputs when - - verify is set to True. If not specified, it will be set to False.- - verify: Determines whether to verify the correctness of an exported model. If not specified, it will be set to False.- - simplify: Determines whether to simplify the exported ONNX model. If not specified, it will be set to False.- - cfg- options: Override some settings in the used config file, the key- value pair in xxx=yyy format will be merged into config file.- - skip- post- process: Determines whether export model without post process. If not specified, it will be set to False. Notice: This is an experimental option. Only work for some single stage models. Users need to implement the post- process by themselves. We do not guarantee the correctness of the exported model.',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text', 'text': 'Example:', 'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': 'Example:python tools/deployment/pytorch2onnx.py \\\\  - - configs/yolo/yolov3_d53_mstrain- 608_273e_coco.py \\\\  - - checkpoints/yolo/yolov3_d53_mstrain- 608_273e_coco.pth \\\\',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 103},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '- -output-file checkpoints/yolo/yolov3_d51_mstrain-608_273e_coco.onnx \\n--input-img demo/demo.jpg \\n--test-img tests/data/color.jpg \\n--shape 608 608 \\n--show \\n--verify \\n--dynamic-export \\n--cfg-options \\\\model.test_cfg.deploy_nms_pre=-1 \\\\',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '15.2 How to evaluate the exported models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'We prepare a tool tools/deployment/test.py to evaluate ONNX models with ONNXRuntime and TensorRT.',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '15.2.1 Prerequisite',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Install onnx and onnxruntime (CPU version)',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install onnx onnxruntime  $\\\\coloneqq = 1$  .5.1',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'If you want to run the model on GPU, please remove the CPU version before using the GPU version.',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'pip uninstall onnxruntime pip install onnxruntime- gpu',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: onnxruntime- gpu is version- dependent on CUDA and CUDNN, please ensure that your environment meets the requirements.',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Build custom operators for ONNX Runtime following How to build custom operators for ONNX Runtime',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Install TensorRT by referring to How to build TensorRT plugins in MMCV (optional)',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text', 'text': '15.2.2 Usage', 'text_level': 1, 'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/deployment/test.py \\\\ ${CONFIG_FILE} \\\\$ {MODEL_FILE} \\\\- - out}\\\\{OUTPUT_FILE\\\\} \\\\- - backend}\\\\{BACKEND\\\\} \\\\- - format- only}\\\\{FORMAT_ONLY\\\\} \\\\- - eval}\\\\{EVALUATION_METRICS\\\\} \\\\- - show- dir}\\\\{SHOW_DIRECTORY\\\\} \\\\- - - show- score- thr}\\\\{SHOW_SCORE_THRESHOLD\\\\} \\\\- - - cfg- options}\\\\{CFG_OPTIONS\\\\} \\\\- - - eval- options}\\\\{EVALUATION_OPTIONS\\\\} \\\\',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '15.2.3 Description of all arguments',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'config: The path of a model config file. model: The path of an input model file. - - out: The path of output result file in pickle format. - - backend: Backend for input model to run and should be onnxruntime or tensorrt. - - format- only : Format the output results without perform evaluation. It is useful when you want to format the result to a specific format and submit it to the test server. If not specified, it will be set to False. - - eval: Evaluation metrics, which depends on the dataset, e.g., \"bbox\", \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC. - - show- dir: Directory where painted images will be saved - - show- score- thr: Score threshold. Default is set to 0.3. - - cfg- options: Override some settings in the used config file, the key- value pair in xxx=yyy format will be merged into config file. - - eval- options: Custom options for evaluation, the key- value pair in xxx=yyy format will be kwargs for dataset.evvaluate() function',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'If the deployed backend platform is TensorRT, please add environment variables before running the file:',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'export ONNX_BACKEND  $\\\\equiv$  MMCVTensorRT',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'If you want to use the - - dynamic - export parameter in the TensorRT backend to export ONNX, please remove the - - simplify parameter, and vice versa.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': '15.2.4 Results and Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 105},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'All ONNX models are evaluated with dynamic shape on coco dataset and images are preprocessed according to the original config file. Note that CornerNet is evaluated without test- time flip, since currently only single- scale evaluation is supported with ONNX Runtime.- Mask AP of Mask R- CNN drops by  $1\\\\%$  for ONNX Runtime. The main reason is that the predicted masks are directly interpolated to original image in PyTorch, while they are at first interpolated to the preprocessed input image of the model and then to original image in other backend.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': '15.3 List of supported models exportable to ONNX',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'The table below lists the models that are guaranteed to be exportable to ONNX and runnable in ONNX Runtime.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'Minimum required version of MMCV is 1.3.5- All models above are tested with Pytorch  $= = 1.6.0$  and onnxruntime  $= = 1.5.1$  , except for CornerNet. For more details about the torch version when exporting CornerNet to ONNX, which involves mmcv: : cummax, please refer to the Known Issues in mmcv.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'Though supported, it is not recommended to use batch inference in onnxruntime for DETER, because there is huge performance gap between ONNX and torch model (e.g. 33.5 vs  $39.9\\\\mathrm{mAP}$  on COCO for onnxruntime and torch respectively, with a batch size 2). The main reason for the gap is that these is non- negligible effect on the predicted regressions during batch inference for ONNX, since the predicted coordinates is normalized by img_shape (without padding) and should be converted to absolute format, but img_shape is not dynamically traceable thus the padded img_shape_for_onnx is used.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'Currently only single- scale evaluation is supported with ONNX Runtime, also mmcv: : SoftNonMaxSuppression is only supported for single image by now.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': '15.4 The Parameters of Non-Maximum Suppression in ONNX Export',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'In the process of exporting the ONNX model, we set some parameters for the NMS op to control the number of output bounding boxes. The following will introduce the parameter setting of the NMS op in the supported models. You can set these parameters through - - cfg- options.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'nms_pre: The number of boxes before NMS. The default setting is 1000. deploy_nms_pre: The number of boxes before NMS when exporting to ONNX model. The default setting is 0. max_per_img: The number of boxes to be kept after NMS. The default setting is 100. max_output_bones_per_class: Maximum number of output boxes per class of NMS. The default setting is 200.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text', 'text': '15.5 Reminders', 'text_level': 1, 'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'When the input model has custom op such as RoIA1. ign and if you want to verify the exported ONNX model, you may have to build mmcv with ONNXRuntime from source. mmcv.onnx. simplify feature is based on onnx- simplifier. If you want to try it, please refer to onnx in mmcv and onnxruntime op in mmcv for more information. If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, please try to dig a little deeper and debug a little bit more and hopefully solve them by yourself. Because this feature is experimental and may change fast, please always try with the latest mmcv and mmdetection.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text', 'text': '15.6 FAQs', 'text_level': 1, 'page_idx': 106},\n",
       " {'type': 'text', 'text': '- None', 'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 107},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '- Tutorial 9: ONNX to TensorRT (Experimental)  \\n- How to convert models from ONNX to TensorRT      * Prerequisite      * Usage  \\n- How to evaluate the exported models  \\n- List of supported models convertible to TensorRT  \\n- Reminders  \\n- FAQs',\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '16.1 How to convert models from ONNX to TensorRT',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '16.1.1 Prerequisite',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '1. Please refer to get_started.md for installation of MMCV and MMDetection from source.  \\n2. Please refer to ONNXRuntime in mmcv and TensorRT plugin in mmcv to install mmcv-full with ONNXRuntime custom ops and TensorRT plugins.  \\n3. Use our tool pytorch2onnx to convert the model from PyTorch to ONNX.',\n",
       "  'page_idx': 108},\n",
       " {'type': 'text', 'text': '16.1.2 Usage', 'text_level': 1, 'page_idx': 108},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/150533eca9b6416a051deda1ef63e6edf959d7e5188376e3ed17bb8b18595aed.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>python tools/deployment/onnx2tensorrt.py</td><td>$ {CONFIG} \\\\\\\\\\n$ {MODEL} \\\\\\\\\\n--trt-file {{TRT_FILE}} \\\\\\\\\\n--input-img {{INPUT_IMAGE_PATH}} \\\\\\\\\\n--shape {{INPUT_IMAGE_SHAPE}} \\\\\\\\\\n--min-shape {{MIN_IMAGE_SHAPE}} \\\\\\\\\\n--max-shape {{MAX_IMAGE_SHAPE}} \\\\\\\\\\n--workspace-size {{WORKSPACE_SIZE}} \\\\\\\\\\n--show \\\\\\\\\\n--verify</td></tr></table>',\n",
       "  'page_idx': 108,\n",
       "  'outline': [67, 553, 543, 693]},\n",
       " {'type': 'text', 'text': 'Description of all arguments:', 'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': \"- config: The path of a model config file.- model: The path of an ONNX model file.- \\n--trt-file: The Path of output TensorRT engine file. If not specified, it will be set to tmp.trt.- \\n--input-img: The path of an input image for tracing and conversion. By default, it will be set to demo/demo.jpg.- \\n--shape: The height and width of model input. If not specified, it will be set to 400 600.- \\n--min-shape: The minimum height and width of model input. If not specified, it will be set to the same as \\n--shape.- \\n--max-shape: The maximum height and width of model input. If not specified, it will be set to the same as \\n--shape.- \\n--workspace-size: The required GPU workspace size in GiB to build TensorRT engine. If not specified, it will be set to 1 GiB.- \\n--show: Determines whether to show the outputs of the model. If not specified, it will be set to False.- \\n--verify: Determines whether to verify the correctness of models between ONNXRuntime and TensorRT. If not specified, it will be set to False.- \\n--verbose: Determines whether to print logging messages. It's useful for debugging. If not specified, it will be set to False.\",\n",
       "  'page_idx': 109},\n",
       " {'type': 'text', 'text': 'Example:', 'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/deployment/onnx2tensorrt.py \\\\ configs/retinanet/retinanet_r50_fpn_1x_coco.py \\\\ checkpoints/retinanet_r50_fpn_1x_coco.onnx \\\\ - - trt- file checkpoints/retinanet_r50_fpn_1x_coco.trt \\\\ - - input- img demo/demo.jpg \\\\ - - shape 400 600 \\\\ - - show \\\\ - - verify \\\\',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': '16.2 How to evaluate the exported models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': 'We prepare a tool tools/dep1opyment/test.py to evaluate TensorRT models.',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to following links for more information.',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': '- how-to-evaluate-the-exported-models- results-and-models',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': '16.3 List of supported models convertible to TensorRT',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'The table below lists the models that are guaranteed to be convertible to TensorRT.',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'Notes:- All models above are tested with Pytorch==1.6.0, onma==1.7.0 and TensorRT- 7.2.1.6. Ubuntu- 16.04. x86_64- gnu.cuda- 10.2. cudnn8.0',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text', 'text': '16.4 Reminders', 'text_level': 1, 'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, we may not provide much help here due to the limited resources. Please try to dig a little deeper and debug by yourself.',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'Because this feature is experimental and may change fast, please always try with the latest mmcv and mmdetecion.',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text', 'text': '16.5 FAQs', 'text_level': 1, 'page_idx': 110},\n",
       " {'type': 'text', 'text': 'None', 'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 111},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 10: WEIGHT INITIALIZATION',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'During training, a proper initialization strategy is beneficial to speeding up the training or obtaining a higher performance. MMCV provide some commonly used methods for initializing modules like nn. Conv2d. Model initialization in MMdetection mainly uses init_cfg. Users can initialize models with following two steps:',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define init_cfg for a model or its components in model_cfg, but init_cfg of children components have higher priority and will override init_cfg of parents modules. \\n2. Build model as usual, but call model.init_weights() method explicitly, and model parameters will be initialized as configuration.',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'The high- level workflow of initialization in MMdetection is :',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': \"model_cfg(init_cfg) - > build_from_cfg - > model - > init_weight() - > initialize(self, self.init_cfg) - > children's init_weight()\",\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': '17.1 Description',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'It is dict or list[dict], and contains the following keys and values:',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': \"type (str), containing the initializer name in INITIALIZERS, and followed by arguments of the initializer. layer (str or list[str]), containing the names of basi layers in Pytorch or MMCV with learnable parameters that will be initialized, e.g. 'Conv2d','DeformConv2d'. override (dict or list[dict]), containing the sub- modules that not inherit from BaseModule and whose initialization configuration is different from other layers' which are in 'layer' key. Initializer defined in type will work for all layers defined in 1ayer, so if sub- modules are not derived Classes of BaseModule but can be initialized as same ways of layers in 1ayer, it does not need to use override. override contains:\",\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'type followed by arguments of initializer; name to indicate sub- module which will be initialized.',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': '17.2 Initialize parameters',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'Inherit a new model from mmcv. runner. BaseModule or mmdet. models Here we show an example of FooModel.',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch.nn as nn from mmcv. runner import BaseModule class FooModel(BaseModule) def init__self,',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'arg1, arg2, init_cfg=None): super(FooModel, self).__init__(init_cfg)',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'Initialize model by using init_cfg directly in code',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch.nn as nn from mcmc:runter import BaseModule # or directly inherit mmdet models class FooModel(BaseModule) def __init__(self, arg1, arg2, init_cfg  $\\\\equiv$  XXX): super(FooModel, self).__init__(init_cfg)',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'Initialize model by using init_cfg directly in mcmc. Sequential or mcmc. ModuleList code',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'from mcmc:runter import BaseModule, ModuleList class FooModel(BaseModule) def __init__(self, arg1, arg2, init_cfg  $\\\\equiv$  None): super(FooModel, self).__init__(init_cfg) self.conv1  $=$  ModuleList(init_cfg  $\\\\equiv$  XXX)',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'Initialize model by using init_cfg in config file',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': \"model  $=$  dict( model  $=$  dict( type  $\\\\equiv$  'FooModel', arg1  $\\\\equiv$  XXX, arg2  $\\\\equiv$  XXX, init_cfg  $\\\\equiv$  XXX),\",\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': '17.3 Usage of init_cfg',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 114},\n",
       " {'type': 'text', 'text': '1. Initialize model by layer key', 'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'If we only define layer, it just initialize the layer in layer key.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'NOTE: Value of layer key is the class name with attributes weights and bias of Pytorch, (so such as MultiheadAttention layer is not supported).',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'Define layer key for initializing module with same configuration.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"init_cfg  $=$  dict(type  $=$  'Constant', layer  $=$  ['Conv1d', 'Conv2d', 'Linear'], val  $= 1$  # initialize whole module with same configuration\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'Define layer key for initializing layer with different configurations.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"init_cfg  $=$  [dict(type  $=$  'Constant', layer  $=$  'Conv1d', val  $= 1$  - dict(type  $=$  'Constant', layer  $=$  'Conv2d', val  $= 2$  - dict(type  $=$  'Constant', layer  $=$  'Linear', val  $= 3$  ] # nn.Conv1d will be initialized with dict(type  $=$  'Constant', val  $= 1$  # nn.Conv2d will be initialized with dict(type  $=$  'Constant', val  $= 2$  # nn.Linear will be initialized with dict(type  $=$  'Constant', val  $= 3$\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': '1. Initialize model by override key',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'When initializing some specific part with its attribute name, we can use override key, and the value in override will ignore the value in init_cfg.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"layers # self.feat  $=$  nn.Conv1d(3, 1, 3) # self.reg  $=$  nn.Conv2d(3, 3, 3) # self.cls  $=$  nn.Linear(1,2) init_cfg  $=$  dict(type  $=$  'Constant', layer  $=$  ['Conv1d','Conv2d'], val  $= 1$  , bias  $= 2$  override  $=$  dict(type  $=$  'Constant', name  $=$  'reg', val  $= 3$  , bias  $= 4$  )) # self.feat and self.cls will be initialized with dict(type  $=$  'Constant', val  $= 1$ $\\\\rightarrow$  bias  $= 2$  # The module called 'reg' will be initialized with dict(type  $=$  'Constant', val  $= 3$  , bias  $= 4$\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'If layer is None in init_cfg, only sub- module with the name in override will be initialized, and type and other args in override can be omitted.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"layers # self.feat  $=$  nn.Conv1d(3, 1, 3) # self.reg  $=$  nn.Conv2d(3, 3, 3) # self.cls  $=$  nn.Linear(1,2) init_cfg  $=$  dict(type  $=$  'Constant', val  $= 1$  , bias  $= 2$  override  $=$  dict(name  $=$  'reg')) # self.feat and self.cls will be initialized by Pytorch # The module called 'reg' will be initialized with dict(type  $=$  Constant', val  $= 1$  , bias  $= 2$\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"If we don't define layer key or override key, it will not initialize anything. Invalid usage\",\n",
       "  'page_idx': 114},\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed21f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMDetectionRelease 2.18.0\n",
      "MMDetection Authors\n",
      "1 Prerequisites 1\n",
      "2 Installation 3\n",
      "3 Verification 7\n",
      "4 Benchmark and Model Zoo 9\n",
      "5 1: Inference and train with existing models and standard datasets 17\n",
      "6 2: Train with customized datasets 29\n",
      "7 3: Train with customized models and standard datasets 35\n",
      "8 Tutorial 1: Learn about Configs 41\n",
      "9 Tutorial 2: Customize Datasets 55\n",
      "10 Tutorial 3: Customize Data Pipelines 67\n",
      "11 Tutorial 4: Customize Models 71\n",
      "12 Tutorial 5: Customize Runtime Settings 79\n",
      "13 Tutorial 6: Customize Losses 87\n",
      "14 Tutorial 7: Finetuning Models 91\n",
      "15 Tutorial 8: Pytorch to ONNX (Experimental) 95\n",
      "16 Tutorial 9: ONNX to TensorRT (Experimental) 101\n",
      "17 Tutorial 10: Weight initialization 105\n",
      "32 Changelog 141\n",
      "33 Frequently Asked Questions 173\n",
      "34 English 177\n",
      "35 179\n",
      "36 mmdet.apis 181\n",
      "37 mmdet.core 183\n",
      "PREREQUISITES\n",
      "MMDetection, Release 2.18.0\n",
      "INSTALLATION\n",
      "2.1 Prepare environment\n",
      "2.2 Install MMDetection\n",
      "Note:\n",
      "2.3 Install without GPU support\n",
      "2.4 Another option: Docker Image\n",
      "2.5 A from-scratch setup script\n",
      "2.6 Developing with multiple MMDetection versions\n",
      "VERIFICATION\n",
      "MMDetection, Release 2.18.0\n",
      "BENCHMARK AND MODEL ZOO\n",
      "4.1 Mirror sites\n",
      "4.2 Common settings\n",
      "4.3 ImageNet Pretrained Models\n",
      "4.4 Baselines\n",
      "4.4.1 RPN\n",
      "4.4.2 Faster R-CNN\n",
      "4.4.3 Mask R-CNN\n",
      "4.4.4 Fast R-CNN (with pre-computed proposals)\n",
      "4.4.5 RetinaNet\n",
      "4.4.6 Cascade R-CNN and Cascade Mask R-CNN\n",
      "4.4.7 Hybrid Task Cascade (HTC)\n",
      "4.4.8 SSD\n",
      "4.4.9 Group Normalization (GN)\n",
      "4.4.10 Weight Standardization\n",
      "4.4.11 Deformable Convolution v2\n",
      "4.4.12 CARAFE: Content-Aware ReAssembly of FEatures\n",
      "4.4.13 Instaboost\n",
      "4.4.14 Libra R-CNN\n",
      "4.4.15 Guided Anchoring\n",
      "4.4.16 FCOS\n",
      "4.4.17 FoveaBox\n",
      "4.4.18 RepPoints\n",
      "4.4.19 FreeAnchor\n",
      "4.4.20 Grid R-CNN (plus)\n",
      "4.4.21 GHM\n",
      "4.4.22 GCNet\n",
      "4.4.23 HRNet\n",
      "4.4.24 Mask Scoring R-CNN\n",
      "4.4.25 Train from Scratch\n",
      "4.4.26 NAS-FPN\n",
      "4.4.27 ATSS\n",
      "4.4.28 FSAF\n",
      "4.4.29 RegNetX\n",
      "4.4.30 Res2Net\n",
      "4.4.31 GRoIE\n",
      "4.4.32 Dynamic R-CNN\n",
      "4.4.33 PointRend\n",
      "4.4.34 DetectorRS\n",
      "4.4.35 Generalized Focal Loss\n",
      "4.4.36 CornerNet\n",
      "4.4.37 YOLOv3\n",
      "4.4.38 PAA\n",
      "4.4.39 SABL\n",
      "4.4.40 CentripetalNet\n",
      "4.4.41 ResNeSt\n",
      "4.4.42 DETR\n",
      "4.4.43 Deformable DETR\n",
      "4.4.44 AutoAssign\n",
      "4.4.45 YOLOF\n",
      "4.4.46 Seesaw Loss\n",
      "4.4.47 CenterNet\n",
      "4.4.48 YOLOX\n",
      "4.4.49 PVT\n",
      "4.4.50 SOLO\n",
      "4.4.51 QueryInst\n",
      "4.4.52 Other datasets\n",
      "4.4.53 Pre-trained Models\n",
      "4.5 Speed benchmark\n",
      "4.5.1 Training Speed benchmark\n",
      "4.5.2 Inference Speed Benchmark\n",
      "4.6 Comparison with Detector2\n",
      "4.6.1 Hardware\n",
      "4.6.2 Software environment\n",
      "4.6.3 Performance\n",
      "4.6.4 Training Speed\n",
      "4.6.5 Inference Speed\n",
      "4.6.6 Training memory\n",
      "1: INFERENCE AND TRAIN WITH EXISTING MODELS AND STANDARD DATASETS\n",
      "5.1 Inference with existing models\n",
      "5.1.1 High-level APIs for inference\n",
      "5.1.2 Asynchronous interface - supported for Python 3.7+\n",
      "5.1.3 Demos\n",
      "Image demo\n",
      "Webcam demo\n",
      "Video demo\n",
      "5.2 Test existing models on standard datasets\n",
      "5.2.1 Prepare datasets\n",
      "5.2.2 Test existing models\n",
      "5.2.3 Examples\n",
      "5.2. Test existing models on standard datasets\n",
      "5.2.4 Test without Ground Truth Annotations\n",
      "5.2.5 Batch Inference\n",
      "5.2.6 Deprecated ImageToTensor\n",
      "5.3 Train predefined models on standard datasets\n",
      "5.3.1 Prepare datasets\n",
      "5.3.2 Training on a single GPU\n",
      "Note:\n",
      "5.3.3 Training on multiple GPUs\n",
      "Launch multiple jobs simultaneously\n",
      "5.3.4 Training on multiple nodes\n",
      "5.3.5 Manage jobs with Slurm\n",
      "2: TRAIN WITH CUSTOMIZED DATASETS\n",
      "6.1 Prepare the customized dataset\n",
      "6.1.1 COCO annotation format\n",
      "6.2 Prepare a config\n",
      "6.3 Train a new model\n",
      "6.4 Test and inference\n",
      "3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS\n",
      "7.1 Prepare the standard dataset\n",
      "7.2 Prepare your own customized model\n",
      "7.2.1 1. Define a new neck (e.g. AugFPN)\n",
      "7.2.2 2. Import the module\n",
      "7.2.3 3. Modify the config file\n",
      "7.3 Prepare a config\n",
      "7.4 Train a new model\n",
      "7.5 Test and inference\n",
      "TUTORIAL 1: LEARN ABOUT CONFIGS\n",
      "8.1 Modify config through script arguments\n",
      "8.2 Config File Structure\n",
      "8.3 Config Name Style\n",
      "8.4 Deprecated train_cfg/test_cfg\n",
      "8.5 An Example of Mask R-CNN\n",
      "8.6 FAQ\n",
      "8.6.1 Ignore some fields in the base configs\n",
      "8.6.2 Use intermediate variables in configs\n",
      "TUTORIAL 2: CUSTOMIZE DATASETS\n",
      "9.1 Support new data format\n",
      "9.1.1 Reorganize new data formats to existing format\n",
      "1. Modify the config file for using the customized dataset\n",
      "2. Check the annotations of the customized dataset\n",
      "Note\n",
      "9.1.2 Reorganize new data format to middle format\n",
      "9.1.3 An example of customized dataset\n",
      "9.2 Customize datasets by dataset wrappers\n",
      "9.2.1 Repeat dataset\n",
      "9.2.2 Class balanced dataset\n",
      "9.2.3 Concatenate dataset\n",
      "9.2. Customize datasets by dataset wrappers\n",
      "Note:\n",
      "9.3 Modify Dataset Classes\n",
      "Note:\n",
      "9.4 COCO Panoptic Dataset\n",
      "Chapter 9. Tutorial 2: Customize Datasets\n",
      "TUTORIAL 3: CUSTOMIZE DATA PIPELINES\n",
      "10.1 Design of Data pipelines\n",
      "10.1.1 Data loading\n",
      "10.1.2 Pre-processing\n",
      "10.1.3 Formatting\n",
      "ToTensor\n",
      "Transpose\n",
      "DefaultFormatBundle\n",
      "10.1.4 Test time augmentation\n",
      "10.2 Extend and use custom pipelines\n",
      "TUTORIAL 4: CUSTOMIZE MODELS\n",
      "11.1 Develop new components\n",
      "11.1.1 Add a new backbone\n",
      "1. Define a new backbone (e.g. MobileNet)\n",
      "2. Import the module\n",
      "3. Use the backbone in your config file\n",
      "11.1.2 Add new necks\n",
      "1. Define a neck (e.g. PAFPN)\n",
      "2. Import the module\n",
      "3. Modify the config file\n",
      "11.1.3 Add new heads\n",
      "11.1.4 Add new loss\n",
      "TUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS\n",
      "12.1 Customize optimization settings\n",
      "12.1.1 Customize optimizer supported by Pytorch\n",
      "12.1.2 Customize self-implemented optimizer\n",
      "1. Define a new optimizer\n",
      "2. Add the optimizer to registry\n",
      "3. Specify the optimizer in the config file\n",
      "12.1.3 Customize optimizer constructor\n",
      "12.1.4 Additional settings\n",
      "12.2 Customize training schedules\n",
      "12.3 Customize workflow\n",
      "Note:\n",
      "12.4 Customize hooks\n",
      "12.4.1 Customize self-implemented hooks\n",
      "1. Implement a new hook\n",
      "2. Register the new hook\n",
      "3. Modify the config\n",
      "12.4.2 Use hooks implemented in MMCV\n",
      "4. Example: NumClassCheckHook\n",
      "12.4.3 Modify default runtime hooks\n",
      "Checkpoint config\n",
      "Log config\n",
      "Evaluation config\n",
      "Chapter 12. Tutorial 5: Customize Runtime Settings\n",
      "TUTORIAL 6: CUSTOMIZE LOSSES\n",
      "13.1 Computation pipeline of a loss\n",
      "13.2 Set sampling method (step 1)\n",
      "13.3 Tweaking loss\n",
      "13.3.1 Tweaking hyper-parameters (step 2)\n",
      "13.3.2 Tweaking the way of reduction (step 3)\n",
      "13.3.3 Tweaking loss weight (step 5)\n",
      "13.4 Weighting loss (step 3)\n",
      "TUTORIAL 7: FINETUNING MODELS\n",
      "14.1 Inherit base configs\n",
      "14.2 Modify head\n",
      "14.3 Modify dataset\n",
      "14.4 Modify training schedule\n",
      "14.5 Use pre-trained model\n",
      "Chapter 14. Tutorial 7: Finetuning Models\n",
      "TUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL)\n",
      "15.1 How to convert models from Pytorch to ONNX\n",
      "15.1.1 Prerequisite\n",
      "15.1.2 Usage\n",
      "15.1.3 Description of all arguments\n",
      "15.2 How to evaluate the exported models\n",
      "15.2.1 Prerequisite\n",
      "15.2.2 Usage\n",
      "15.2.3 Description of all arguments\n",
      "15.2.4 Results and Models\n",
      "15.3 List of supported models exportable to ONNX\n",
      "15.4 The Parameters of Non-Maximum Suppression in ONNX Export\n",
      "15.5 Reminders\n",
      "15.6 FAQs\n",
      "MMDetection, Release 2.18.0\n",
      "TUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL)\n",
      "16.1 How to convert models from ONNX to TensorRT\n",
      "16.1.1 Prerequisite\n",
      "16.1.2 Usage\n",
      "16.2 How to evaluate the exported models\n",
      "16.3 List of supported models convertible to TensorRT\n",
      "16.4 Reminders\n",
      "16.5 FAQs\n",
      "MMDetection, Release 2.18.0\n",
      "TUTORIAL 10: WEIGHT INITIALIZATION\n",
      "17.1 Description\n",
      "17.2 Initialize parameters\n",
      "17.3 Usage of init_cfg\n",
      "LOG ANALYSIS\n",
      "RESULT ANALYSIS\n",
      "Usage\n",
      "Examples:\n",
      "VISUALIZATION\n",
      "20.1 Visualize Datasets\n",
      "20.2 Visualize Models\n",
      "20.3 Visualize Predictions\n",
      "MMDetection, Release 2.18.0\n",
      "ERROR ANALYSIS\n",
      "MMDetection, Release 2.18.0\n",
      "MODEL SERVING\n",
      "22.1 1. Convert model from MMDetection to TorchServe\n",
      "22.2 2. Build mmdet - serve docker image\n",
      "22.3 3. Run mmdet - serve\n",
      "22.4 4. Test deployment\n",
      "MMDetection, Release 2.18.0\n",
      "MODEL COMPLEXITY\n",
      "MMDetection, Release 2.18.0\n",
      "MODEL CONVERSION\n",
      "24.1 MMDetection model to ONNX (experimental)\n",
      "24.2 MMDetection 1.x model to MMDetection 2.x\n",
      "24.3 RegNet model to MMDetection\n",
      "24.4  Detection ResNet to Pytorch\n",
      "24.5 Prepare a model for publishing\n",
      "DATASET CONVERSION\n",
      "MMDetection, Release 2.18.0\n",
      "BENCHMARK\n",
      "26.1 Robust Detection Benchmark\n",
      "26.2 FPS Benchmark\n",
      "MMDetection, Release 2.18.0\n",
      "MISCELLANEOUS\n",
      "27.1 Evaluating a metric\n",
      "27.2 Print the entire config\n",
      "MMDetection, Release 2.18.0\n",
      "HYPER-PARAMETER OPTIMIZATION\n",
      "28.1 YOLO Anchor Optimization\n",
      "CONVENTIONS\n",
      "29.1 Loss\n",
      "29.2 Empty Proposals\n",
      "29.3 Coco Panoptic Dataset\n",
      "COMPATIBILITY OF MMDETECTION 2.X\n",
      "30.1 MMDetection 2.18.0\n",
      "30.1.1 DllHead compatibility\n",
      "30.2 MMDetection 2.14.0\n",
      "30.2.1 MMCV Version\n",
      "30.2.2 SSD compatibility\n",
      "30.3 MMDetection 2.12.0\n",
      "30.3.1 MMCV Version\n",
      "30.3.2 Unified model initialization\n",
      "30.3.3 Unified model registry\n",
      "30.3.4 Mask AP evaluation\n",
      "30.4 Compatibility with MMDetection 1.x\n",
      "30.4.1 Coordinate System\n",
      "30.4.2 Codebase Conventions\n",
      "30.4.3 Training Hyperparameters\n",
      "30.4.4 Upgrade Models from 1.x to 2.0\n",
      "30.5 pycocotools compatibility\n",
      "PROJECTS BASED ON MMDETECTION\n",
      "31.1 Projects as an extension\n",
      "31.2 Projects of papers\n",
      "CHANGELOG\n",
      "32.1 v2.18.0 (27/10/2021)\n",
      "32.1.1 Highlights\n",
      "32.1.2 New Features\n",
      "32.1.3 Bug Fixes\n",
      "32.1.4 Improvements\n",
      "32.1.5 Refactors\n",
      "32.1.6 Contributors\n",
      "32.2 v2.17.0 (28/9/2021)\n",
      "32.2.1 Highlights\n",
      "32.2.2 New Features\n",
      "32.2.3 Bug Fixes\n",
      "32.2.4 Improvements\n",
      "32.2.5 Contributors\n",
      "32.3 v2.16.0 (30/8/2021)\n",
      "32.3.1 Highlights\n",
      "32.3.2 New Features\n",
      "32.3.3 Bug Fixes\n",
      "32.3.4 Improvements\n",
      "32.3.5 Contributors\n",
      "32.4 v2.15.1 (11/8/2021)\n",
      "32.4.1 Highlights\n",
      "32.4.2 New Features\n",
      "32.4.3 Bug Fixes\n",
      "32.4.4 Improvements\n",
      "32.4.5 Contributors\n",
      "32.5 v2.15.0 (02/8/2021)\n",
      "32.5.1 Highlights\n",
      "32.5.2 New Features\n",
      "32.5.3 Bug Fixes\n",
      "32.5.4 Improvements\n",
      "32.5.5 Contributors\n",
      "32.6 v2.14.0 (29/6/2021)\n",
      "32.6.1 Highlights\n",
      "32.6.2 New Features\n",
      "32.6.3 Bug Fixes\n",
      "32.6.4 Improvements\n",
      "32.7 v2.13.0 (01/6/2021)\n",
      "32.7.1 Highlights\n",
      "32.7.2 New Features\n",
      "32.7.3 Bug Fixes\n",
      "32.7.4 Improvements\n",
      "32.8 v2.12.0 (01/5/2021)\n",
      "32.8.1 Highlights\n",
      "32.8.2 Backwards Incompatible Changes\n",
      "32.8.3 New Features\n",
      "32.8.4 Improvements\n",
      "32.8.5 Bug Fixes\n",
      "32.9 v2.11.0 (01/4/2021)\n",
      "Highlights\n",
      "New Features\n",
      "Improvements\n",
      "Bug Fixes\n",
      "32.10 v2.10.0 (01/03/2021)\n",
      "32.10.1 Highlights\n",
      "32.10.2 New Features\n",
      "32.10.3 Bug Fixes\n",
      "32.10.4 Improvements\n",
      "32.11 v2.9.0 (01/02/2021)\n",
      "32.11.1 Highlights\n",
      "32.11.2 New Features\n",
      "32.11.3 Bug Fixes\n",
      "32.11.4 Improvements\n",
      "32.12 v2.8.0 (04/01/2021)\n",
      "32.12.1 Highlights\n",
      "32.12.2 New Features\n",
      "32.12.3 Bug Fixes\n",
      "32.12.4 Improvements\n",
      "32.13 v2.7.0 (30/11/2020)\n",
      "32.13.1 New Features\n",
      "32.13.2 Bug Fixes\n",
      "32.13.3 Improvements\n",
      "32.14 v2.6.0 (1/11/2020)\n",
      "32.14.1 New Features\n",
      "32.14.2 Bug Fixes\n",
      "32.14.3 Improvements\n",
      "32.15 v2.5.0 (5/10/2020)\n",
      "32.15.1 Highlights\n",
      "32.15.2 Backwards Incompatible Changes\n",
      "32.15.3 New Features\n",
      "32.15.4 Bug Fixes\n",
      "32.15.5 Improvements\n",
      "32.16 v2.4.0 (5/9/2020)\n",
      "Highlights\n",
      "Backwards Incompatible Changes\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.17 v2.3.0 (5/8/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.18 v2.2.0 (1/7/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.19 v2.1.0 (8/6/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.20 v2.0.0 (6/5/2020)\n",
      "Improvements\n",
      "Bug Fixes\n",
      "New Features\n",
      "32.21 v1.1.0 (24/2/2020)\n",
      "Highlights\n",
      "Breaking Changes\n",
      "Bug Fixes\n",
      "Improvements\n",
      "New Features\n",
      "32.22 v1.0.0 (30/1/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "Improvements\n",
      "New Features\n",
      "32.23 v1.0rc1 (13/12/2019)\n",
      "Highlights\n",
      "Breaking Changes\n",
      "Bug Fixes\n",
      "Improvements\n",
      "New Features\n",
      "32.24 v1.0rc0 (27/07/2019)\n",
      "32.25 v0.6.0 (14/04/2019)\n",
      "32.26 v0.6rc0(06/02/2019)\n",
      "32.27 v0.5.7 (06/02/2019)\n",
      "32.28 v0.5.6 (17/01/2019)\n",
      "32.29 v0.5.5 (22/12/2018)\n",
      "32.30 v0.5.4 (27/11/2018)\n",
      "32.31 v0.5.3 (26/11/2018)\n",
      "32.32 v0.5.2 (21/10/2018)\n",
      "32.33 v0.5.1 (20/10/2018)\n",
      "FREQUENTLY ASKED QUESTIONS\n",
      "33.1 MMCV Installation\n",
      "33.2 PyTorch/CUDA Environment\n",
      "33.3 Training\n",
      "33.4 Evaluation\n",
      "Chapter 33. Frequently Asked Questions\n",
      "MMDetection, Release 2.18.0\n",
      "CHAPTERTHIRTYFIVE\n",
      "MMDetection, Release 2.18.0\n",
      "MMDET.APIS\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "MMDET.CORE\n",
      "37.1 anchor\n",
      "Parameters\n",
      "Examples\n",
      "gen_base_anchors()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property num_baseanchors\n",
      "property num_base_priors\n",
      "property num_levels\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Examples\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property num_base_priors\n",
      "property num_levels\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property num_levels\n",
      "Parameters\n",
      "Parameters\n",
      "Return type torch.Tensor\n",
      "Parameters\n",
      "Parameters\n",
      "37.2 bbox\n",
      "num_gts\n",
      "gt_inds\n",
      "max_overlap\n",
      "labels\n",
      "Example\n",
      "property info\n",
      "property num_preds\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "property bboxes\n",
      "Parameters\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "static random_choice(gallery, num)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "1) is_aligned is False\n",
      "Parameters\n",
      "Return type Tensor\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "37.3 export\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Examples\n",
      "37.4 mask\n",
      "Parameters\n",
      "Parameters\n",
      "Return type BaseInstanceMasks\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "property areas\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "property areas\n",
      "References\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "37.5 evaluation\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "37.6 post_processing\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "37.7 utils\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "MMDET.DATASETS\n",
      "38.1 datasets\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "get_ann_info(idx)\n",
      "get_cat_ids(idx)\n",
      "Parameters\n",
      "xyxy2xywh(bbox)\n",
      "Parameters\n",
      "get_ann_info(idx)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "get_cat_ids(idx)\n",
      "Parameters\n",
      "Parameters\n",
      "load_annotations(ann_file)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "get_ann_info(idx)\n",
      "get_cat_ids(idx)\n",
      "Parameters\n",
      "Examples\n",
      "Return type list\n",
      "Examples\n",
      "38.2 pipelines\n",
      "Parameters\n",
      "Examples\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "get_indexes(dataset)\n",
      "Parameters\n",
      "get_indexes(dataset)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Note:\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "static random_sample(img_scales)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "38.3 samplers\n",
      "Parameters\n",
      "Parameters\n",
      "set_epoch(epoch)\n",
      "Parameters\n",
      "set_epoch(epoch)\n",
      "38.4 api_wrappers\n",
      "MMDetection, Release 2.18.0\n",
      "MMDET.MODELS\n",
      "39.1 detectors\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type dict\n",
      "property with bbox\n",
      "property with mask\n",
      "property with neck\n",
      "property with shared_head\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_dummy(img)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Return type list[np.ndarray]\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "forward_dummy(img)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "39.2 backbones\n",
      "Parameters\n",
      "Example\n",
      "forward  $(x)$\n",
      "train (mode  $\\equiv$  True)\n",
      "Parameters\n",
      "Example\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "train (mode=True)\n",
      "Parameters\n",
      "forward(x)\n",
      "init_weights()\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "make_res_layer(\\*\\*kwargs)\n",
      "Parameters\n",
      "Example\n",
      "Examples\n",
      "Parameters\n",
      "property norm1\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "forward(x)\n",
      "init_weights()\n",
      "Parameters\n",
      "39.3 necks\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "forward(feature)\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Example\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Parameters\n",
      "tensor_add(a, b)\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Note:\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "39.4 dense heads\n",
      "forward(fears)\n",
      "Returns\n",
      "Usually a tuple of classification scores and bbox prediction\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single  $(x)$\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "additional Returns: This function enables user-defined returns from\n",
      "Return type tuple\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Decoded output of CenterNetHead, containing\n",
      "forward (feats)\n",
      "Returns\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "init_weightsC\n",
      "Parameters\n",
      "Returns\n",
      "which has components below:\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "init_weights()\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "forward(feats)\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple[Tensor]\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type dict\n",
      "init_weights()\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "forward(fears)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "forward(feats)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_single  $(x)$\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Example\n",
      "Returns\n",
      "Usually a tuple of classification scores and bbox prediction\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Return type Tensor\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "centers_to_bboxes(point_list)\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "forward_single(x)\n",
      "Returns\n",
      "forward(feats)\n",
      "Returns\n",
      "Usually a tuple of classification scores and bbox prediction\n",
      "init_weights()\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "forward(feats)\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "forward(feats)\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple\n",
      "Parameters\n",
      "property numanchors\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "additional Returns: This function enables user-defined returns from\n",
      "Return type tuple\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Usually returns a tuple containing learning targets.\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "property num anchors\n",
      "property num_attrb\n",
      "Parameters\n",
      "Parameters\n",
      "forward(fears)\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "init_weights()\n",
      "Parameters\n",
      "39.5 roi heads\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "property num_inputs\n",
      "Parameters\n",
      "property with bbox\n",
      "Parameters\n",
      "init_assigner sampler()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward(x)\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type Tuple[Tensor]\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "update_hyperparameters()\n",
      "forward(x)\n",
      "Parameters\n",
      "Returns\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Parameters\n",
      "calc_sub_regions()\n",
      "forward(x)\n",
      "Parameters\n",
      "Parameters\n",
      "property with semantic\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type Tensor\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "forward(x)\n",
      "train (mode  $\\equiv$  True)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property with_feat_relay\n",
      "property with_glbctx\n",
      "property with semantic\n",
      "Parameters\n",
      "Parameters\n",
      "Queries\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "39.6 losses\n",
      "Parameters\n",
      "Return type tuple[float]\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "39.7 utils\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "forward(mask)\n",
      "Returns\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "forward(mask)\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "MMDetection, Release 2.18.0\n",
      "INDICES AND TABLES\n",
      "MMDetection, Release 2.18.0\n",
      "m\n",
      "MMDetection, Release 2.18.0\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "N\n",
      "O\n",
      "P\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "for i in content_list:\n",
    "    if \"text_level\" in i.keys():\n",
    "        print(i[\"text\"])\n",
    "        k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe9994d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_content_list_files(root_dir):\n",
    "    matched_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('content_list.json'):\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "                matched_files.append(full_path)\n",
    "    return matched_files\n",
    "\n",
    "# \n",
    "root_directory = '/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/'  # \n",
    "files = find_content_list_files(root_directory)\n",
    "\n",
    "levels = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        file_contents = json.load(f)\n",
    "    cnt = 0\n",
    "    for i in file_contents:\n",
    "        if \"text_level\" in i.keys():\n",
    "            cnt += 1\n",
    "    levels.append(cnt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51ec1392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/AMAZON_2017_10K/58085999-29c2-4982-b4b5-a57f4d7c126b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/AMAZON_2017_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Campaign_038_Introducing_AC_Whitepaper_v5e/2e5a21db-5439-4ff1-9d0d-0b08f857e1b3_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Campaign_038_Introducing_AC_Whitepaper_v5e/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2018.11.19_algorithms_FINAL/e49f25d9-4974-4db8-900a-c8a867132147_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2018.11.19_algorithms_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.05.21_International-Cooperation-COVID_FINAL/2da63574-5d44-4f01-b832-d7dfa16d5a9b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.05.21_International-Cooperation-COVID_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8e7c4cb542ad160f80fb3d795ada35d8/c2d6620a-43e0-4533-89e6-1bcb21ac5a11_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8e7c4cb542ad160f80fb3d795ada35d8/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/User_Manual_1500S_Classic_EN/a7daf174-a1cf-41ed-b0b2-fdeff1df7e62_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/User_Manual_1500S_Classic_EN/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/caltraincapacitymountainview1-150701205750-lva1-app6891_95/55448177-0815-48b5-8397-fe9b5d470122_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/caltraincapacitymountainview1-150701205750-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8dfc21ec151fb9d3578fc32d5c4e5df9/6029ae2a-169b-45dd-8abb-868029936746_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8dfc21ec151fb9d3578fc32d5c4e5df9/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0b85477387a9d0cc33fca0f4becaa0e5/b3fb76ef-a972-476d-9592-82ec584f2028_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0b85477387a9d0cc33fca0f4becaa0e5/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2019.01.17_Trump-economy_FINAL2/e6dccc4c-65a8-454d-bd2e-c22eb9876f1c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2019.01.17_Trump-economy_FINAL2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2005.12872v3/9377366f-07c5-4015-bf22-2aedc8d96984_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2005.12872v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PS_2018.01.09_STEM_FINAL/bff688ef-a07c-4781-8fe5-629be36288ba_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PS_2018.01.09_STEM_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/csewt7zsecmmbzjufbyx-signature-24d91a254426c21c3079384270e1f138dc43a271cfe15d6d520d68205855b2a3-poli-150306115347-conversion-gate01_95/a20b1251-0b0f-4847-a4bc-c6ce3b5b6a41_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/csewt7zsecmmbzjufbyx-signature-24d91a254426c21c3079384270e1f138dc43a271cfe15d6d520d68205855b2a3-poli-150306115347-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.10997v5/742531ca-2e7d-4195-ae36-228b275a6980_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.10997v5/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/52b3137455e7ca4df65021a200aef724/b6e174e0-de64-4f83-b13b-b6950e88000c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/52b3137455e7ca4df65021a200aef724/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/asdaaburson-marstellerarabyouthsurvey2014-140407100615-phpapp01_95/79f704e9-846e-43f7-a3ba-ca8a734bd86e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/asdaaburson-marstellerarabyouthsurvey2014-140407100615-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PRE_2022.09.29_NSL-politics_REPORT/0c6d39af-c6f8-416e-966a-d3ddd5dd5fb7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PRE_2022.09.29_NSL-politics_REPORT/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2309.17421v2/804f46ab-97e3-4f52-9567-8a6a73313434_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2309.17421v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/competitiveoutcomes-091006065143-phpapp01_95/eedd8a81-6eba-4d1c-9e88-57e4db0b4014_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/competitiveoutcomes-091006065143-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2024.ug.eprospectus/4ebae30b-a7a3-4fa0-bd31-bda013b6e912_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2024.ug.eprospectus/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e639029d16094ea71d964e2fb953952b/30eaa1bc-2695-470a-b6e2-de5c9938ee5b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e639029d16094ea71d964e2fb953952b/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/STEPBACK/37f3741e-1be1-4664-8c00-b3e55a4b841c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/STEPBACK/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/GPL-Graduate-Studies-Professional-Learning-Brochure-Jul-2021/e7b0e602-e500-40dd-b834-f7296b727401_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/GPL-Graduate-Studies-Professional-Learning-Brochure-Jul-2021/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PIP_Seniors-and-Tech-Use_040314/0da1d022-feba-4bf1-ab4c-86bf376cd03a_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PIP_Seniors-and-Tech-Use_040314/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/camry_ebrochure/64af210a-f6d7-47dd-8e30-1a8b2ec77c7f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/camry_ebrochure/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bariumswallowpresentation-090810084400-phpapp01_95/585d3cb5-0bb0-43bf-9ccc-c65da7e2d091_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bariumswallowpresentation-090810084400-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2307.09288v2/eb7c3bf2-bfd6-439b-98fe-df3a3fda2623_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2307.09288v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2021.03.04_US-Views-on-China_FINAL/8494bee5-9a7f-4c33-9839-3e4983a04acc_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2021.03.04_US-Views-on-China_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3276a5b991c49cf5f9a4af0f7d6fce67/315386f2-4c32-4e27-bd57-d03083326c2c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3276a5b991c49cf5f9a4af0f7d6fce67/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NETFLIX_2015_10K/5d63a4c0-c5ed-41be-a2c6-e6b39d6a2208_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NETFLIX_2015_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_20.07.30_U.S.-Views-China_final/65252ada-6ab7-4091-af0a-b10e4d8769ea_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_20.07.30_U.S.-Views-China_final/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/chapter8-geneticscompatibilitymode-141214140247-conversion-gate02_95/583aa212-63f7-48b2-8bba-c1ea4d0af92f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/chapter8-geneticscompatibilitymode-141214140247-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.05039v2/54111238-c55d-4722-9966-73cdacbf30d1_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.05039v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ISEP_student_handbook_2020/e97d657f-769d-4b03-b425-4e0dfcd86365_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ISEP_student_handbook_2020/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SnapNTell/62e5eea8-266e-4d16-b6be-5a4857bc7887_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SnapNTell/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2017.10.04_Automation_FINAL/f7b6fcd5-3d26-4657-b727-37b3c76b8ea6_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2017.10.04_Automation_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NYU_graduate/eba5c529-2717-4867-b052-881813eb195f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NYU_graduate/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/amb-siteaudits-ds15-150204174043-conversion-gate01_95/d0e52b53-e531-4502-a8e6-5ecd5aa14a54_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/amb-siteaudits-ds15-150204174043-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Macbook_air/8929f4e0-cb45-4239-b21e-ec25c00b938c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Macbook_air/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/91521110100M_4K_UHD_Display_User_Manual_V1.1/c6d92953-b8e3-4054-b8ed-21649f5a53b9_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/91521110100M_4K_UHD_Display_User_Manual_V1.1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Guide-for-international-students-web/b4fd9169-8008-4ae4-8397-92a5a7a69d2c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Guide-for-international-students-web/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PH_2016.06.08_Economy-Final/422a2be9-22c5-4fea-ac2d-3ceafa6a2e02_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PH_2016.06.08_Economy-Final/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-Business-School-BBA-Brochure-2024/5c38912d-f849-4d4a-ac2c-09a423beb22f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-Business-School-BBA-Brochure-2024/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-FASS-Graduate-Guidebook-2021-small/de59cea2-a938-4a42-b56d-0ab38ff7d08d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-FASS-Graduate-Guidebook-2021-small/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/formwork-150318073913-conversion-gate01_95/36504667-22ab-428c-b119-e1db9abb8859_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/formwork-150318073913-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0e94b4197b10096b1f4c699701570fbf/37a81f14-0e13-4b71-8756-3ca9b8df408c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0e94b4197b10096b1f4c699701570fbf/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e79deb02a0c0e87511080836c5d4347b/fcad443f-7621-4d74-adb9-5bdbf688c617_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e79deb02a0c0e87511080836c5d4347b/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/germanwingsdigitalcrisisanalysis-150403064828-conversion-gate01_95/9b50dc40-86fc-4685-80e6-4a3d101ca708_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/germanwingsdigitalcrisisanalysis-150403064828-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/stereo_headset/58312fd4-3d1c-4221-9e21-d2ced1b216f9_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/stereo_headset/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.13186v3/3c9bd192-8e4c-408e-99cf-67f0de94d847_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.13186v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2021-Apple-Catalog/0574e0f3-c4d1-4915-9e37-9d796f5ae6cf_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2021-Apple-Catalog/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/c31e6580d0175ab3f9d99d1ff0bfa000/dc70ed02-fb51-4a6a-be90-67ceadcf01c7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/c31e6580d0175ab3f9d99d1ff0bfa000/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/honor_watch_gs_pro/fe2437b6-63cd-4b26-96ad-23c194e19f52_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/honor_watch_gs_pro/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/reportq32015-151009093138-lva1-app6891_95/cef24e0b-595b-41ea-884f-d260855e7c5b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/reportq32015-151009093138-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f1f5242528411b262be447e61e2eb10f/263ee5dd-c342-4ec2-a93b-41957b88559e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f1f5242528411b262be447e61e2eb10f/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.05634v2/e2082d53-f2db-4b0c-8ad2-27609f324786_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.05634v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2020.08.06_COVID-19-Restrictions_FINAL-1/c5cf9f1f-c3d5-4a3a-9209-7d923e9bd35b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2020.08.06_COVID-19-Restrictions_FINAL-1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efd88e41c5f2606c57929cac6c1c0605/9433f5e8-b300-4514-817a-12d3fc71530d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efd88e41c5f2606c57929cac6c1c0605/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/tacl_a_00660/77502d71-f2e5-4d8a-ab6b-49cd9192a444_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/tacl_a_00660/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BESTBUY_2023_10K/7d121574-9b24-43eb-a817-50b325143bcd_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BESTBUY_2023_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/avalaunchpresentationsthatkickasteriskv3copy-150318114804-conversion-gate01_95/d3b0e34d-6452-4473-bee6-f1b2420329a5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/avalaunchpresentationsthatkickasteriskv3copy-150318114804-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/DSA-278777/696e43d2-cee4-4c75-ba60-24d7789bdcfb_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/DSA-278777/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ddoseattle-150627210357-lva1-app6891_95/35e51de7-ae1d-4a89-91c5-0aa0d6568d44_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ddoseattle-150627210357-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/b3m5kaeqm2w8n4bwcesw-140602121350-phpapp02_95/f935003e-4934-4179-8a7f-df9df5176238_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/b3m5kaeqm2w8n4bwcesw-140602121350-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/RAR/31d98fe0-9e13-40a3-baea-6304df70092c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/RAR/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2401.18059v1/c26eabe0-e483-47ad-adc2-4fcd8589e27c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2401.18059v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Sinopolis-Chengdu/a791013f-2046-4c6b-ae37-ed8ce9355e19_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Sinopolis-Chengdu/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Bergen-Brochure-en-2022-23/d2c37a73-24ea-434e-8b6d-76212b766baf_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Bergen-Brochure-en-2022-23/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a4f3ced0696009fec3179f493e4f28c4/28275dac-4928-4be4-912e-1ee77ee0736d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a4f3ced0696009fec3179f493e4f28c4/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a5879805d70c854ea4361e43a84e3bb2/99c381de-e60d-4f54-a3a2-d59749b91ed6_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a5879805d70c854ea4361e43a84e3bb2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SAO-StudentSupport_Guidebook-Content/9941e983-2314-43e9-bfbe-af3ea8b01319_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SAO-StudentSupport_Guidebook-Content/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/watch_d/e4528add-49b6-45f5-a1e4-fbdd3c0f916c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/watch_d/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3M_2018_10K/5db1be9e-ec5f-4217-947d-392dc60d01ef_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3M_2018_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fd76bbefe469561966e5387aa709c482/29145350-94b8-416d-a1c1-2791445d1996_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fd76bbefe469561966e5387aa709c482/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2311.16502v3/c42296f7-09ae-4e88-a6c7-54e99433401f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2311.16502v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.09158v1/ed408fac-1abf-42d0-9d59-74b2bee6e993_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.09158v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.acl-long.386/601af515-dbb3-473a-b8ca-e63b28eef1a7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.acl-long.386/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nielsen2015musicbizpresentation-final-150526143534-lva1-app6891_95/8ad100e9-0005-46c0-b7e1-67786dfb0878_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nielsen2015musicbizpresentation-final-150526143534-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ACTIVISIONBLIZZARD_2019_10K/b5aaecc2-7d18-42cd-8a69-ce936349b690_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ACTIVISIONBLIZZARD_2019_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/san-francisco-11-contents/066b86f3-d98c-44b2-95d6-7fc2b25e6dc5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/san-francisco-11-contents/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/obs-productdesc-en/0b2545a7-f535-42f9-afee-60d706f3f88d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/obs-productdesc-en/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.07609v1/84043656-0c89-445d-9f3b-dd4f1e919d4f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.07609v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/indonesiamobilemarketresearch-ag-150106055934-conversion-gate02_95/5ae2d295-cef0-47e5-b945-88c1c4259a55_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/indonesiamobilemarketresearch-ag-150106055934-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/afe620b9beac86c1027b96d31d396407/4256f2ea-811e-4dae-a9e6-3c09c716d762_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/afe620b9beac86c1027b96d31d396407/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2021.04.22_voting-access_REPORT/064f9c6d-c6e1-4fb2-95a8-1ebb9c27c56e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2021.04.22_voting-access_REPORT/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/edb88a99670417f64a6b719646aed326/19ef3a03-8232-48aa-b74c-a83b25f5cdfd_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/edb88a99670417f64a6b719646aed326/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/guojixueshengshenghuozhinanyingwen9.1/e297d614-ac0d-417b-b282-f871e8eb6e08_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/guojixueshengshenghuozhinanyingwen9.1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BRO-GL-MMONEY/d2ce48ed-e8c8-4ad0-abe9-64114eda1ab5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BRO-GL-MMONEY/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.08559v2/486b4b41-05f3-442e-af37-dfe0789c73b8_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.08559v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/379f44022bb27aa53efd5d322c7b57bf/9777fdaf-b74e-457b-a34f-ce7e3e4b9c13_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/379f44022bb27aa53efd5d322c7b57bf/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bigdatatrends-120723191058-phpapp02_95/bd1209cb-a372-48c5-b83c-3ae7a2ba0947_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bigdatatrends-120723191058-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/digitalmeasurementframework22feb2011v6novideo-110221233835-phpapp01_95/4f3d86b9-4b54-414a-af40-ce1c873eac5d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/digitalmeasurementframework22feb2011v6novideo-110221233835-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalmediafindingspdf-141228031149-conversion-gate02_95/9f34cb69-f10d-4d76-ad7f-5aaf5d75516f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalmediafindingspdf-141228031149-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2306.05425v1/bb7a03da-8c0a-4df9-894a-c23924d572b4_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2306.05425v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/11-21-16-Updated-Post-Election-Release/65a37edf-8108-4f93-984b-c8e884292cc3_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/11-21-16-Updated-Post-Election-Release/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/dr-vorapptchapter1emissionsources-121120210508-phpapp02_95/4f3d86b9-4b54-414a-af40-ce1c873eac5d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/dr-vorapptchapter1emissionsources-121120210508-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nova_y70/ef6248ae-cd3b-4680-927f-5c8cbce005fb_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nova_y70/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/698bba535087fa9a7f9009e172a7f763/bb318d38-ec2b-4bb0-89f1-1858ff3f4068_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/698bba535087fa9a7f9009e172a7f763/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/welcome-to-nus/d9c91204-2622-44ff-9fc8-62ac47657a83_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/welcome-to-nus/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Independents-Report/4aa85629-e267-4190-a22a-c81f6f48d4ff_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Independents-Report/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efis-140411041451-phpapp01_95/9067c1e1-739c-46a0-9290-2066bedb2322_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efis-140411041451-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earlybird-110722143746-phpapp02_95/3ce1c5b3-7ac7-40cc-9e7d-ec53dd86fb98_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earlybird-110722143746-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2210.02442v1/9b97ca7b-3b33-4586-9d63-04eed6757b8e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2210.02442v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalpresentationdeck-whatwhyhowofcertificationsocial-160324220748_95/341675c9-0e3f-43ef-904a-0dc606fbaf8e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalpresentationdeck-whatwhyhowofcertificationsocial-160324220748_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/q1-2023-bilibili-inc-investor-presentation/11dcd0d3-ff07-4217-93d3-d88503f847cf_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/q1-2023-bilibili-inc-investor-presentation/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fdac8d1e9ef56519371df7e6532df27d/284af9cb-e633-483c-a141-9e7462c4ba97_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fdac8d1e9ef56519371df7e6532df27d/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2405.09818v1/1995dc42-9449-40bf-8a68-dec7ace2983d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2405.09818v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/StudentSupport_Guidebook/9941e983-2314-43e9-bfbe-af3ea8b01319_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/StudentSupport_Guidebook/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/disciplined-agile-business-analysis-160218012713_95/7aa01fea-a924-4801-b340-428edc048fa6_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/disciplined-agile-business-analysis-160218012713_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Pew-Research-Center_Hispanic-Identity-Report_12.20.2017/3db7305f-2aad-47ee-97bf-e17face4960a_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Pew-Research-Center_Hispanic-Identity-Report_12.20.2017/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/12-15-15-ISIS-and-terrorism-release-final/4437ed95-de42-4bae-9747-6d782059a59d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/12-15-15-ISIS-and-terrorism-release-final/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.04350v3/9dab9a49-2e73-4060-9c9e-481e7652944f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.04350v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/936c0e2c2e6c8e0c07c51bfaf7fd0a83/6b2cdea8-32b3-4fb3-aded-66ab9eeb26c4_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/936c0e2c2e6c8e0c07c51bfaf7fd0a83/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/owners-manual-2170416/27ccdae4-15fe-4c15-9070-dffd429a6d11_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/owners-manual-2170416/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f86d073b0d735ac873a65d906ba82758/67f18b13-3c8c-4add-8368-589bbc384e3e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f86d073b0d735ac873a65d906ba82758/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/87a24eb7-d010-4cc6-80a7-fb87a2ce7dff_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/COSTCO_2021_10K/cf7d7462-07d7-4b6a-a190-b9463744be05_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/COSTCO_2021_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bdf54dxa/f4a08913-6f89-438e-9d8b-f20e3b46d3c5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bdf54dxa/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ecommerceopportunityindia-141124010546-conversion-gate01_95/022ffa06-6936-4b08-bc2b-a4f649628ece_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ecommerceopportunityindia-141124010546-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/measuringsuccessonfacebooktwitterlinkedin-160317142140_95/89a49661-2c08-4b3c-9370-8f657aa854e4_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/measuringsuccessonfacebooktwitterlinkedin-160317142140_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/catvsdogdlpycon15se-150512122612-lva1-app6891_95/711c9d6a-a58f-4c4e-8e4d-f7bd39b236bb_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/catvsdogdlpycon15se-150512122612-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ADOBE_2015_10K/799d62d8-716c-459a-b848-32b3b69ae43c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ADOBE_2015_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/05-03-18-political-release/690659f1-5167-48ea-9123-67b88df35de7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/05-03-18-political-release/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/transform-software-delivery-with-valueedge-brochure/64ceba93-a59c-437b-adb2-7cbc25eab7bc_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/transform-software-delivery-with-valueedge-brochure/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f8d3a162ab9507e021d83dd109118b60/38ff2d85-afd7-47d3-986f-401c3c9d5034_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f8d3a162ab9507e021d83dd109118b60/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.03.09_US-Germany_FINAL/11c0f4e4-b457-475a-891b-54c88da1e550_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.03.09_US-Germany_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earthlinkweb-150213112111-conversion-gate02_95/075cede7-3224-4bf8-9ca3-fba1aba1e030_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earthlinkweb-150213112111-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.findings-emnlp.248/8606b0dc-dc7f-4eb3-b6f6-3354140a4330_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.findings-emnlp.248/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/7c3f6204b3241f142f0f8eb8e1fefe7a/7bc5bbe9-12f7-4e0e-9cd2-3188b43cbc48_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/7c3f6204b3241f142f0f8eb8e1fefe7a/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NIKE_2021_10K/02bb4fa5-bb66-423b-bb4a-a1deeabcacd8_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NIKE_2021_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.14160v4/37a440f1-f997-49b1-87ae-267a190ac0a8_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.14160v4/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.09390v1/d21a647f-6acf-4176-993e-4a07a9f2b4fd_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.09390v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/t480_ug_en/c19e962d-aaf0-47b4-b56f-7d14e60107c0_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/t480_ug_en/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PWC_opportunity_of_lifetime/f66d5194-207c-45c6-82ab-d77807801dbc_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PWC_opportunity_of_lifetime/content_list.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def rename_content_list_files(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('_content_list.json'):\n",
    "                old_path = os.path.join(dirpath, filename)\n",
    "                new_path = os.path.join(dirpath, 'content_list.json')\n",
    "\n",
    "                #  content_list.json\n",
    "                if os.path.exists(new_path):\n",
    "                    print(f\"  Skipped (already exists): {new_path}\")\n",
    "                    continue\n",
    "\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\" Renamed: {old_path} -> {new_path}\")\n",
    "\n",
    "# \n",
    "root_directory = '/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB'  #  \n",
    "rename_content_list_files(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61a561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
