from __future__ import annotations

from typing import Dict, List, Tuple, Iterable, Optional
from dataclasses import dataclass

from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo


def _clean_text(s: Optional[str]) -> str:
    if not isinstance(s, str):
        return ""
    return " ".join(s.split()).strip()


def _section_path_titles(node_id: str, id2node: Dict[str, Dict]) -> List[str]:
    path: List[str] = []
    cur = id2node.get(node_id)
    safety = 0
    while cur and safety < 256:
        title = cur.get("title")
        if isinstance(title, str) and title.strip():
            path.append(title.strip())
        pid = cur.get("_parent_id")
        cur = id2node.get(pid)
        safety += 1
    return list(reversed(path))


def _gather_units(section: Dict) -> List[Dict]:
    """Collect atomic units inside a section: text paragraphs, lists (as bullet text), image captions.
    Only direct children of the section are considered; child sections are handled recursively by caller.
    """
    units: List[Dict] = []
    children = list(section.get("children", []) or [])
    children.sort(key=lambda x: x.get("read_order_idx", 0))
    for ch in children:
        t = ch.get("type")
        if t == "text":
            content = _clean_text(ch.get("text"))
            if content:
                units.append({
                    "text": content,
                    "page_idx": ch.get("page_idx"),
                    "node_id": ch.get("node_id"),
                    "read_order_idx": ch.get("read_order_idx"),
                })
        elif t == "list":
            items = []
            for it in ch.get("items", []) or []:
                tx = _clean_text(it.get("text"))
                if tx:
                    items.append(f"â€¢ {tx}")
            if items:
                units.append({
                    "text": "\n".join(items),
                    "page_idx": ch.get("page_idx"),
                    "node_id": ch.get("node_id"),
                    "read_order_idx": ch.get("read_order_idx"),
                })
        elif t == "image":
            # 1) caption from child nodes
            cap = None
            for cc in ch.get("children", []) or []:
                if cc.get("role") == "caption":
                    cap = _clean_text(cc.get("text"))
                    break
            if cap:
                units.append({
                    "text": cap,
                    "page_idx": ch.get("page_idx"),
                    "node_id": ch.get("node_id"),
                    "read_order_idx": ch.get("read_order_idx"),
                })
            # 2) description generated by media enhancer (VLM)
            desc = _clean_text(ch.get("description"))
            if desc and desc != cap:
                units.append({
                    "text": desc,
                    "page_idx": ch.get("page_idx"),
                    "node_id": ch.get("node_id"),
                    "read_order_idx": ch.get("read_order_idx"),
                })
        # tables could be added similarly via a textual preview if desired
    return units


def _chunk_units_to_leaf_nodes(
    section: Dict,
    id2node: Dict[str, Dict],
    target_chars: int,
    overlap_chars: int,
    avoid_cross_page: bool,
    doc_id: str,
    parent_node: TextNode,
) -> Tuple[List[TextNode], Optional[Tuple[int, int]]]:
    units = _gather_units(section)
    if not units:
        return [], None

    section_path = " > ".join(_section_path_titles(section["node_id"], id2node))

    buf: List[str] = []
    buf_ids: List[str] = []
    buf_pages: List[Optional[int]] = []
    cur_chars = 0
    leaf_nodes: List[TextNode] = []
    page_lo = page_hi = None

    def flush(make_overlap: bool) -> None:
        nonlocal buf, buf_ids, buf_pages, cur_chars, page_lo, page_hi
        if not buf:
            return
        text = f"[{section_path}]\n" + "\n".join(buf).strip()
        md = {
            "doc_id": doc_id,
            "section_path": section_path,
            "source_node_ids": buf_ids.copy(),
        }
        pages = [p for p in buf_pages if isinstance(p, int)]
        if pages:
            if len(set(pages)) == 1:
                md["page_idx"] = pages[0]
                pg_lo = pg_hi = pages[0]
            else:
                pg_lo, pg_hi = min(pages), max(pages)
                md["page_range"] = [pg_lo, pg_hi]
            if page_lo is None:
                page_lo, page_hi = pg_lo, pg_hi
            else:
                page_lo, page_hi = min(page_lo, pg_lo), max(page_hi, pg_hi)
        leaf = TextNode(text=text, metadata=md)
        leaf.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=parent_node.node_id)
        leaf_nodes.append(leaf)

        # overlap
        if make_overlap and overlap_chars > 0:
            total = 0
            tail: List[str] = []
            for t in reversed(buf):
                if total + len(t) + 1 > overlap_chars:
                    break
                tail.append(t)
                total += len(t) + 1
            tail.reverse()
            k = len(tail)
            buf = tail
            buf_ids[:] = buf_ids[-k:] if k else []
            buf_pages[:] = buf_pages[-k:] if k else []
            cur_chars = sum(len(x) for x in buf)
        else:
            buf, buf_ids, buf_pages, cur_chars = [], [], [], 0

    last_page = None
    for u in units:
        u_text = u["text"]
        u_page = u.get("page_idx")
        u_len = len(u_text)

        if avoid_cross_page and last_page is not None and u_page != last_page and buf:
            flush(make_overlap=False)
        last_page = u_page

        if cur_chars + u_len > target_chars and buf:
            flush(make_overlap=True)

        buf.append(u_text)
        buf_ids.append(u["node_id"])
        buf_pages.append(u_page)
        cur_chars += u_len

    flush(make_overlap=False)
    return leaf_nodes, (page_lo, page_hi) if page_lo is not None else None


def build_nodes_from_tree(doc: Dict, cfg: Dict) -> Tuple[List[TextNode], List[TextNode]]:
    """Convert DocTree JSON to LlamaIndex TextNodes.

    Returns: (all_nodes, leaf_nodes)
      - all_nodes: parent sections + leaf chunks; add to DocStore
      - leaf_nodes: leaf chunks only; build VectorStore index
    """
    id2node: Dict[str, Dict] = {}

    def index_tree(n: Dict, parent_id: Optional[str]):
        n["_parent_id"] = parent_id
        nid = n.get("node_id")
        if isinstance(nid, str):
            id2node[nid] = n
        for ch in n.get("children", []) or []:
            if isinstance(ch, dict):
                index_tree(ch, nid)

    for ch in doc.get("children", []) or []:
        if isinstance(ch, dict):
            index_tree(ch, None)

    all_nodes: List[TextNode] = []
    leaf_nodes: List[TextNode] = []

    def walk_section(sec: Dict) -> None:
        section_path = " > ".join(_section_path_titles(sec["node_id"], id2node))
        parent = TextNode(
            text=f"[{section_path}]",
            metadata={
                "doc_id": doc.get("doc_id"),
                "level": "section",
                "title": sec.get("title") or "",
                "section_path": section_path,
            },
        )
        all_nodes.append(parent)

        leaves, prange = _chunk_units_to_leaf_nodes(
            section=sec,
            id2node=id2node,
            target_chars=cfg["chunking"]["target_chars"],
            overlap_chars=cfg["chunking"]["overlap_chars"],
            avoid_cross_page=cfg["chunking"]["avoid_cross_page"],
            doc_id=doc.get("doc_id"),
            parent_node=parent,
        )
        if prange:
            parent.metadata["page_range"] = [prange[0], prange[1]]

        for lf in leaves:
            all_nodes.append(lf)
            leaf_nodes.append(lf)

        for ch in sec.get("children", []) or []:
            if ch.get("type") == "section":
                walk_section(ch)

    for ch in doc.get("children", []) or []:
        if ch.get("type") == "section":
            walk_section(ch)

    return all_nodes, leaf_nodes
