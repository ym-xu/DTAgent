{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5297acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ec12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/layout.json', 'r', encoding='utf-8') as f:\n",
    "    layout = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb31f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/content_list.json', 'r', encoding='utf-8') as f:\n",
    "    content_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1fbbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5622)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layout['pdf_info'][0]), len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a29320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c0b7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = []\n",
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        content_img.append(i['img_path'].split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b88bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/layout.json', 'r', encoding='utf-8') as f:\n",
    "    layout = json.load(f)\n",
    "\n",
    "with open('/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/87a24eb7-d010-4cc6-80a7-fb87a2ce7dff_content_list.json', 'r', encoding='utf-8') as f:\n",
    "    content_list = json.load(f)\n",
    "\n",
    "layout_info = dict()\n",
    "def recursive_search(obj, path=\"\"):\n",
    "    \"\"\"Recursively search for image_path keys in nested structures\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            current_path = f\"{path}.{key}\" if path else key\n",
    "            if key == \"image_path\" and isinstance(value, str):\n",
    "                layout_info[value] = obj[\"bbox\"]\n",
    "                # print(f\"Found image_path at {current_path}: {value}\")\n",
    "            else:\n",
    "                recursive_search(value, current_path)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            current_path = f\"{path}[{i}]\"\n",
    "            recursive_search(item, current_path)\n",
    "\n",
    "recursive_search(layout['pdf_info'])\n",
    "\n",
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        i[\"outline\"] = layout_info[i['img_path'].split('/')[1]]\n",
    "len(layout_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65400ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        i[\"outline\"] = layout_info[i['img_path'].split('/')[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2efa575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'MMDetectionRelease 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 0},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection Authors',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 0},\n",
       " {'type': 'text', 'text': '', 'page_idx': 1},\n",
       " {'type': 'text', 'text': '1 Prerequisites 1', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text', 'text': '2 Installation 3', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '2 Installation 32.1 Prepare environment 32.2 Install MMDetection 32.3 Install without GPU support 52.4 Another option: Docker Image 52.5 A from- scratch setup script 52.6 Developing with multiple MMDetection versions 6',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text', 'text': '3 Verification 7', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '4 Benchmark and Model Zoo 9',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '4 Benchmark and Model Zoo 94.1 Mirror sites 94.2 Common settings 94.3 ImageNet Pretrained Models 94.4 Baselines 104.5 Speed benchmark 154.6 Comparison with Detector2 16',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '5 1: Inference and train with existing models and standard datasets 17',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '5.1 Inference with existing models 175.2 Test existing models on standard datasets 205.3 Train predefined models on standard datasets 26',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '6 2: Train with customized datasets 29',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '6.1 Prepare the customized dataset 296.2 Prepare a config 336.3 Train a new model 346.4 Test and inference 34',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '7 3: Train with customized models and standard datasets 35',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '7.1 Prepare the standard dataset 357.2 Prepare your own customized model 367.3 Prepare a config 377.4 Train a new model 407.5 Test and inference 40',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '8 Tutorial 1: Learn about Configs 41',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '8.1 Modify config through script arguments 418.2 Config File Structure 41',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': '8.3 Config Name Style 42  8.4 Deprecated train_cfg/test_cfg 42  8.5 An Example of Mask R- CNN 43  8.6 FAQ 51',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '9 Tutorial 2: Customize Datasets 55',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '9.1 Support new data format 55  9.2 Customize datasets by dataset wrappers 60  9.3 Modify Dataset Classes 63  9.4 COCO Panoptic Dataset 64',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '10 Tutorial 3: Customize Data Pipelines 67',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '10.1 Design of Data pipelines 67  10.2 Extend and use custom pipelines 70',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '11 Tutorial 4: Customize Models 71',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text', 'text': '11.1 Develop new components 71', 'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '12 Tutorial 5: Customize Runtime Settings 79',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '12.1 Customize optimization settings 79  12.2 Customize training schedules 81  12.3 Customize workflow 82  12.4 Customize hooks 82',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '13 Tutorial 6: Customize Losses 87',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '13.1 Computation pipeline of a loss 87  13.2 Set sampling method (step 1) 87  13.3 Tweaking loss 88  13.4 Weighting loss (step 3) 89',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '14 Tutorial 7: Finetuning Models 91',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '14.1 Inherit base configs 91  14.2 Modify head 91  14.3 Modify dataset 92  14.4 Modify training schedule 92  14.5 Use pre- trained model 93',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '15 Tutorial 8: Pytorch to ONNX (Experimental) 95',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '15.1 How to convert models from Pytorch to ONNX 95  15.2 How to evaluate the exported models 97  15.3 List of supported models exportable to ONNX 98  15.4 The Parameters of Non- Maximum Suppression in ONNX Export 99  15.5 Reminders 99  15.6 FAQs 99',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '16 Tutorial 9: ONNX to TensorRT (Experimental) 101',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '16.1 How to convert models from ONNX to TensorRT 101  16.2 How to evaluate the exported models 102  16.3 List of supported models convertible to TensorRT 103  16.4 Reminders 103  16.5 FAQs 103',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '17 Tutorial 10: Weight initialization 105',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': '17.1 Description 105  17.2 Initialize parameters 105',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text', 'text': '17.3 Usage of init_cfg 107', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '18 Log Analysis 109', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '19 Result Analysis 111', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '20 Visualization 113', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '20.1 Visualize Datasets 113', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '20.2 Visualize Models 113  20.3 Visualize Predictions 113',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '21 Error Analysis 115', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '22 Model Serving 117', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '22.1 1. Convert model from MMDetection to TorchServe 117',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '22.2 2. Build mmdet- serve docker image 117',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '22.3 3. Run mmdet- serve 117', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '22.4 4. Test deployment 118', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '23 Model Complexity 121', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '24 Model conversion 123', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.1 MMDetection model to ONNX (experimental) 123',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.2 MMDetection 1. x model to MMDetection 2. x 123',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.3 RegNet model to MMDetection 123',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.4 Detectron ResNet to Pytorch 124',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '24.5 Prepare a model for publishing 124',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '25 Dataset Conversion 125', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '26 Benchmark 127', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '26.1 Robust Detection Benchmark 127',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '26.2 FPS Benchmark 127', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '27 Miscellaneous 129', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '27.1 Evaluating a metric 129  27.2 Print the entire config 129',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '28 Hyper- parameter Optimization 131',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '28.1 YOLO Anchor Optimization 131', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29 Conventions 133', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29.1 Loss 133', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29.2 Empty Proposals 133', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '29.3 Coco Panoptic Dataset 134', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '30 Compatibility of MMDetection 2. x 135',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.1 MMDetection 2.18.0 135', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.2 MMDetection 2.14.0 135', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.3 MMDetection 2.12.0 135', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '30.4 Compatibility with MMDetection 1. x 136',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '30.5 pycocotools compatibility 138', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': '31 Projects based on MMDetection 139',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': '31.1 Projects as an extension 139', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '31.2 Projects of papers 139', 'page_idx': 4},\n",
       " {'type': 'text', 'text': '32 Changelog 141', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '32.1 v2.18.0 (27/10/2021) 141 32.2 v2.17.0 (28/9/2021) 142 32.3 v2.16.0 (30/8/2021) 144 32.4 v2.15.1 (11/8/2021) 145 32.5 v2.15.0 (02/8/2021) 146 32.6 v2.14.0 (29/6/2021) 147 32.7 v2.13.0 (01/6/2021) 148 32.8 v2.12.0 (01/5/2021) 150 32.9 v2.11.0 (01/4/2021) 151 32.10 v2.10.0 (01/03/2021) 152 32.11 v2.9.0 (01/02/2021) 153 32.12 v2.8.0 (04/01/2021) 154 32.13 v2.7.0 (30/11/2020) 156 32.14 v2.6.0 (1/11/2020) 157 32.15 v2.5.0 (5/10/2020) 158 32.16 v2.4.0 (5/9/2020) 159 32.17 v2.3.0 (5/8/2020) 161 32.18 v2.2.0 (1/7/2020) 162 32.19 v2.1.0 (8/6/2020) 163 32.20 v2.0.0 (6/5/2020) 165 32.21 v1.1.0 (24/2/2020) 166 32.22 v1.0.0 (30/1/2020) 167 32.23 v1.0rc1 (13/12/2019) 168 32.24 v1.0rc0 (27/07/2019) 171 32.25 v0.6.0 (14/04/2019) 171 32.26 v0.6rc0(06/02/2019) 171 32.27 v0.5.7 (06/02/2019) 171 32.28 v0.5.6 (17/01/2019) 171 32.29 v0.5.5 (22/12/2018) 171 32.30 v0.5.4 (27/11/2018) 172 32.31 v0.5.3 (26/11/2018) 172 32.32 v0.5.2 (21/10/2018) 172 32.33 v0.5.1 (20/10/2018) 172',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '33 Frequently Asked Questions 173',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '33.1 MMCV Installation 173 33.2 PyTorch/CUDA Environment 173 33.3 Training 174 33.4 Evaluation 175',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': '34 English 177', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text', 'text': '35 179', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text', 'text': '36 mmdet.apis 181', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text', 'text': '37 mmdet.core 183', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': '37.1 anchor 183 37.2 bbox 191 37.3 export 208 37.4 mask 211 37.5 evaluation 219 37.6 post_processing 221 37.7 utils 224',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': '38 mmdet.datasets 227', 'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': '38.1 datasets 227  38.2 pipelines 239  38.3 samplers 256  38.4 api_wrappers 257',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text', 'text': '39 mmdet.models 259', 'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': '39.1 detectors 259  39.2 backbones 273  39.3 necks 291  39.4 dense-heads 301  39.5 roi-heads 383  39.6 losses 412  39.7 utils 425',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text', 'text': '40 mmdet.utils 437', 'page_idx': 6},\n",
       " {'type': 'text', 'text': '41 Indices and tables 439', 'page_idx': 6},\n",
       " {'type': 'text', 'text': 'Python Module Index 441', 'page_idx': 6},\n",
       " {'type': 'text', 'text': 'Index 443', 'page_idx': 6},\n",
       " {'type': 'text', 'text': 'vi', 'page_idx': 7},\n",
       " {'type': 'text', 'text': 'PREREQUISITES', 'text_level': 1, 'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': '- Linux or macOS (Windows is in experimental support)- Python 3.6+- PyTorch 1.3+- CUDA 9.2+ (If you build PyTorch from source, CUDA 9.0 is also compatible)- GCC 5+- MMCV',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Compatible MMDetection and MMCV versions are shown as below. Please install the correct version of MMCV to avoid installation issues.',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: You need to run pip uninstall mmcv first if you have mmcv installed. If mmcv and mmcv- full are both installed, there will be ModuleNotFoundError.',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 9},\n",
       " {'type': 'text', 'text': 'INSTALLATION', 'text_level': 1, 'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '2.1 Prepare environment',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '1. Create a conda virtual environment and activate it.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda create - n openmmlab python  $= 3.7$  - y conda activate openmmlab',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '2. Install PyTorch and torchvision following the official instructions, e.g.,',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda install pytorch torchvision - c pytorch',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: Make sure that your compilation CUDA version and runtime CUDA version match. You can check the supported CUDA version for precompiled packages on the PyTorch website.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'E.g. 1 If you have CUDA 10.1 installed under /usr/local/cuda and would like to install PyTorch 1.5, you need to install the prebuilt PyTorch with CUDA 10.1.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda install pytorch cudatoolkit  $= 10.1$  torchvision - c pytorch',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'E.g. 2 If you have CUDA 9.2 installed under /usr/local/cuda and would like to install PyTorch 1.3.1. you need to install the prebuilt PyTorch with CUDA 9.2.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'conda install pytorch  $= 1.3.1$  cudatoolkit  $= 9.2$  torchvision  $= 0.4.2$  - c pytorch',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'If you build PyTorch from source instead of installing the prebuilt package, you can use more CUDA versions such as 9.0.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '2.2 Install MMDetection',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'It is recommended to install MMDetection with MM, which automatically handles the dependencies of OpenMMLab projects, including mmcv and other python packages.',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install openmim mim install mmdet',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Or you can still install MMDetection manually:',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': '1. Install mmcv-full.```pythonpip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/{cu_version}/ -ftorch_version}/index.html```(continues on next page)',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'Please replace {cu_version} and {torch_version} in the url to your desired one. For example, to install the latest mmcv- full with CUDA 11.0 and PyTorch 1.7.0, use the following command:',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install mmcv- full - f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/  $\\\\rightarrow$  index.html',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'See here for different versions of MMCV compatible to different PyTorch and CUDA versions.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'Optionally you can compile mmcv from source if you need to develop both mmcv and mmdet. Refer to the guide for details.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': '2. Install MMDetection.', 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'You can simply install mmdetection with the following command:',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': 'pip install mmdet', 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'or clone the repository and then install it:',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'git clone https://github.com/open- mmlab/mmdetection.git cd mmdetection pip install - r requirements/build.txt pip install - v - e . # or \"python setup.py develop\"',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': '3. Install extra dependencies for Instaboost, Panoptic Segmentation, LVIS dataset, or Albumentations.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': '```python# for instaboostpip install instaboostfast# for panoptic segmentationpip install git+https://github.com/cocodataset/panopticapi.git# for LVIS datasetpip install git+https://github.com/lvis- dataset/lvis- api.git# for albumentationspip install albumentations>=0.3.2 - - no- binary imgaug,albumentations```',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'a. When specifying -e or develop, MMDetection is installed on dev mode, any local modifications made to the code will take effect without reinstallation.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'b. If you would like to use opencv-python-headless instead of opencv-python, you can install it before installing MMCV.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'c. Some dependencies are optional. Simply running pip install -v -e . will only install the minimum runtime requirements. To use optional dependencies like albumentations and imagecorruptions either install them manually with pip install -r requirements/optional.txt or specify desired extras when calling pip (e.g. pip install -v -e . [optional]). Valid keys for the extras field are: all, tests, build, and optional.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'd. If you would like to use albumentations, we suggest using pip install albumentations>=0.3.2 - - no-binary imgaug,albumentations. If you simply use pip install albumentations>=0.3.2, it will install opencv-python-headless simultaneously (even though you have already installed opencv-python). We should not allow opencv-python and opencv-python-headless installed at the same time, because it might cause unexpected issues. Please refer to official documentation for more details.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': '2.3 Install without GPU support',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': \"MMDetection can be built for CPU only environment (where CUDA isn't available).\",\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'In CPU mode you can run the demo/webcam_demo.py for example. However some functionality is gone in this mode:',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Deformable Convolution Modulated Deformable Convolution ROI pooling Deformable ROI pooling CARAFE: Content- Aware ReAssembly of FFeatures SyncBatchNorm CrissCrossAttention: Criss- Cross Attention MaskedConv2d Temporal Interlace Shift nms_cuda sigmoid_focal_loss_cuda bbox_overlapss',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'If you try to run inference with a model containing above ops, an error will be raised. The following table lists affected algorithms.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Notice: MMDetection does not support training with CPU for now.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': '2.4 Another option: Docker Image',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide a Dockerfile to build an image. Ensure that you are using docker version  $> = 19.03$',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'build an image with PyTorch 1.6, CUDA 10.1 docker build - t mmdetection docker/',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text', 'text': 'Run it with', 'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'docker run - - gpus all - - shm- size  $= 8g$  - it - v {DATA_DIR}: /mmdetection/data mmdetection',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': '2.5 A from-scratch setup script',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming that you already have CUDA 10.1 installed, here is a full script for setting up MMDetection with conda.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'conda create - n openmmlab python  $= 3$  .7 - y conda activate openmmlab conda install pytorch  $= = 1$  .6.0 torchvision  $= = 0$  .7.0 cudatoolkit  $= 10$  .1 - c pytorch - y # install the latest mmcv pip install mmcv- full - f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index. html (continues on next page)',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'install mmdetection git clone https://github.com/open- mmlab/mmdetection.git cd mmdetection pip install - r requirements/build.txt pip install - v - e.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': '2.6 Developing with multiple MMDetection versions',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'The train and test scripts already modify the PYTHONPATH to ensure the script use the MMDetection in the current directory.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'To use the default MMDetection installed in the environment rather than that you are working with, you can remove the following line in those scripts',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text', 'text': 'VERIFICATION', 'text_level': 1, 'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': 'To verify whether MMDetection is installed correctly, we can run the following sample code to initialize a detector and inference a demo image.',\n",
       "  'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': \"from mmdet.apis import init_detector, inference_detector config_file  $=$  'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' # download the checkpoint from model zoo and put it in checkpoints/ # url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_  $\\\\leftrightarrow$  1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth checkpoint_file  $=$  'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth' device  $=$  'cuda:0' # init a detector model  $=$  initDetector(config_file, checkpoint_file, device  $\\\\coloneqq$  device) # inference the demo image inference_detector(model, 'demo/demo.jpg')\",\n",
       "  'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': 'The above code is supposed to run successfully upon you finish the installation.',\n",
       "  'page_idx': 14},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': 'BENCHMARK AND MODEL ZOO',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text', 'text': '4.1 Mirror sites', 'text_level': 1, 'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'We only use aiyun to maintain the model zoo since MMDetection V2.0. The model zoo of V1. x has been deprecated.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '4.2 Common settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '- All models were trained on coco_2017_train, and tested on the coco_2017_val.- We use distributed training.- All pytorch-style pretrained backbones on ImageNet are from PyTorch model zoo, caffe-style pretrained backbones are converted from the newly released model from detectron2.- For fair comparison with other codebases, we report the GPU memory as the maximum value of torch.cuda.max_memory_allocated() for all 8 GPUs. Note that this value is usually less than what nvidia-smi shows.- We report the inference time as the total time of network forwarding and post-processing, excluding the data loading time. Results are obtained with the script benchmark.py which computes the average time on 2000 images.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '4.3 ImageNet Pretrained Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'It is common to initialize from backbone models pre- trained on ImageNet classification task. All pre- trained model links can be found at open_mmlab. According to img_norm_cfg and source of weight, we can divide all the ImageNet pre- trained model weights into some cases:',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': '- TorchVision: Corresponding to torchvision weight, including ResNet50, ResNet101. The img_norm_cfg is dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True).- Pycis: Corresponding to pycis weight, including RegNetX. The img_norm_cfg is dict(mean=[103.530, 116.280, 123.675], std=[57.375, 57.12, 58.395], to_rgb=False).- MSRA styles: Corresponding to MSRA weights, including ResNet50_Caffe and ResNet101_Caffe. The img_norm_cfg is dict(mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False).- Caffe2 styles: Currently only contains ResNext101_32x8d. The img_norm_cfg is dict(mean=[103.530, 116.280, 123.675], std=[57.375, 57.120, 58.395], to_rgb=False).',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Other styles: E.g SSD which corresponds to img_norm_cfg is dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True) and YOLOv3 which corresponds to img_norm_cfg is dict(mean=[0, 0, 0], std=[255. , 255. , 255. ], to_rgb=True).',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'The detailed table of the commonly used backbone models in MMDetection is listed below :',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4 Baselines', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.1 RPN', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text', 'text': 'Please refer to RPN for details.', 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.2 Faster R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Faster R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.3 Mask R-CNN', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Mask R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.4 Fast R-CNN (with pre-computed proposals)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Fast R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.5 RetinaNet', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to RetinaNet for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.6 Cascade R-CNN and Cascade Mask R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Cascade R- CNN for details.',\n",
       "  'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.7 Hybrid Task Cascade (HTC)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 17},\n",
       " {'type': 'text', 'text': 'Please refer to HTC for details.', 'page_idx': 17},\n",
       " {'type': 'text', 'text': '4.4.8 SSD', 'text_level': 1, 'page_idx': 17},\n",
       " {'type': 'text', 'text': 'Please refer to SSD for details.', 'page_idx': 17},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.9 Group Normalization (GN)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Group Normalization for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.10 Weight Standardization',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Weight Standardization for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.11 Deformable Convolution v2',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Deformable Convolutional Networks for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.12 CARAFE: Content-Aware ReAssembly of FEatures',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CARAFE for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.13 Instaboost',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Instaboost for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.14 Libra R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Libra R- CNN for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.15 Guided Anchoring',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Guided Anchoring for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text', 'text': '4.4.16 FCOS', 'text_level': 1, 'page_idx': 18},\n",
       " {'type': 'text', 'text': 'Please refer to FCOS for details.', 'page_idx': 18},\n",
       " {'type': 'text', 'text': '4.4.17 FoveaBox', 'text_level': 1, 'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to FoveaBox for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text', 'text': '4.4.18 RepPoints', 'text_level': 1, 'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to RepPoints for details.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.19 FreeAnchor',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to FreeAnchor for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.20 Grid R-CNN (plus)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Grid R- CNN for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.21 GHM', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text', 'text': 'Please refer to GHM for details.', 'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.22 GCNet', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to GCNet for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.23 HRNet', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to HRNet for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.24 Mask Scoring R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Mask Scoring R- CNN for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.25 Train from Scratch',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Rethinking ImageNet Pre- training for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.26 NAS-FPN', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to NAS- FPN for details.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.27 ATSS', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text', 'text': 'Please refer to ATSS for details.', 'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.28 FSAF', 'text_level': 1, 'page_idx': 19},\n",
       " {'type': 'text', 'text': 'Please refer to FSAF for details.', 'page_idx': 19},\n",
       " {'type': 'text', 'text': '4.4.29 RegNetX', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to RegNet for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.30 Res2Net', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Res2Net for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.31 GRoIE', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to GRoIE for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.32 Dynamic R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Dynamic R- CNN for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.33 PointRend', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to PointRend for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.34 DetectorRS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to DetectorRS for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.35 Generalized Focal Loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Generalized Focal Loss for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.36 CornerNet', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CornerNet for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.37 YOLOv3', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to YOLOv3 for details.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.38 PAA', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text', 'text': 'Please refer to PAA for details.', 'page_idx': 20},\n",
       " {'type': 'text', 'text': '4.4.39 SABL', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text', 'text': 'Please refer to SABL for details.', 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.40 CentripetalNet',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CentripetalNet for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.41 ResNeSt', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to ResNeSt for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.42 DETR', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text', 'text': 'Please refer to DETR for details.', 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.43 Deformable DETR',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Deformable DETR for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.44 AutoAssign',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to AutoAssign for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.45 YOLOF', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to YOLOF for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.46 Seesaw Loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to Seesaw Loss for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.47 CenterNet', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to CenterNet for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.48 YOLOX', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to YOLOX for details.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': '4.4.49 PVT', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text', 'text': 'Please refer to PVT for details.', 'page_idx': 22},\n",
       " {'type': 'text', 'text': '4.4.50 SOLO', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text', 'text': 'Please refer to SOLO for details.', 'page_idx': 22},\n",
       " {'type': 'text', 'text': '4.4.51 QueryInst', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to QueryInst for details.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.52 Other datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We also benchmark some methods on PASCAL VOC, Cityscapes and WIDER FACE.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.53 Pre-trained Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.4.53 Pre- trained ModelsWe also train Faster R- CNN and Mask R- CNN using ResNet- 50 and RegNetX- 3.2G with multi- scale training and longer schedules. These models serve as strong pre- trained models for downstream tasks for convenience.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.5 Speed benchmark',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.5.1 Training Speed benchmark',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide analyze_logs.py to get average time of iteration in training. You can find examples in Log Analysis.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We compare the training speed of Mask R- CNN with some other popular frameworks (The data is copied from detectron2). For mmdetection, we benchmark with mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1. py, which should have the same setting with mask_rcnn_R_50_FPN_noaug_1x.yaml of detectron2. We also provide the checkpoint and training log for reference. The throughput is computed as the average throughput in iterations 100- 500 to skip GPU warmup time.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.5.2 Inference Speed Benchmark',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide benchmark.py to benchmark the inference latency. The script benchmarks the model with 2000 images and calculates the average time ignoring first 5 times. You can change the output log interval (defaults: 50) by setting LOG- INTERVAL.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'python toools/benchmark.py \\\\ ${CONFIG}$ {CHECKPOINT}[- - log- interval \\\\$[LOG- INTERVAL]] [- - fuse- conv- bn]',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'The latency of all models in our model zoo is benchmarked without setting fuse- conv- bn, you can get a lower latency by setting it.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '4.6 Comparison with Detector2',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': 'We compare mmdetection with Detector2 in terms of speed and performance. We use the commit id 185c27e(30/4/2020) of detectron. For fair comparison, we install and run both frameworks on the same machine.',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text', 'text': '4.6.1 Hardware', 'text_level': 1, 'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '- 8 NVIDIA Tesla V100 (32G) GPUs- Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.2 Software environment',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '- Python 3.7- PyTorch 1.4- CUDA 10.1- CUDNN 7.6.03- NCCL 2.4.08',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.3 Performance',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.4 Training Speed',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': 'The training speed is measured with s/iter. The lower, the better.',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.5 Inference Speed',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': 'The inference speed is measured with fps (img/s) on a single GPU, the higher, the better. To be consistent with Detectron2, we report the pure inference speed (without the time of data loading). For Mask R- CNN, we exclude the time of RLE encoding in post- processing. We also include the officially reported speed in the parentheses, which is slightly higher than the results tested on our server due to differences of hardwares.',\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '4.6.6 Training memory',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 23},\n",
       " {'type': 'text',\n",
       "  'text': '1: INFERENCE AND TRAIN WITH EXISTING MODELS AND STANDARD DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection provides hundreds of existing and existing detection models in Model Zoo), and supports multiple standard datasets, including Pascal VOC, COCO, CityScapes, LVIS, etc. This note will show how to perform common tasks on these existing models and standard datasets, including:',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'Use existing models to inference on given images. Test existing models on standard datasets. Train predefined models on standard datasets.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': '5.1 Inference with existing models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'By inference, we mean using trained models to detect objects on images. In MMDetection, a model is defined by a configuration file and existing model parameters are save in a checkpoint file.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'To start with, we recommend Faster RCNN with this configuration file and this checkpoint file. It is recommended to download the checkpoint file to checkpoints directory.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': '5.1.1 High-level APIs for inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection provide high- level Python APIs for inference on images. Here is an example of building the model and inference on given images or videos.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': \"from mmdet.apis import initDetector, inferenceDetector import mmcv # Specify the path to model config and checkpoint file config_file  $=$  'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' checkpoint_file  $=$  'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth' # build the model from a config file and a checkpoint file model  $=$  initDetector(config_file, checkpoint_file, device  $=$  cuda:0') # test a single image and show the results img  $=$  'test.jpg' # or img  $=$  mmcv.imread(img), which will only load it once result  $=$  inferenceDetector(model, img) # visualize the results in a new window model.show_result(img, result)\",\n",
       "  'page_idx': 24},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': \"or save the visualization results to image files model.show_result(img, result, out_file  $\\\\equiv$  'result.jpg') # test a video and show the results video  $=$  mmcv.VideoReader('video.mp4') for frame in video: result  $=$  inference_detector(model, frame) model.show_result(frame, result, wait_time  $= 1$\",\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'A notebook demo can be found in demo/inference_demo.ipynb.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: inference_detector only supports single- image inference for now.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': '5.1.2 Asynchronous interface - supported for Python 3.7+',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'For Python  $3.7+$  , MMDetection also supports async interfaces. By utilizing CUDA streams, it allows not to block CPU on GPU bound inference code and enables better CPU/GPU utilization for single- threaded application. Inference can be done concurrently either between different input data samples or between different models of some inference pipeline.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': 'See tests/async_benchmark.py to compare the speed of synchronous and asynchronous interfaces.',\n",
       "  'page_idx': 25},\n",
       " {'type': 'text',\n",
       "  'text': \"import asyncio import torch from mmdet.apis import init_detector, async_inference_detector from mmdet.utils.contextmanagers import concurrent async def main(): config_file  $=$  'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' checkpoint_file  $=$  'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth' device  $=$  'cuda:0' model  $=$  initDetector(config_file, checkpoint  $=$  checkpoint_file, device  $=$  device) # queue is used for concurrent inference of multiple images streamqueue  $=$  asyncio.Queue() # queue size defines concurrency level streamqueue_size  $= 3$  for _ in range(streamqueue_size): streamqueue.put_nowait(torch.cuda.Stream(device  $=$  device)) # test a single image and show the results img  $=$  'test.jpg' # or img  $=$  mmcv.inread(img), which will only load it once async with concurrent(streamqueue): result  $=$  await async_inference_detector(model, img) # visualize the results in a new window model.show_result(img, result) # or save the visualization results to image files model.show_result(img, result, out_file  $=$  result.jpg')\",\n",
       "  'page_idx': 25},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 26},\n",
       " {'type': 'text', 'text': '5.1.3 Demos', 'text_level': 1, 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'We also provide three demo scripts, implemented with high- level APIs and supporting functionality codes. Source codes are available here.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Image demo', 'text_level': 1, 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'This script performs inference on a single image.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/e6d0b39ec58bce120f37a9425a06f8f03ed952d0e569c7fea4d33e88de3cdc1f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>python demo/image_demo.py</td><td>\\\\</td></tr><tr><td>$ {IMAGE_FILE} \\\\</td><td></td></tr><tr><td>$ {CONFIG_FILE} \\\\</td><td></td></tr><tr><td>$ {CHECKPOINT_FILE} \\\\</td><td></td></tr><tr><td>[--device]{GPU_ID} \\\\</td><td></td></tr><tr><td>[--score-thr]{SCORE_THR} ]</td><td></td></tr></table>\\n\\n\\n</figure>',\n",
       "  'page_idx': 26,\n",
       "  'outline': [69, 250, 543, 328]},\n",
       " {'type': 'text', 'text': 'Examples:', 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/image_demo.py demo/demo.jpg \\\\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\ - - device cpu',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Webcam demo', 'text_level': 1, 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'This is a live demo from a webcam.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/webcam_demo.py \\\\ \\\\\\\\({CONFIG_FILE} \\\\ \\\\\\\\){CHECKPOINT_FILE} \\\\ \\\\\\\\({GPU_ID} \\\\ \\\\\\\\({camera - id}\\\\){CAMERA- ID} \\\\ \\\\\\\\({score - thr}\\\\){SCORE_THR} ]',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Examples:', 'page_idx': 26},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/webcam_demo.py \\\\ \\\\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth',\n",
       "  'page_idx': 26},\n",
       " {'type': 'text', 'text': 'Video demo', 'text_level': 1, 'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'This script performs inference on a video.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/video_demo.py \\\\\\\\\\\\({VIDEO_FILE}\\\\)CONFIG_FILE\\\\({CHECKPOINT_FILE}\\\\)- - device}\\\\({GPU_ID}\\\\)\\\\[-- score - th \\\\){SCORE THR}\\\\[-- out \\\\){OUT_FILE}\\\\]- - show\\\\[- - wait - time \\\\){WAIT_TIME}',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text', 'text': 'Examples:', 'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'python demo/video_demo.py demo/demo.mp4 \\\\configigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\- - out result.mp4',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': '5.2 Test existing models on standard datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': \"To evaluate a model's accuracy, one usually tests the model on some standard datasets. MMDetection supports multiple public datasets including COCO, Pascal VOC, CityScapes, and more. This section will show how to test existing models on supported datasets.\",\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.1 Prepare datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'Public datasets like Pascal VOC or mirror and COCO are available from official websites or mirrors. Note: In the detection task, Pascal VOC 2012 is an extension of Pascal VOC 2007 without overlap, and we usually use them together. It is recommended to download and extract the dataset somewhere outside the project directory and symlink the dataset root to  $\\\\mathfrak{S}$  MMDETECTION/data as below. If your folder structure is different, you may need to change the corresponding paths in config files.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/ad9c1df455c01d871b0a3e0d05a7a0ad0316692add45118b9141403152b3e279.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 27,\n",
       "  'outline': [67, 514, 543, 712]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 27},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/70ba71c7b94a8656df7fbdd69e7f51dbe6061d2218b0f6ab7349d2ef5802c46b.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 84, 543, 150]},\n",
       " {'type': 'text',\n",
       "  'text': 'Some models require additional COCO- stuff datasets, such as HTC, DetectoRS and SCNet, you can download and unzip then move to the coco folder. The directory should be like this.',\n",
       "  'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/015a6e28a7221a9e77123ee0fa0399f958bf04d0bf3c128b0fcaa48b23784458.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 190, 543, 293]},\n",
       " {'type': 'text',\n",
       "  'text': 'Panoptic segmentation models like PanopticFPN require additional COCO Panoptic datasets, you can download and unzip then move to the coco annotation folder. The directory should be like this.',\n",
       "  'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/7c29cab430344af1ab97e97bafaec3cb824e4b3b4dd06fb19c7cc3e4952da12e.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 330, 543, 468]},\n",
       " {'type': 'text',\n",
       "  'text': 'The cityscapes annotations need to be converted into the coco format using tools/dataset_converters/cityscapes.py:',\n",
       "  'page_idx': 28},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/70efa7e3b477764b3373f407470301859325b127d4d20163f978002d2faf2e51.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [67, 507, 543, 586]},\n",
       " {'type': 'text', 'text': 'TODO: CHANGE TO THE NEW PATH', 'page_idx': 28},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.2 Test existing models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide testing scripts for evaluating an existing model on the whole dataset (COCO, PASCAL VOC, Cityscapes, etc.). The following testing environments are supported:',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'single GPU single node multiple GPUs multiple nodes',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Choose the proper script to perform testing depending on the testing environment.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'single- gpu testing python tools/test.py \\\\ \\\\\\\\({CONFIG_FILE} \\\\ \\\\\\\\){CHECKPOINT_FILE} \\\\ [--out \\\\\\\\({RESULT_FILE} \\\\ [--eval \\\\\\\\){EVAL_METRICS}} \\\\ [--show] # multi- gpu testing bash tools/dist_test.sh \\\\ \\\\\\\\({CONFIG_FILE} \\\\ \\\\\\\\){CHECKPOINT_FILE} \\\\ \\\\\\\\({GPU_NUM} \\\\ [--out \\\\\\\\){RESULT_FILE} \\\\ [--eval \\\\\\\\({EVAL_METRICS}}',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"tools/dist_test. sh also supports multi- node testing, but relies on PyTorch's launch utility.\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text', 'text': 'Optional arguments:', 'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"- RESULT_FILE: Filename of the output results in pickle format. If not specified, the results will not be saved to a file.- EVAL_METRICS: Items to be evaluated on the results. Allowed values depend on the dataset, e.g., proposal_fast, proposal, bbox, segm are available for COCO, mAP, recall for PASCAL VOC. Cityscapes could be evaluated by cityscapes as well as all COCO metrics.- --show: If specified, detection results will be plotted on the images and shown in a new window. It is only applicable to single GPU testing and used for debugging and visualization. Please make sure that GUI is available in your environment. Otherwise, you may encounter an error like cannot connect to X server.- --show-dir: If specified, detection results will be plotted on the images and saved to the specified directory. It is only applicable to single GPU testing and used for debugging and visualization. You do NOT need a GUI available in your environment for using this option.- --show-score-thr: If specified, detections with scores below this threshold will be removed.- --cfg-options: if specified, the key-value pair optional cfg will be merged into config file.- --eval-options: if specified, the key-value pair optional eval cfg will be kwargs for dataset.evaluate() function, it's only for evaluation\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text', 'text': '5.2.3 Examples', 'text_level': 1, 'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming that you have already downloaded the checkpoints to the directory checkpoints/.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '1. Test Faster R-CNN and visualize the results. Press any key for the next image. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py \\\\ config/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\ - - show',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '2. Test Faster R-CNN and save the painted images for future visualization. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py \\\\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130- 047c8118. pth \\\\ - - show- dir faster_rcnn_r50_fpn_1x_recults',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '3. Test Faster R-CNN on PASCAL VOC (without saving the test results) and evaluate the mAP. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py \\\\ configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc.py \\\\ checkpoints/faster_rcnn_r50_fpn_1x_voc0712_20200624- c9895d40. pth \\\\ - - eval mAP',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '4. Test Mask R-CNN with 8 GPUs, and evaluate the bbox and mask AP. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\ 8 \\\\ - - out results.pkl \\\\ - - eval bbox segm',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '5. Test Mask R-CNN with 8 GPUs, and evaluate the classwise bbox and mask AP. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\ 8 \\\\ - - out results.pkl \\\\ - - eval bbox segm \\\\ - - options \"classwise=True\"',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '6. Test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files for submitting to the official evaluation server. Config and checkpoint files are available here.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '5.2. Test existing models on standard datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 30},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/a94edb4aadcf4812cfc1d2e104fe1a71a02d9b044499e6dc4b03e41947c7ba50.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>8 \\\\</td></tr><tr><td>--format-only\\\\</td></tr><tr><td>--options &quot;jsonfile_prefix=./mask_rcnn_test-dev_results&quot;</td></tr></table>',\n",
       "  'page_idx': 31,\n",
       "  'outline': [91, 84, 542, 124]},\n",
       " {'type': 'text',\n",
       "  'text': 'This command generates two JSON files mask_rcnn_test- dev_results. bbox.json and mask_rcnn_test- dev_results.segm.json.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': '7. Test Mask R-CNN on Cityscapes test with 8 GPUs, and generate txt and png files for submitting to the official evaluation server. Config and checkpoint files are available here.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227- afe51d5a.pth \\\\ 8 \\\\ - - format- only \\\\ - - options \"txtfile_prefix  $=$  ./mask_rcnn_cityscapes_test_results\"',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'The generated png and txt would be under ./mask_rcnn_cityscapes_test_results directory.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.4 Test without Ground Truth Annotations',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection supports to test models without ground- truth annotations using CocoDataset. If your dataset format is not in COCO format, please convert them to COCO format. For example, if your dataset format is VOC, you can directly convert it to COCO format by the script in tools. If your dataset format is Cityscapes, you can directly convert it to COCO format by the script in tools. The rest of the formats can be converted using this script.',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/dataset_converters/images2coco.py \\\\ \\\\\\\\({IMG_PATH} \\\\ \\\\\\\\){CLASSES} \\\\ \\\\\\\\({OUT} \\\\ [ - - exclude - extensions]',\n",
       "  'page_idx': 31},\n",
       " {'type': 'text', 'text': 'arguments', 'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': \"- IMG_PATH: The root path of images.- CLASSES: The text file with a list of categories.- OUT: The output annotation json file name. The save dir is in the same directory as IMG_PATH.- exclude-extensions: The suffix of images to be excluded, such as 'png' and 'bmp'.\",\n",
       "  'page_idx': 31},\n",
       " {'type': 'text',\n",
       "  'text': 'After the conversion is complete, you can use the following command to test',\n",
       "  'page_idx': 31},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/fcc3da535e6975e30933ade2d428ca2f746e575fb8375fa3b7d82972ea7d046f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td># single-gpu testing\\npython tools/test.py \\\\ \\n{{CONFIG_FILE} \\\\ \\n{{CHECKPOINT_FILE} \\\\ \\n--format-only \\\\ \\n--options {{JSONFILE_PREFIX} \\\\ \\n[--show]}\\n# multi-gpu testing\\nbash tools/dist_test.sh \\\\ \\n{{CONFIG_FILE} \\\\}</td></tr></table>',\n",
       "  'page_idx': 31,\n",
       "  'outline': [67, 576, 543, 714]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 31},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': '${\\\\mathcal{S}}\\\\left\\\\{\\\\begin{array}{rl}\\\\end{array}\\\\right.$  CHECKPOINT_FILE} \\\\${GPU_NUM} \\\\- - format- only \\\\- - options \\\\${JSONFILE_PREFIX} \\\\[-- show\\\\]',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming that the checkpoints in the model zoo have been downloaded to the directory checkpoints/, we can test Mask R- CNN on COCO test- dev with 8 GPUs, and generate JSON files using the following command.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': './tools/dist_test.sh \\\\ config /mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\\ checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205- d4b0c5d6. pth \\\\ 8 \\\\ - format- only \\\\ - - options \"jsonfile_prefix=./mask_rcnn_test- dev_results\"',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'This command generates two JSON files mask_rcnn_test- dev_results.bbox.json and mask_rcnn_test- dev_results.segm.json.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.5 Batch Inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection supports inference with a single image or batched images in test mode. By default, we use single- image inference and you can use batch inference by modifying samples_per_gpu in the config of test data. You can do that either by modifying the config as below.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'data  $=$  dict(traitn=dict(...), val=dict(...), test=dict(samples_per_gpu=2, ...)',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Or you can set it through - - cfg- options as - - cfg- options data.test.samples_per_gpu=2',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': '5.2.6 Deprecated ImageToTensor',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"In test mode, ImageToTensor pipeline is deprecated, it's replaced by DefaultFormatBundle that recommended to manually replace it in the test data pipeline in your config file. examples:\",\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"use ImageToTensor (deprecated) pipelines  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict( type  $\\\\coloneqq$  'MultiScaleFlipAug', img_scale  $=$  (1333, 800), flip  $\\\\coloneqq$  False, transforms  $\\\\coloneqq$  [ dict(type  $\\\\coloneqq$  'Resize', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip'), dict(type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [0, 0, 0], std=[1, 1, 1]), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  'ImageToTensor', keys=['img']), dict(type  $\\\\coloneqq$  'Collect', keys=['img'], ]) ]\",\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"manually replace ImageToTensor to DefaultFormatBundle (recommended) pipelines  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict type  $\\\\coloneqq$  'MultiScaleFlipAug', img_scale  $\\\\coloneqq$  (1333, 800), flip  $\\\\coloneqq$  False, transforms  $\\\\coloneqq$  [ dict(type  $\\\\coloneqq$  'Resize', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip'), dict(type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [0, 0, 0], std=[1, 1, 1]), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), dict(type  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img'], ] ]\",\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': '5.3 Train predefined models on standard datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection also provides out- of- the- box tools for training detection models. This section will show how to train predefined models (under configs) on standard datasets i.e. COCO.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Important: The default learning rate in config files is for 8 GPUs and 2 img/gpu (batch size  $= 8^{*}2 = 16$  . According to the linear scaling rule, you need to set the learning rate proportional to the batch size if you use different GPUs or images per GPU, e.g.,  $\\\\mathbf{lr} = \\\\emptyset$  .01 for 4 GPUs \\\\* 2 imgs/gpu and  $\\\\mathbf{lr} = \\\\emptyset$  .08 for 16 GPUs \\\\* 4 imgs/gpu.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.1 Prepare datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Training requires preparing datasets too. See section Prepare datasets above for details.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: Currently, the config files under configs/cityscapes use COCO pretrained weights to initialize. You could download the existing models in advance if the network connection is unavailable or slow. Otherwise, it would cause errors at the beginning of training.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.2 Training on a single GPU',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide tools/train.py to launch training jobs on a single GPU. The basic usage is as follows.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/dbff7adc695fbd067625874a2b8fdfdf83cab8ef45283e3db0021466683fae7b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>python tools/train.py\\n  {{CONFIC_DIR:E} \\\\ \\n[optional arguments]}</td></tr></table>',\n",
       "  'page_idx': 33,\n",
       "  'outline': [69, 571, 543, 612]},\n",
       " {'type': 'text',\n",
       "  'text': 'During training, log files and checkpoints will be saved to the working directory, which is specified by work_dir in the config file or via CLI argument - - work- dir.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'By default, the model is evaluated on the validation set every epoch, the evaluation interval can be specified in the config file as shown below.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'evaluate the model every 12 epoch. evaluation  $=$  dict(interval  $= 12$',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'This tool accepts several optional arguments, including:',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': \"- --no-validate (not suggested): Disable evaluation during training.- --work-dir  ${{WORK_DIR}}$ : Override the working directory.- --resume-from  ${{CHECKPOINT_FILE}}$ : Resume from a previous checkpoint file.- --options 'Key-value': Overrides other settings in the used config.\",\n",
       "  'page_idx': 34},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Difference between resume- from and load- from:',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'resume- from loads both the model weights and optimizer status, and the epoch is also inherited from the specified checkpoint. It is usually used for resuming the training process that is interrupted accidentally. Load- from only loads the model weights and the training epoch starts from 0. It is usually used for finetuning.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.3 Training on multiple GPUs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'We provide tools/dist_train.sh to launch training on multiple GPUs. The basic usage is as follows.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'bash ./tools/dist_train.sh  ${{CONFIG_FILE}}\\\\ \\\\backslash$ ${{GPU_NUM}}\\\\ \\\\backslash$  [optional arguments]',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Optional arguments remain the same as stated above.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Launch multiple jobs simultaneously',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'If you would like to launch multiple jobs on a single machine, e.g., 2 jobs of 4- GPU training on a machine with 8 GPUs, you need to specify different ports (29500 by default) for each job to avoid communication conflict.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'If you use dist_train.sh to launch training jobs, you can set the port in commands.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'CUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh  ${{CONFIG_FILE}}\\\\ 4$  CUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh  ${{CONFIG_FILE}}\\\\ 4$',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.4 Training on multiple nodes',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': \"MMDetection relies on torch. distributed package for distributed training. Thus, as a basic usage, one can launch distributed training via PyTorch's launch utility.\",\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '5.3.5 Manage jobs with Slurm',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Slurm is a good job scheduling system for computing clusters. On a cluster managed by Slurm, you can use slurm_train.sh to spawn training jobs. It supports both single- node and multi- node training.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text', 'text': 'The basic usage is as follows.', 'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': '$[GPUS =$ \\\\{\\\\text{GPUS}\\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\} \\\\}',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'Below is an example of using 16 GPUs to train Mask R- CNN on a Slurm partition named dev, and set the work- dir to some shared file systems.',\n",
       "  'page_idx': 34},\n",
       " {'type': 'text',\n",
       "  'text': 'GPUS=16 ./tools/slurm_train.sh dev mask_r50_ix configs/mask_rcnn_r50_fpn_1x_coco.py /nfs/  $\\\\rightarrow$  xxxx/mask_rcnn_r50_fpn_1x',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'You can check the source code to review full arguments and environment variables.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'When using Slurm, the port option need to be set in one of the following ways:',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '1. Set the port through - - options. This is more recommended since it does not change the original configs.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': \"CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh \\\\(\\\\)\\\\{PARTITION\\\\}\\\\(\\\\)\\\\{JOB_NAME\\\\}\\\\(\\\\)\\\\rightarrow\\\\(config1. py\\\\)\\\\{\\\\(WORK_DIR\\\\}\\\\(- - options 'dist_params.port\\\\)=29500\\\\(\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,5,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\(\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(5,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathcal{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(7,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\)\",\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '2. Modify the config files to set different communication ports.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': \"In config1. py, set dist_params = dict(backend='nccl', port=29500)\",\n",
       "  'page_idx': 35},\n",
       " {'type': 'text', 'text': '', 'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': \"In config2. py, set dist_params = dict(backend='nccl', port=29501)\",\n",
       "  'page_idx': 35},\n",
       " {'type': 'text', 'text': '', 'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Then you can launch two jobs with config1. py and config2. py.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh \\\\(\\\\)\\\\{PARTITION\\\\}\\\\(\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\_ \\\\mathbb{D}\\\\mathbb{E}\\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{C}\\\\mathbb{E}\\\\mathbb{S}=4,\\\\\\\\ (5,6,7\\\\mathbb{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(5,6,7\\\\mathcal{G}\\\\mathbb{P}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(5,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\mathbb{S}=4\\\\)\\\\mathbb{C}\\\\mathbb{U}\\\\mathbb{D}\\\\mathbb{A}\\\\_ \\\\mathbb{V}\\\\mathbb{I}\\\\mathbb{S}\\\\mathbb{I}\\\\mathbb{B}\\\\mathbb{L}\\\\mathbb{E}\\\\mathrm{S}=4,\\\\\\\\(5,6,7\\\\mathcal{G}\\\\mathbb{T}\\\\mathbb{U}\\\\)',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '2: TRAIN WITH CUSTOMIZED DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, you will know how to inference, test, and train predefined models with customized datasets. We use the balloon dataset as an example to describe the whole process.The',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text', 'text': 'The basic steps are as below:', 'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '1. Prepare the customized dataset2. Prepare a config3. Train, test, inference models on the customized dataset.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '6.1 Prepare the customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'There are three ways to support a new dataset in MMDetection:',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '1. reorganize the dataset into COCO format.2. reorganize the dataset into a middle format.3. implement a new dataset.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'Usually we recommend to use the first two methods which are usually easier than the third.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, we give an example for converting the data into COCO format.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: MMDetection only supports evaluating mask AP of dataset in COCO format for now. So for instance segmentation task users should convert the data into coco format.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '6.1.1 COCO annotation format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': 'The necessary keys of COCO format for instance segmentation is as below, for the complete details, please refer here.',\n",
       "  'page_idx': 36},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/ba8442b856ebb73437c5390d8e3c3097de7495769f2ccd761ccb6bc60ea219c0.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>{\\n    &quot;images&quot;: [image],\\n    &quot;annotations&quot;: [annotation],\\n    &quot;categories&quot;: [category]\\n}</td></tr><tr><td>image = {\\n    &quot;id&quot;: int,\\n    &quot;width&quot;: int,\\n    &quot;height&quot;: int,</td></tr></table>',\n",
       "  'page_idx': 36,\n",
       "  'outline': [67, 574, 543, 712]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 36},\n",
       " {'type': 'text',\n",
       "  'text': '\"file_name\": str, } annotation  $\\\\begin{array}{rl}{=}&{\\\\left\\\\{ \\\\begin{array}{ll} \\\\end{array} \\\\right.}\\\\end{array}$  \"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1, } categories  $\\\\begin{array}{rl}{=}&{\\\\left\\\\{ \\\\begin{array}{ll} \\\\end{array} \\\\right.}\\\\end{array}$  \"id\": int, \"name\": str, \"supercategory\": str, }]',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume we use the balloon dataset. After downloading the data, we need to implement a function to convert the annotation format into the COCO format. Then we can use implemented COCODataset to load the data and perform training and evaluation.',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'If you take a look at the dataset, you will find the dataset format is as below:',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': \"['base64_img_data': '', 'file_attributes': {}, 'filename': '34020010494_e5cb88e1c4_k.jpg', 'fileref': '', 'regions': {'0': {'region_attributes': {}, 'shape_attributes': {'all_points_x': [1020, 1000, 994, 1003, 1023, 1050, 1089, 1134, 1190, 1265, 1321, 1361, 1403, 1428, 1442, 1445, 1441, 1427, 1400, 1361, 1316, 1269, 1228,\",\n",
       "  'page_idx': 37},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/bc5b5cb48a30e123cd0eb2b2428b73c74734efa74fd73e685e6bf9ad0883752c.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>1198,</td></tr><tr><td>1207,</td></tr><tr><td>1210,</td></tr><tr><td>1190,</td></tr><tr><td>1177,</td></tr><tr><td>1172,</td></tr><tr><td>1174,</td></tr><tr><td>1170,</td></tr><tr><td>1153,</td></tr><tr><td>1127,</td></tr><tr><td>1104,</td></tr><tr><td>1061,</td></tr><tr><td>1032,</td></tr><tr><td>1020,</td></tr><tr><td>&#x27;all_points_y&#x27;: [963,</td></tr><tr><td>899,</td></tr><tr><td>841,</td></tr><tr><td>787,</td></tr><tr><td>738,</td></tr><tr><td>700,</td></tr><tr><td>663,</td></tr><tr><td>638,</td></tr><tr><td>621,</td></tr><tr><td>619,</td></tr><tr><td>643,</td></tr><tr><td>672,</td></tr><tr><td>720,</td></tr><tr><td>765,</td></tr><tr><td>800,</td></tr><tr><td>860,</td></tr><tr><td>896,</td></tr><tr><td>942,</td></tr><tr><td>990,</td></tr><tr><td>1035,</td></tr><tr><td>1079,</td></tr><tr><td>1112,</td></tr><tr><td>1129,</td></tr><tr><td>1134,</td></tr><tr><td>1144,</td></tr><tr><td>1153,</td></tr><tr><td>1166,</td></tr><tr><td>1166,</td></tr><tr><td>1150,</td></tr><tr><td>1136,</td></tr><tr><td>1129,</td></tr><tr><td>1122,</td></tr><tr><td>1112,</td></tr><tr><td>1084,</td></tr><tr><td>1037,</td></tr><tr><td>989,</td></tr><tr><td>963],</td></tr><tr><td>&#x27;name&#x27;: &#x27;polygon&#x27;{{}},</td></tr></table>',\n",
       "  'page_idx': 38,\n",
       "  'outline': [67, 82, 546, 712]},\n",
       " {'type': 'text',\n",
       "  'text': \"The annotation is a JSON file where each key indicates an image's all annotations. The code to convert the balloon dataset into coco format is as below.\",\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': \"import os.path as osp def convert_balloon_to_coco(ann_file, out_file, image_prefix): data_infos  $=$  mmcv.load(ann_file) annotations  $=$  [] images  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\\\\end{array}\\\\right]}\\\\end{array}$  obj_count - 0 for idx, v in enumerate(mmcv.track_iter_progress(data_infos.values())): filename  $=$  v['filename'] img_path  $=$  osp.join(image_prefix, filename) height, width  $=$  mmcv.imread(img_path).shape[:2] images.append(dict(  $\\\\scriptstyle {\\\\dot{\\\\mathbf{1}}} = \\\\dot{\\\\mathbf{1}}\\\\mathbf{d}\\\\mathbf{x}$  file_name  $=$  filename, height  $=$  height, width  $=$  width)) bboxes  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\\\\end{array}\\\\right]}\\\\end{array}$  labels  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\\\\end{array}\\\\right]}\\\\end{array}$  masks  $\\\\begin{array}{rl}{=}&{\\\\left[\\\\begin{array}{llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll} \\\\end{array}\\\\right]}\\\\end{array}$  for - , obj in v['regions'].items(): assert not obj['region_attributes'] obj  $=$  obj['shape_attributes'] px  $=$  obj['all_points_x'] py  $=$  obj['all_points_y'] poly  $=$  [(x + 0.5, y + 0.5) for x, y in zip(px, py)] poly  $=$  [p for x in poly for p in x] x_min, y_min, x_max, y_max = ( min(px), min(py), max(px), max(py)) data anno  $=$  dict( image_id  $=$  idx, id  $=$  obj_count, category_id  $= 0$  bbox  $=$  [x_min, y_min, x_max - x_min, y_max - y_min], area  $=$  (x_max - x_min) \\\\* (y_max - y_min), segmentation  $=$  [poly], iscrowd  $= 0$  annotations.append(data anno) obj_count  $+ = 1$  coco_format_json  $=$  dict( images  $=$  images,\",\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Using the function above, users can successfully convert the annotation file into json format, then we can use CocoDataset to train and evaluate the model.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': '6.2 Prepare a config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'The second step is to prepare a config thus the dataset could be successfully loaded. Assume that we want to use Mask R- CNN with FPN, the config to train the detector on balloon dataset is as below. Assume the config is under directory configs/balloon/ and named as mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py, the config is as below.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': \"The new config inherits a base config to highlight the necessary modification _base_  $=$  'mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_coco.py' # We also need to change the num_classes in head to match the dataset's annotation model  $=$  dict( roi_head  $=$  dict( bbox_head  $=$  dict(num_classes  $= 1$  1 mask_head  $=$  dict(num_classes  $= 1$  1) # Modify dataset related settings dataset_type  $=$  'COCODataset classes  $=$  ('balloon',) data  $=$  dict( train  $=$  dict( img_prefix  $=$  'balloon/train/' classes  $=$  classes, ann_file  $=$  'balloon/train/annotation_coco.json'), val  $=$  dict( img_prefix  $=$  'balloon/val/' classes  $=$  classes, ann_file  $=$  'balloon/val/annotation_coco.json'), test  $=$  dict( img_prefix  $=$  'balloon/val/' classes  $=$  classes, ann_file  $=$  'balloon/val/annotation_coco.json')) # We can use the pre- trained Mask RCNN model to obtain higher performance load_from  $=$  'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain- poly_3x_coco_bbox_mAP- 0.408.  $\\\\hookrightarrow$  segm_mAP- 0.37_20200504_163245- 42aa3d00. pth\",\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': '6.3 Train a new model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'To train a model with the new config, you can simply run',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/train.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': '6.4 Test and inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'To test the trained model, you can simply run python tools/test.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py work_dirs/mask_rcnn_r50_caffe_fpn_mstrain- poly_1x_balloon.py/latest.pth - - eval bbox_ segm',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': '', 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': '3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, you will know how to train, test and inference your own customized models under standard datasets. We use the cityscapes dataset to train a customized Cascade Mask R- CNN R50 model as an example to demonstrate the whole process, which using AugFPN to replace the default FPN as neck, and add Rotate or Translate as training- time auto augmentation.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text', 'text': 'The basic steps are as below:', 'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': '1. Prepare the standard dataset  \\n2. Prepare your own customized model  \\n3. Prepare a config  \\n4. Train, test, and inference models on the standard dataset.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': '7.1 Prepare the standard dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'In this note, as we use the standard cityscapes dataset as an example.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'It is recommended to symlink the dataset root to $MMDETECTION/data. If your folder structure is different, you may need to change the corresponding paths in config files.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/c85bc2a8c1e1d6ed6f5b4f94da866d6033782be48fb212176e2278b635bf56c3.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 42,\n",
       "  'outline': [67, 467, 544, 701]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'The cityscapes annotations have to be converted into the coco format using tools/dataset_converters/cityscapes.py:',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install cityscapesscriptspython tools/dataset_converters/cityscapes.py ./data/cityscapes - - nproc 8 - - out- dir ./data/cityscapes/annotations',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Currently the config files in cityscapes use COCO pre- trained weights to initialize. You could download the pre- trained models in advance if network is unavailable or slow, otherwise it would cause errors at the beginning of training.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': '7.2 Prepare your own customized model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': '7.2 Prepare your own customized modelThe second step is to use your own module or training setting. Assume that we want to implement a new neck called AugFPN to replace with the default FPN under the existing detector Cascade Mask R- CNN R50. The following implementsAugFPN under MMDetection.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': '7.2.1 1. Define a new neck (e.g. AugFPN)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Firstly create a new file mmdet/models/necks/augfpn.py.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c43e20e1493bc2dab0a74f6522c2a9287699fe28d587916616807063474614cc.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from .builder import NECKS</td></tr><tr><td>@NECKS.register_module()</td></tr><tr><td>class AugFPN(nn.Module):</td></tr><tr><td>def __init__(self, \\n    in_channels, \\n    out_channels, \\n    num_outs, \\n    start_level=0, \\n    end_level=-1, \\n    add_extra_conv=False):</td></tr><tr><td>pass</td></tr><tr><td>def forward(self, inputs):</td></tr><tr><td># implementation is ignored</td></tr><tr><td>pass</td></tr></table>',\n",
       "  'page_idx': 43,\n",
       "  'outline': [67, 384, 544, 596]},\n",
       " {'type': 'text',\n",
       "  'text': '7.2.2 2. Import the module',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'You can either add the following line to mmdet/models/necks/__init__.py,',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text', 'text': 'from .augfpn import AugFPN', 'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': \"or alternatively add custom_imports  $=$  dict( imports  $=$  ['mmdet.models.necks.augfpn.py'] allow_failed_imports  $=$  False)\",\n",
       "  'page_idx': 44},\n",
       " {'type': 'text', 'text': '', 'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'to the config file and avoid modifying the original code.',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': '7.2.3 3. Modify the config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': \"neck=dict( type  $\\\\coloneqq$  'AugFPN', in_channels  $=$  [256, 512, 1024, 2048], out_channels  $= 256$  num_outs  $= 5$\",\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages about customize your own models (e.g. implement a new backbone, head, loss, etc) and runtime training settings (e.g. define a new optimizer, use gradient clip, customize training schedules and hooks, etc), please refer to the guideline Customize Models and Customize Runtime Settings respectively.',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': '7.3 Prepare a config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': 'The third step is to prepare a config for your own training setting. Assume that we want to add AugFPN and Rotate or Translate augmentation to existing Cascade Mask R- CNN R50 to train the cityscapes dataset, and assume the config is under directory configs/cityscapes/ and named as cascade_mask_rcnn_r50_augfpn_autoaug_10e_cityscapes.py, the config is as below.',\n",
       "  'page_idx': 44},\n",
       " {'type': 'text',\n",
       "  'text': \"The new config inherits the base configs to highlight the necessary modification _base_ = [ '._base_/models/cascade_mask_rcnn_r50_fpn.py', '._base_/datasets/cityscapes_instance.py', '../base_/default_runtime.py'] model = dict( # set None to avoid loading ImageNet pretrained backbone, # instead here we set `load_from` to load from COCO pretrained detectors. backbone=dict(init_cfg=None), # replace neck from defaultly `FPN` to our new implemented module `AugFPN` neck=dict( type='AugFPN', in_channels=[256, 512, 1024, 2048], out_channels=256, num_outs=5), # We also need to change the num_classes in head from 80 to 8, to match the # cityscapes dataset's annotation. This modification involves `bbox_head` and `mask_head`.\",\n",
       "  'page_idx': 44},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/0af98f27eebb7d89b4c822400bf5668cb0512944beba4e7f574b5cad596309f6.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 45,\n",
       "  'outline': [67, 82, 495, 712]},\n",
       " {'type': 'text',\n",
       "  'text': \"use sigmoid=False, loss_weight=1.0), loss_bbox=dict(type='SmoothLoss', beta=1.0, loss_weight=1.0)) ], mask_head=dict( type=FCNMaskHead', num_conv=4, in_channels=256, conv_out_channels=256, # change the number of classes from defaultlY COcO to cityscapes num_classes=8, loss_mask=dict( type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))) # over- write `train_pipeline` for new added `AutoAugment` training setting img_norm_cfg = dict( mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True) train_pipeline = [ dict(type='LoadImageFromFile'), dict(type='LoadAnnotations', with_bbox=True, with_mask=True), dict( type='AutoAugment', policies=[ [dict( type='Rotate', level=5, img_fill_val=(124, 116, 104), prob=0.5, scale=1) ], [dict(type='Rotate', level=7, img_fill_val=(124, 116, 104)), dict( type='Translate', level=5, prob=0.5, img_fill_val=(124, 116, 104)) ], ]), dict( type='Resize', img_scale=[(2048, 800), (2048, 1024)], keep_ratio=True), dict(type='RandomFlip', flip_ratio=0.5), dict(type='Normalize', **img_norm_cfg), dict(type='Pad', size_divisor=32), dict(type='DefaultFormatBundle'), dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']), ] # set batch_size per gpu, and set new training pipeline data = dict( samples_per_gpu=1, workers_per_gpu=3, # over- write `pipeline` with new training pipeline setting\",\n",
       "  'page_idx': 46},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': \"train=dict(dataset=dict(pipeline=train_pipeline))) # Set optimizer optimizer  $=$  dict(type  $\\\\coloneqq$  'SGD',  $\\\\mathtt{lr} = \\\\mathtt{0}$  .01, momentum  $= 0$  .9, weight_decay  $= 0$  .0001) optimizer_config  $=$  dict(grad clip  $\\\\equiv$  None) # Set customized learning policy lr_config  $=$  dict( policy  $\\\\coloneqq$  'step', warmup  $=$  'linear', warmup_iter  $= 500$  warmup_ratio  $= 0$  .001, step  $\\\\coloneqq$  [8]) runner  $=$  dict(type  $=$  'EpochBasedRunner', max_epochs  $= 10$  # We can use the COcO pretrained Cascade Mask R- CNN R50 model for more stable.  $\\\\hookrightarrow$  performance initialization load_from  $=$  'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_  $\\\\hookrightarrow$  rcnn_r50_fpn_lx_coco/cascade_mask_rcnn_r50_fpn_lx_coco_20200203- 9d4dcb24. pth'\",\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': '7.4 Train a new model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'To train a model with the new config, you can simply run',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/train.py configs/cityscapes/cascade_mask_rcnn_r50_augfpn_autoaug_10e_  $\\\\hookrightarrow$  cityscapes.py',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': '7.5 Test and inference',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'To test the trained model, you can simply run',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/test.py configs/cityscapes/cascade_mask_rcnn_r50_augfpn_autoaug_10e_  $\\\\hookrightarrow$  cityscapes.py work_dirs/cascade_mask_rcnn_r50_augfpn_autoaug_10e_cityscapes.py/latest.  $\\\\hookrightarrow$  pth - - eval bbox segm',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'For more detailed usages, please refer to the Case 1.',\n",
       "  'page_idx': 47},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 1: LEARN ABOUT CONFIGS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'We incorporate modular and inheritance design into our config system, which is convenient to conduct various experiments. If you wish to inspect the config file, you may run python tools/misc/print_config.py /PATH/TO/ CONFIG to see the complete config.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '8.1 Modify config through script arguments',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'When submitting jobs using \"tools/train.py\" or \"tools/test.py\", you may specify - - cfg- options to in- place modify the config.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '- Update config keys of dict chains. The config options can be specified following the order of the dict keys in the original config. For example, \\n--cfg-options model.backbone.norm_eval=False changes the all BN modules in model backbones to train mode.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '- Update keys inside a list of configs.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': \"Some config dicts are composed as a list in your config. For example, the training pipeline data.train. pipeline is normally a list e.g. [dict(type  $\\\\equiv$  'LoadImageFromFile'), ...]. If you want to change 'LoadImageFromFile' to 'LoadImageFromWebcam' in the pipeline, you may specify - - cfg- options data.train.pipeline.0. type  $\\\\equiv$  LoadImageFromFileWebcam.\",\n",
       "  'page_idx': 48},\n",
       " {'type': 'text', 'text': '- Update values of list/tuples.', 'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'If the value to be updated is a list or a tuple. For example, the config file normally sets workflow  $\\\\equiv$  [\\'train\\', 1)]. If you want to change this key, you may specify - - cfg- options workflow  $\\\\equiv$  \"[(train,1), (val,1)]\". Note that the quotation mark \" is necessary to support list/tuple data types, and that NO white space is allowed inside the quotation marks in the specified value.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': '8.2 Config File Structure',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'There are 4 basic component types under config/_base_ dataset, model, schedule, default_runtime. Many methods could be easily constructed with one of each like Faster R- CNN, Mask R- CNN, Cascade R- CNN, RPN, SSD. The configs that are composed by components from _base_ are called primitive.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'For all configs under the same folder, it is recommended to have only one primitive config. All other configs should inherit from the primitive config. In this way, the maximum of inheritance level is 3.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'For easy understanding, we recommend contributors to inherit from existing methods. For example, if some modification is made base on Faster R- CNN, user may first inherit the basic Faster R- CNN structure by specifying _base_ = ../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py, then modify the necessary fields in the config files.',\n",
       "  'page_idx': 48},\n",
       " {'type': 'text',\n",
       "  'text': 'If you are building an entirely new method that does not share the structure with any of the existing methods, you may create a folder xxx_rcnn under configs,',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to mmcw for detailed documentation.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '8.3 Config Name Style',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'We follow the below style to name config files. Contributors are advised to follow the same style.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '{model}_5model setting]{backbone}{neck}_norm setting}{misc}{gpu x batch_per_gpu}  $\\\\rightarrow$  {schedule}{dataset}',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'xxx} is required field and [yyy] is optional.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '- {model}: model type like faster_rcnn, mask_rcnn, etc.- [model setting]: specific setting for some model, like without_semantic for htc, moment for reppoints, etc.- {backbone}: backbone type like r50 (ResNet-50), x101 (ResNeXt-101).- {neck}: neck type like fpn, pafpn, nasfpn, c4.- [norm_setting]: bn (Batch Normalization) is used unless specified, other norm layer type could be gn (Group Normalization), syncbn (Synchronized Batch Normalization). gn-head/gn-neck indicates GN is applied in head/neck only, while gn-all means GN is applied in the entire model, e.g. backbone, neck, head.- [misc]: miscellaneous setting/plugins of model, e.g. dconv, gcb, attention, albu, mstrain.- [gpu x batch_per_gpu]: GPUs and samples per GPU, 8xz is used by default.- {schedule}: training schedule, options are 1x, 2x, 20e, etc. 1x and 2x means 12 epochs and 24 epochs respectively. 20e is adopted in cascade models, which denotes 20 epochs. For 1x/2x, initial learning rate decays by a factor of 10 at the 8/16th and 11/22th epochs. For 20e, initial learning rate decays by a factor of 10 at the 16th and 19th epochs.- {dataset}: dataset like coco, cityscapes, voc_0712, wider_face.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '8.4 Deprecated train_cfg/test_cfg',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'The train_cfg and test_cfg are deprecated in config file, please specify them in the model config. The original config structure is as below.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '```c# deprecatedmodel = dict(    type=...,    ...)train_cfg=dict(...)test_cfg=dict(...)```',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': 'The migration example is as below.',\n",
       "  'page_idx': 49},\n",
       " {'type': 'text',\n",
       "  'text': '```python# recommendedmodel = dict(    type=...,    ...    train_cfg=dict(...),    test_cfg=dict(...),)```',\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': '8.5 An Example of Mask R-CNN',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': 'To help the users have a basic idea of a complete config and the modules in a modern detection system, we make brief comments on the config of Mask R- CNN using ResNet50 and FPN as the following. For more detailed usage and the corresponding alternative for each module, please refer to the API documentation.',\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': \"```pythonmodel = dict(    type='MaskRCNN',    # The name of detector    backbone=dict(    # The config of backbone        type='ResNet',    # The type of the backbone, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/backbones/resnet.py#L308 for more details.        depth=50,    # The depth of backbone, usually it is 50 or 101 for ResNet and.    ResNet backbones:        num stages=4,    # Number of stages of the backbone.        out_indices=(0, 1, 2, 3),    # The index of output feature maps produced in each.    stages        frozen stages=1,    # The weights in the first 1 stage are frozen        norm_cfg=dict(    # The config of normalization layers.        type='BN',    # Type of norm layer, usually it is BN or GN        requires_grad=True),    # Whether to train the gamma and beta in BN        norm_eval=True,    # Whether to freeze the statistics in BN        style='pytorch'    # The style of backbone, 'pytorch' means that stride 2 layers.    are in 3x3 conv, 'caffe' means stride 2 layers are in 1x1 conv.        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),    # The ImageNet pretrained backbone to be loaded        neck=dict(    type='FPN',    # The neck of detector is FPN. We also support 'NASFPN', 'PAFPN', etc. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/neck/sfpn.py#L10 for more details.        in_channels=[256, 512, 1024, 2048],    # The input channels, this is consistent.        with the output channels of backbone        out_channels=256,    # The output channels of each level of the pyramid feature map        num outs=5),    # The number of output scales        rpn_head=dict(    type='RPNHead',    # The type of RPN head is 'RPNHead', we also support 'GARPNHead', etc. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/dense_heads/ron_head.py#L12 for more details.        in_channels=256,    # The input channels of each input feature map, this is consistent with the output channels of neck        feat_channels=256,    # Feature channels of convolutional layers in the head.        anchor_generator=dict(    # The config of anchor generator        type='AnchorGenerator',    # Most of methods use AnchorGenerator, SSD.    Detectors uses `SSDAnchorGenerator`. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/core/anchor/anchor_generator.py#L10 for more details. )```\",\n",
       "  'page_idx': 50},\n",
       " {'type': 'text',\n",
       "  'text': \"(continued from previous page) scales  $\\\\coloneqq$  [8], # Basic scale of the anchor, the area of the anchor in one.  $\\\\rightharpoonup$  position of a feature map will be scale \\\\* base_sizes ratios  $\\\\coloneqq$  [0.5, 1.0, 2.0], # The ratio between height and width. strides  $\\\\coloneqq$  [4, 8, 16, 32, 64]), # The strides of the anchor generator. This is.  $\\\\rightharpoonup$  consistent with the FPN feature strides. The strides will be taken as base_sizes if.  $\\\\rightharpoonup$  base_sizes is not. bboxCoder  $\\\\coloneqq$  dict( # Config of box coder to encode and decode the boxes during.  $\\\\rightharpoonup$  training and testing type  $\\\\coloneqq$  'DeltaXYwHBBoxCoder', # Type of box coder. 'DeltaXYwHBBoxCoder' is.  $\\\\rightharpoonup$  applied for most of methods. Refer to https://github.com/open- mmlab/mmdetection/blob/  $\\\\rightharpoonup$  master/mmdet/core/bbox/coder/delta_xywh_bboxCoder.py#L9 for more details. target_means  $\\\\coloneqq$  [0.0, 0.0, 0.0, 0.0], # The target means used to encode and.  $\\\\rightharpoonup$  decode boxes target_std  $\\\\coloneqq$  [1.0, 1.0, 1.0, 1.0]], # The standard variance used to encode.  $\\\\rightharpoonup$  and decode boxes loss_cls  $\\\\coloneqq$  dict( # Config of loss function for the classification branch type  $\\\\coloneqq$  'CrossEntropyLoss', # Type of loss for classification branch, we also.  $\\\\rightharpoonup$  support FocalLoss etc. use sigmoid  $\\\\coloneqq$  True, # RPN usually perform two- class classification, so it.  $\\\\rightharpoonup$  usually uses sigmoid function. loss_weight  $\\\\coloneqq$  1.0), # Loss weight of the classification branch. loss_bbox  $\\\\coloneqq$  dict( # Config of loss function for the regression branch. type  $\\\\coloneqq$  'l1Loss', # Type of loss, we also support many IoU Losses and smooth.  $\\\\rightharpoonup$  L1- loss, etc. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/  $\\\\rightharpoonup$  models/losses/smooth_11_loss.py#L56 for implementation. loss_weight  $\\\\coloneqq$  1.0), # Loss weight of the regression branch. roi_head  $\\\\coloneqq$  dict( # RoIHead encapsulates the second stage of two- stage/cascade.  $\\\\rightharpoonup$  detectors. type  $\\\\coloneqq$  'StandardRoIHead', # Type of the RoI head. Refer to https://github.com/  $\\\\rightharpoonup$  open- mmlab/mmdetection/blob/master/mmdet/models/roi heads/standard_roi head.py#L10 for.  $\\\\rightharpoonup$  implementation. bbox ROI extractor  $\\\\coloneqq$  dict( # RoI feature extractor for bbox regression. type  $\\\\coloneqq$  'SingleRoIExtractor', # Type of the RoI feature extractor, most of.  $\\\\rightharpoonup$  methods uses SingleRoIExtractor. Refer to https://github.com/open- mmlab/mmdetection/  $\\\\rightharpoonup$  blob/master/mmdet/models/roi heads/roi extractors/single_level.py#L10 for details. roi_layer  $\\\\coloneqq$  dict( # Config of RoI Layer type  $\\\\coloneqq$  'RoIAlign', # Type of RoI Layer, DeformRoIPoolingPack and.  $\\\\rightharpoonup$  ModulatedDeformRoIPoolingPack are also supported. Refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/ops/roi_align/roi_align.py#L79 for details. output_size  $\\\\coloneqq$  7, # The output size of feature maps. sampling_ratio  $\\\\coloneqq$  0), # Sampling ratio when extracting the RoI features.  $\\\\theta_{\\\\perp}$ $\\\\rightharpoonup$  means adaptive ratio. out_channels  $= 256$  , # output channels of the extracted feature. featmap_strides  $\\\\coloneqq$  [4, 8, 16, 32]), # Strides of multi- scale feature maps. It.  $\\\\rightharpoonup$  should be consistent to the architecture of the backbone. bbox_head  $\\\\coloneqq$  dict( # Config of box head in the RoIHead. type  $\\\\coloneqq$  'Shared2FCBBoxHead', # Type of the bbox head, Refer to https://github. com/open- mmlab/mmdetection/blob/master/mmdet/models/roi heads/bbox heads/convfc_bbox_  $\\\\rightharpoonup$  head.py#L177 for implementation details. in_channels  $= 256$  , # Input channels for bbox head. This is consistent with.  $\\\\rightharpoonup$  the out_channels in roi extractor fc_out_channels  $= 1024$  , # Output feature channels of FC layers.\",\n",
       "  'page_idx': 51},\n",
       " {'type': 'text',\n",
       "  'text': \"roilfeat_size  $= 7$  , # Size of RoI features num_classes  $= 80$  , # Number of classes for classification bbok coder  $\\\\coloneqq$  dict( # Box coder used in the second stage. type  $\\\\coloneqq$  'DeltaXYwHBoxCoder', # Type of box coder. 'DeltaXYwHBoxCoder' is. applied for most of methods. target_means  $=$  [0.0, 0.0, 0.0, 0.0], # Means used to encode and decode box target stds  $=$  [0.1, 0.1, 0.2, 0.2]), # Standard variance for encoding and. decoding. It is smaller since the boxes are more accurate. [0.1, 0.1, 0.2, 0.2] is a. conventional setting. reg_class_agnostic  $=$  False, # Whether the regression is class agnostic. loss cls  $\\\\coloneqq$  dict( # Config of loss function for the classification branch. type  $\\\\coloneqq$  'CrossEntropyLoss', # Type of loss for classification branch, we. also support FocalLoss etc. use_sigmoid  $\\\\coloneqq$  False, # Whether to use sigmoid. loss_weight  $= 1.0$  ), # Loss weight of the classification branch. loss_bbox  $\\\\coloneqq$  dict( # Config of loss function for the regression branch. type  $\\\\coloneqq$  'LlLoss', # Type of loss, we also support many IoU losses and. smooth L1- loss, etc. loss_weight  $= 1.0$  ), # Loss weight of the regression branch. mask ROI extractor  $\\\\coloneqq$  dict( # RoI feature extractor for mask generation. type  $\\\\coloneqq$  'SingleRoIExtractor', # Type of the RoI feature extractor, most of. methods uses SingleRoIExtractor. roi_layer  $\\\\coloneqq$  dict( # Config of RoI Layer that extracts features for instance. segmentation type  $\\\\coloneqq$  'RoIAlign', # Type of RoI Layer, DeformRoIPoolingPack and. ModulatedDeformRoIPoolingPack are also supported. output_size  $= 14$  , # The output size of feature maps. sampling_ratio  $= 0$  ), # Sampling ratio when extracting the RoI features. out_channels  $= 256$  , # Output channels of the extracted feature. featmap_strides  $=$  [4, 8, 16, 32]), # Strides of multi- scale feature maps. mask_head  $\\\\coloneqq$  dict( # Mask prediction head. type  $\\\\coloneqq$  'FCNMaskHead', # Type of mask head, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/models/roi heads/mask heads/fcn_mask_head.py#L21. for implementation details. num_conv  $s = 4$  , # Number of convolutional layers in mask head. in_channels  $= 256$  , # Input channels, should be consistent with the output. channels of mask roi extractor. conv_out_channels  $= 256$  , # Output channels of the convolutional layer. num_classes  $= 80$  , # Number of class to be segmented. loss_mask  $\\\\coloneqq$  dict( # Config of loss function for the mask branch. type  $\\\\coloneqq$  'CrossEntropyLoss', # Type of loss used for segmentation use_mask  $\\\\coloneqq$  True, # Whether to only train the mask in the correct class. loss_weight  $= 1.0$  )) # Loss weight of mask branch. train_cfg  $=$  dict( # Config of training hyperparameters for rpn and rconn. rpn  $\\\\coloneqq$  dict( # Training config of rpn. assigner  $\\\\coloneqq$  dict( # Config of assigner type  $\\\\coloneqq$  'MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for. many common detectors. Refer to https://github.com/open- mmlab/mmdetection/blob/master/ mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details. pos_iou_thr  $= 0.7$  , # IoU  $> =$  threshold 0.7 will be taken as positive. samples neg_iou_thr  $= 0.3$  , # IoU  $<$  threshold 0.3 will be taken as negative samples\",\n",
       "  'page_idx': 52},\n",
       " {'type': 'text',\n",
       "  'text': \"min_pos_iou  $= 0.3$  , # The minimal IoU threshold to take boxes as positive. samples match_low_quality  $=$  True, # Wether to match the boxes under low quality.  $\\\\leftrightarrow$  (see API doc for more details). ignore_iof_thr  $\\\\scriptstyle = - 1$  ), # IoF threshold for ignoring bboxes sampler=dict( # Config of positive/negative sampler type  $\\\\coloneqq$  'RandomSampler', # Type of sampler, PseudoSampler and other.  $\\\\leftrightarrow$  samplers are also supported. Refer to https://github.com/open- mmlab/mmdetection/blob/  $\\\\leftrightarrow$  master/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details. num  $= 256$  , # Number of samples. pos_fraction  $= 0.5$  , # The ratio of positive samples in the total samples. neg_pos_ub  $\\\\scriptstyle = - 1$  , # The upper bound of negative samples based on the.  $\\\\leftrightarrow$  number of positive samples. add_gt_as_proposals  $=$  False), # Whether add GT as proposals after.  $\\\\leftrightarrow$  sampling. allowed_border  $\\\\scriptstyle = - 1$  , # The border allowed after padding for valid anchors. pos_weight  $\\\\scriptstyle = - 1$  , # The weight of positive samples during training. debug  $\\\\coloneqq$  False), # Whether to set the debug mode. rpn_proposal  $=$  dict( # The config to generate proposals during training nms_across_levels  $=$  False, # Whether to do NMS for boxes across levels. Only.  $\\\\leftrightarrow$  work in GARPNHead', naive rpn does not support do nms cross levels. nms_prc  $= 2000$  , # The number of boxes before NMS nms_post  $= 1000$  , # The number of boxes to be kept by NMS, Only work in.  $\\\\leftrightarrow$  GARPNHead'. max_per_img  $= 1000$  , # The number of boxes to be kept after NMS. nms- dict( # Config of NMS type  $\\\\coloneqq$  'nms', # Type of NMS iou_threshold  $= 0.7$  # NMS threshold ), min_bbox_size  $= 0$  ), # The allowed minimal box size rcnn=dict( # The config for the roi heads. assigner  $=$  dict( # Config of assigner for second stage, this is different for.  $\\\\leftrightarrow$  that in rpn type  $\\\\coloneqq$  'MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for.  $\\\\leftrightarrow$  all roi heads for now. Refer to https://github.com/open- mmlab/mmdetection/blob/master/  $\\\\leftrightarrow$  mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details. pos_iou_thr  $= 0.5$  , # IoU  $> =$  threshold 0.5 will be taken as positive.  $\\\\leftrightarrow$  samples neg_iou_thr  $= 0.5$  , # IoU  $<$  threshold 0.5 will be taken as negative samples min_pos_iou  $= 0.5$  , # The minimal IoU threshold to take boxes as positive.  $\\\\leftrightarrow$  samples match_low_quality  $=$  False, # Whether to match the boxes under low quality.  $\\\\leftrightarrow$  (see API doc for more details). ignore_iof_thr  $\\\\scriptstyle = - 1$  ), # IoF threshold for ignoring bboxes sampler=dict( type  $\\\\coloneqq$  'RandomSampler', # Type of sampler, PseudoSampler and other.  $\\\\leftrightarrow$  samplers are also supported. Refer to https://github.com/open- mmlab/mmdetection/blob/  $\\\\leftrightarrow$  master/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details. num  $= 512$  , # Number of samples pos_fraction  $= 0.25$  , # The ratio of positive samples in the total samples. neg_pos_ub  $\\\\scriptstyle = - 1$  , # The upper bound of negative samples based on the.  $\\\\leftrightarrow$  number of positive samples.\",\n",
       "  'page_idx': 53},\n",
       " {'type': 'text',\n",
       "  'text': \"add_gt_as_proposals  $=$  True ), # Whether add GT as proposals after sampling. mask_size  $= 28$  , # Size of mask pos_weight  $\\\\coloneqq - 1$  , # The weight of positive samples during training. debug  $\\\\equiv$  False)) # Whether to set the debug mode test_cfg  $=$  dict( # Config for testing hyperparameters for rpn and rcnn. rpn  $\\\\equiv$  dict( # The config to generate proposals during testing nms_lacross_levels  $\\\\equiv$  False, # Whether to do NMS for boxes across levels. Only. work in `GARPNHead`, naive rpn does not support do nms cross levels. nms_pre  $= 1000$  , # The number of boxes before NMS nms_post  $= 1000$  , # The number of boxes to be kept by NMS, Only work in.  $\\\\leftrightarrow$  GARPNHead'. max_per_img  $= 1000$  , # The number of boxes to be kept after NMS. nms=dict( # Config of NMS type  $\\\\equiv$  'nms', #Type of NMS iou_threshold  $= 0.7$  # NMS threshold ), min_bbox_size  $= 0$  ), # The allowed minimal box size rcnn  $\\\\equiv$  dict( # The config for the roi heads. score thr  $= 0.05$  , # Threshold to filter out boxes nms=dict( # Config of NMS in the second stage type  $\\\\equiv$  'nms', # Type of NMS iou thr  $= 0.5$  ), # NMS threshold max_per_img  $= 100$  , # Max number of detections of each image mask_thr_binary  $= 0.5$  ) # Threshold of mask prediction dataset_type  $=$  'CocoDataset' # Dataset type, this will be used to define the dataset data_root  $=$  'data/coco/' # Root path of data img_norm_cfg  $=$  dict( # Image normalization config to normalize the input images mean  $=$  [123.675, 116.28, 103.53], # Mean values used to pre- training the pre- trained.  $\\\\leftrightarrow$  backbone models std  $=$  [58.395, 57.12, 57.375], # Standard variance used to pre- training the pretrained backbone models to_rg  $\\\\beta =$  True ) # The channel orders of image used to pre- training the pre- trained backbone models train_pipeline  $=$  [ # Training pipeline dict(type  $=$  'LoadImageFromFile'), # First pipeline to load images from file path dict( type  $=$  'LoadAnnotations', # Second pipeline to load annotations for current image with_bbox=True, # Whether to use bounding box, True for detection with_mask  $\\\\equiv$  True, # Whether to use instance mask, True for instance segmentation poly2mask  $\\\\equiv$  False), # Whether to convert the polygon mask to instance mask, set.  $\\\\leftrightarrow$  False for acceleration and to save memory dict( type  $=$  'Resize', # Augmentation pipeline that resize the images and their annotations img_scale  $=$  (1333, 800), # The largest scale of image keep_ratio  $\\\\equiv$  True ), # whether to keep the ratio between height and width. dict( type  $=$  'RandomFlip', # Augmentation pipeline that flip the images and their.  $\\\\leftrightarrow$  annotations flip_ratio  $= 0.5$  ), # The ratio or probability to flip\",\n",
       "  'page_idx': 54},\n",
       " {'type': 'text',\n",
       "  'text': \"dict( type  $\\\\coloneqq$  'Normalize', # Augmentation pipeline that normalize the input images mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], # These keys are the same of img_norm_cfg since_ the std=[58.395, 57.12, 57.375], # keys of img_norm_cfg are used here as arguments to_rgb  $\\\\coloneqq$  True), dict( type  $\\\\coloneqq$  'Pad', # Padding config size_divisor  $= 32$  ), # The number the padded images should be divisible dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), # Default format bundle to gather data in the_ pipeline dict( type  $\\\\coloneqq$  'Collect', # Pipeline that decides which keys in the data should be passed_ to the detector keys  $\\\\coloneqq$  ['img', 'gt_bboxes', 'gt_labels', 'gt_masks']) ] test_pipeline  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), # First pipeline to load images from file path dict( type  $\\\\coloneqq$  'MultiScaleFlipAug', # An encapsulation that encapsulates the testing_ augmentations img_scale  $\\\\coloneqq$  (1333, 800), # Decides the largest scale for testing, used for the_ Resize pipeline flip  $\\\\coloneqq$  False, # Whether to flip images during testing transforms  $=$  [ dict(type  $\\\\coloneqq$  'Resize', # Use resize augmentation keep_ratio  $\\\\coloneqq$  True), # Whether to keep the ratio between height and width, the img_scale set here will be suppressed by the img_scale set above. dict(type  $\\\\coloneqq$  'RandomFlip'), # Thought RandomFlip is added in pipeline, it is_ not used because flip  $\\\\coloneqq$  False dict( type  $\\\\coloneqq$  'Normalize', # Normalization config, the values are from img_norm_ cfg mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True), dict( type  $\\\\coloneqq$  'Pad', # Padding config to pad images divisible by 32. size_divisor  $= 32$  ), dict( type  $\\\\coloneqq$  'ImageToTensor', # convert image to tensor keys  $\\\\coloneqq$  ['img']), dict( type  $\\\\coloneqq$  'Collect', # Collect pipeline that collect necessary keys for_ testing. keys  $\\\\coloneqq$  ['img']) ]) ] data  $=$  dict( samples_per_gpu  $= 2$  , # Batch size of a single GPU workers_per_gpu  $= 2$  , # Worker to pre- fetch data for each single GPU train  $\\\\coloneqq$  dict( # Train dataset config\",\n",
       "  'page_idx': 55},\n",
       " {'type': 'text',\n",
       "  'text': \"type  $\\\\coloneqq$  'CocoDataset', # Type of dataset, refer to https://github.com/open- mmlab/ _mmdetection/blob/master/mmdet/datasets/coco.py#L19 for details. ann_file  $\\\\coloneqq$  'data/coco/annotations/instances_train2017. json', # Path of annotation file img_prefix  $\\\\coloneqq$  'data/coco/train2017/' # Prefix of image path pipeline  $\\\\coloneqq$  # pipeline, this is passed by the train_pipeline created before. dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict( type  $\\\\coloneqq$  'LoadAnnotations', with_bbox  $\\\\coloneqq$  True, with_mask  $\\\\coloneqq$  True, poly2mask  $\\\\coloneqq$  False), dict(type  $\\\\coloneqq$  'Resize', img_scale  $\\\\coloneqq$  (1333, 800), keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip', flip_ratio  $= 0.5$  ), dict( type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  ), dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), dict( type  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img', 'gt_bboxes', 'gt_labels', 'gt_masks']) ], val=dict( # Validation dataset config type  $\\\\coloneqq$  'CocoDataset', ann_file  $\\\\coloneqq$  'data/coco/annotations/instances_val2017. json', img_prefix  $\\\\coloneqq$  'data/coco/val2017/', pipeline  $\\\\coloneqq$  [ # Pipeline is passed by test_pipeline created before dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict( type  $\\\\coloneqq$  'MultiScaleFlipAug', img_scale  $\\\\coloneqq$  (1333, 800), flip=False, transforms=[ dict(type  $\\\\coloneqq$  'Resize', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip'), dict( type  $\\\\coloneqq$  'Normalize', mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  ), disttype  $\\\\coloneqq$  'ImageToTensor', keys  $\\\\coloneqq$  ['img'], disttype  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img']) ] ) test=dict( # Test dataset config, modify the ann_file for test- dev/test submission type  $\\\\coloneqq$  'CocoDataset', ann_file  $\\\\coloneqq$  'data/coco/annotations/instances_val2017. json', img_prefix  $\\\\coloneqq$  'data/coco/val2017/',\",\n",
       "  'page_idx': 56},\n",
       " {'type': 'text',\n",
       "  'text': \"```pythonswitch (continued from previous page)pipeline=[ # Pipeline is passed by test_pipeline created before    dict(type='LoadImageFromFile'),    dict(        type='MultiScaleFlipAug',        img_scale=(1333, 800),        flip=False,        transforms=[            dict(type='Resize', keep_ratio=True),            dict(type='RandomFlip'),            dict(                type='Normalize',                mean=[123.675, 116.28, 103.53],                std=[58.395, 57.12, 57.375],                to_rgb=True),            dict(type='Pad', size_divisor=32),            dict(type='ImageToTensor', keys=['img']),            dict(type='Collect', keys=['img'])        ])    ],    samples_per_gpu=2  # Batch size of a single GPU used in testing)evaluation = dict( # The config to build the evaluation hook, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/core/evaluation/eval_hooks.py#L7 for more_details.interval=1, # Evaluation interval    metric=['bbox', 'segm'])  # Metrics used during evaluationoptimizer = dict( # Config used to build optimizer, support all the optimizers inPyTorch whose arguments are also the same as those in PyTorch    type='SGD',  # Type of optimizers, refer to https://github.com/open- mmlab/mmdetection/blob/master/mmdet/core/optimizer/default_constructor.py#L13 for more_details    lr=0.02,  # Learning rate of optimizers, see detail usages of the parameters in the_documentation of PyTorch    momentum=0.9,  # Momentum        weight_decay=0.0001)  # Weight decay of SGDoptimizer_config = dict( # Config used to build the optimizer hook, refer to https://github.com/open- mmlab/mmcv/blob/master/mmcv/runner/hooks/optimizer.py#L8 forimplementation details.    grad_clip=None)  # Most of the methods do not use gradient cliplr_config = dict( # Learning rate scheduler config used to register LrUpdater hook    policy='step',  # The policy of scheduler, also support CosineAnnealing, Cyclic, etc.    Refer to details of supported LrUpdater from https://github.com/open- mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py#L9. warmup='linear',  # The warmup policy, also support `exp` and `constant`.    warmup_iters=500,  # The number of iterations for warmup    warmup_ratio=    0.001,  # The ratio of the starting learning rate used for warmup    step=[8, 11])  # Steps to decay the learning raterunner = dict(type='EpochBasedRunner',  # Type of runner to use (i.e. IterBasedRunner or_EpochBasedRunner)    max_epochs=12)  # Runner that runs the workflow in total max_epochs. For    IterBasedRunner_use `max_itors`  ```\",\n",
       "  'page_idx': 57},\n",
       " {'type': 'text',\n",
       "  'text': \"checkpoint_config  $=$  dict( # Config to set the checkpoint hook, Refer to https://github. com/open- mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation. interval  $= 1$  # The save interval is 1 log_config  $=$  dict( # config to register logger hook interval  $= 50$  # Interval to print the log hooks=[ # dict(type  $\\\\coloneqq$  'TensorboardLoggerHook') # The Tensorboard logger is also supported dict(type  $\\\\coloneqq$  'TextLoggerHook') ]) # The logger used to record the training process. dist_params  $=$  dict(backend  $\\\\equiv$  'ncc1') # Parameters to setup distributed training, the port. can also be set. log_level  $=$  'INFO' # The level of logging. load_from  $=$  None # load models as a pre- trained model from a given path. This will not. resume training. resume_from  $=$  None # Resume checkpoints from a given path, the training will be resumed.  $\\\\rightharpoonup$  from the epoch when the checkpoint's is saved. workflow  $=$  [('train', 1)] # Workflow for runner. [('train', 1)] means there is only one. workflow and the workflow named 'train' is executed once. The workflow trains the. model by 12 epochs according to the total_epochs. work_dir  $=$  'work_dir' # Directory to save the model checkpoints and logs for the. current experiments.\",\n",
       "  'page_idx': 58},\n",
       " {'type': 'text', 'text': '8.6 FAQ', 'text_level': 1, 'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': '8.6.1 Ignore some fields in the base configs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': 'Sometimes, you may set _delete_=True to ignore some of fields in base configs. You may refer to mmcv for simple illustration.',\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': 'In MMDetection, for example, to change the backbone of Mask R- CNN with the following config.',\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': \"model  $=$  dict( type  $\\\\coloneqq$  'MaskRCNN', pretrained  $=$  'torchvision://resnet50', backbone  $=$  dict( type  $\\\\coloneqq$  'ResNet', depth  $= 50$  num stages  $= 4$  out_indices  $= (0,1,2,3)$  frozen_stages  $= 1$  norm_cfg=dict(type  $= 1\\\\mathrm{BN}$  , requires_grad  $=$  True), norm_eval  $=$  True, style  $=$  'pytorch'), neck  $=$  dict(...), rpn_head  $=$  dict(...), roi_head  $=$  dict(...))\",\n",
       "  'page_idx': 58},\n",
       " {'type': 'text',\n",
       "  'text': 'ResNet and HRNet use different keywords to construct.',\n",
       "  'page_idx': 58},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/0afdf850715327fe0701f4af9550bd9485929957f4b7dbf81f1741b5d53cfb84.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>_base_ = &#x27;../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py&#x27;</td></tr><tr><td>model = dict(\\n    pretrained=&#x27;open-mlab://msra/hrnetv2_w32&#x27;,\\n    backbone=dict(\\n        _delete=True,\\n        type=&#x27;HRNet&#x27;,\\n        extra=dict(\\n            stage1=dict(\\n                num_modules=1,\\n                numbranches=1,\\n                block=&#x27;BOTTLENECK&#x27;,\\n                num_blocks=(4, ),\\n                num_channels=(64, )),\\n                stage2=dict(\\n                    num_modules=1,\\n                    numbranches=2,\\n                    block=&#x27;BASIC&#x27;,\\n                    num_blocks=(4, 4),\\n                    num_channels=(32, 64)),\\n                stage3=dict(\\n                    num_modules=4,\\n                    numbranches=3,\\n                    block=&#x27;BASIC&#x27;,\\n                    num_blocks=(4, 4, 4),\\n                    num_channels=(32, 64, 128)),\\n                stage4=dict(\\n                    num_modules=3,\\n                    numbranches=4,\\n                    block=&#x27;BASIC&#x27;,\\n                    num_blocks=(4, 4, 4, 4),\\n                    num_channels=(32, 64, 128, 256))&#x27;)</td></tr><tr><td>neck=dict(...)</td></tr></table>',\n",
       "  'page_idx': 59,\n",
       "  'outline': [67, 73, 543, 465]},\n",
       " {'type': 'text',\n",
       "  'text': 'The _delete_=True would replace all old keys in backbone field with new keys.',\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': '8.6.2 Use intermediate variables in configs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': \"Some intermediate variables are used in the configs files, like train_pipeline/test_pipeline in datasets. It's worth noting that when modifying intermediate variables in the children configs, user need to pass the intermediate variables into corresponding fields again. For example, we would like to use multi scale strategy to train a Mask R- CNN. train_pipeline/test_pipeline are intermediate variable we would like modify.\",\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': \"base_  $=$  '../mask_rcnn_r50_fpn_1x_coco.py' img_norm_cfg  $=$  dict( mean  $\\\\equiv$  [123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\equiv$  True) train_pipeline  $=$  [ dict(type  $=$  'LoadImageFromFile'), dict(type  $=$  'LoadAnnotations', with_bbox  $\\\\equiv$  True, with_mask  $\\\\equiv$  True), dict( type  $=$  'Resize', img_scale  $=$  [(1333, 640), (1333, 672), (1333, 704), (1333, 736), (1333, 768), (1333, 800)]\",\n",
       "  'page_idx': 59},\n",
       " {'type': 'text',\n",
       "  'text': 'multiscale_mode  $\\\\coloneqq$  \"value\", keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  \\'RandomFlip\\', flip_ratio  $= 0$  .5), dict(type  $\\\\coloneqq$  \\'Normalize\\', \\\\*\\\\*img_norm_cfg), dict(type  $\\\\coloneqq$  \\'Pad\\', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  \\'DefaultFormatBundle\\'), dict(type  $\\\\coloneqq$  \\'Collect\\', keys  $\\\\coloneqq$  [\\'img\\', \\'gt_bboxes\\', \\'gt_labels\\', \\'gt_masks\\']), ] test_pipeline  $=$  [ dict(type  $\\\\coloneqq$  \\'LoadImageFromFile\\'), dict( type  $\\\\coloneqq$  \\'MultiScaleFlipAug\\', img_scale  $=$  (1333, 800), flip  $\\\\coloneqq$  False, transforms  $\\\\coloneqq$  [ dict(type  $\\\\coloneqq$  \\'Resize\\', keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  \\'RandomFlip\\'), dict(type  $\\\\coloneqq$  \\'Normalize\\', \\\\*\\\\*img_norm_cfg), dict(type  $\\\\coloneqq$  \\'Pad\\', size_divisor  $= 32$  1 dict(type  $\\\\coloneqq$  \\'ImageToTensor\\', keys  $\\\\coloneqq$  [\\'img\\']), dict(type  $\\\\coloneqq$  \\'Collect\\', keys  $\\\\coloneqq$  [\\'img\\']), ] ] data  $=$  dict( train  $\\\\coloneqq$  dict(pipeline  $\\\\coloneqq$  train_pipeline), val  $\\\\coloneqq$  dict(pipeline  $\\\\coloneqq$  test_pipeline), test  $\\\\coloneqq$  dict(pipeline  $\\\\coloneqq$  test_pipeline))',\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': 'We first define the new train_pipeline/test_pipeline and pass them into data.',\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': 'Similarly, if we would like to switch from SyncBN to BN or MMSyncBN, we need to substitute every norm_cfg in the config.',\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': \"base_ = './mask_rcnn_r50_fpn_1x_coco.py'  norm_cfg = dict(type='BN', requires_grad=True)  model = dict(    backbone=dict(norm_cfg=norm_cfg),    neck=dict(norm_cfg=norm_cfg),    ...)\",\n",
       "  'page_idx': 60},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 8. Tutorial 1: Learn about Configs',\n",
       "  'page_idx': 61},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 2: CUSTOMIZE DATASETS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': '9.1 Support new data format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': \"To support a new data format, you can either convert them to existing formats (COCO format or PASCAL format) or directly convert them to the middle format. You could also choose to convert them offline (before training by a script) or online (implement a new dataset and do the conversion at training). In MMDetection, we recommend to convert the data into COCO formats and do the conversion offline, thus you only need to modify the config's data annotation paths and classes after the conversion of your data.\",\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': '9.1.1 Reorganize new data formats to existing format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': 'The simplest way is to convert your dataset to existing dataset formats (COCO or PASCAL VOC).',\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': 'The annotation json files in COCO format has the following necessary keys:',\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': \"'images': [ { 'file_name': 'COCO_val2014_000000001268. jpg', 'height': 427, 'width': 640, 'id': 1268 }, ... ], 'annotations': [ { 'segmentation': [[192.81, 247.09, ... 219.03, 249.06]], # if you have mask labels 'area': 1035.749, 'iscrowd': 0, 'image_id': 1268, 'bbox': [192.81, 224.8, 74.73, 33.43], 'category_id': 16, 'id': 42986 }, ...\",\n",
       "  'page_idx': 62},\n",
       " {'type': 'text',\n",
       "  'text': \"], 'categories': [ {id': 0, 'name': 'car'}, ]\",\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'There are three necessary keys in the json file:',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '- images: contains a list of images with their information like file_name, height, width, and id.- annotations: contains the list of instance annotations.- categories: contains the list of categories names and their ID.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'After the data pre- processing, there are two steps for users to train the customized new dataset with existing format (e.g. COCO format):',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '1. Modify the config file for using the customized dataset.2. Check the annotations of the customized dataset.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'Here we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format to train an existing Cascade Mask R- CNN R50- FPN detector.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '1. Modify the config file for using the customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': 'There are two aspects involved in the modification of config file:',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': '1. The data field. Specifically, you need to explicitly add the classes fields in data.train, data.val and data.test.2. The num_classes field in the model part. Explicitly over-write all the num_classes from default value (e.g. 80 in COCO) to your classes number.',\n",
       "  'page_idx': 63},\n",
       " {'type': 'text', 'text': 'In configs/my_custom_config.py:', 'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': \"the new config inherits the base configs to highlight the necessary modification _base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py' # 1. dataset settings dataset_type  $=$  'CocoDataset' classes  $=$  ('a','b','c','d','e') data  $=$  dict( samples_per_gpu  $= 2$  workers_per_gpu  $= 2$  train  $=$  dict( type  $=$  dataset_type, # explicitly add your class names to the field classes classes  $=$  classes, ann_file  $=$  'path/to/your/train/annotation_data', img_prefix  $=$  'path/to/your/train/image_data'), val  $=$  dict( type  $=$  dataset_type, # explicitly add your class names to the field classes classes  $=$  classes,\",\n",
       "  'page_idx': 63},\n",
       " {'type': 'text',\n",
       "  'text': \"(continued from previous page)ann_file='path/to/your/val/annotation_data',img_prefix='path/to/your/val/image_data'),test=dict(type=dataset_type, # explicitly add your class names to the field `classes` classes=classes, ann_file='path/to/your/test/annotation_data', img_prefix='path/to/your/test/image_data'))# 2. model settings# explicitly over- write all the `num_classes` field from default 80 to 5. model = dict(roi_head=dict(    bbox_head=[        dict(            type='Shared2FCBBoxHead',            # explicitly over- write all the `num_classes` field from default 80 to 5.            num_classes=5),            dict(            type='Shared2FCBBoxHead',            # explicitly over- write all the `num_classes` field from default 80 to 5.            num_classes=5),            dict(            type='Shared2FCBBoxHead',            # explicitly over- write all the `num_classes` field from default 80 to 5.            num_classes=5)],# explicitly over- write all the `num_classes` field from default 80 to 5. mask_head=dict(num_classes=5)))\",\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': '2. Check the annotations of the customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': 'Assuming your customized dataset is COCO format, make sure you have the correct annotations in the customized dataset:',\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': '1. The length for categories field in annotations should exactly equal the tuple length of classes fields in your config, meaning the number of classes (e.g. 5 in this example).2. The classes fields in your config file should have exactly the same elements and the same order with the name in categories of annotations. MMDetection automatically maps the uncontinuous id in categories to the continuous label indices, so the string order of name in categories field affects the order of label indices. Meanwhile, the string order of classes in config affects the label text during visualization of predicted bounding boxes.3. The category_id in annotations field should be valid, i.e., all values in category_id should belong to id in categories.',\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': 'Here is a valid example of annotations:',\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': \"'annotations': [{    'segmentation': [[192.81,\",\n",
       "  'page_idx': 64},\n",
       " {'type': 'text',\n",
       "  'text': \"247.09, 219.03, 249.06], # if you have mask labels 'area': 1035.749, 'iscrowd': 0, 'image_id': 1268, 'bbox': [192.81, 224.8, 74.73, 33.43], 'category_id': 16, 'id': 42986 }, 2, 1, # MMDetection automatically maps the uncontinuous `id` to the continuous label indices. 'categories': ['id': 1, 'name': 'a'}, {'id': 3, 'name': 'b'}, {'id': 4, 'name': 'c'}, {'id': 16, 'name': 'd'}, {'id': 17, 'name': 'e'}, ]\",\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': 'We use this way to support CityScapes dataset. The script is in cityscapes.py and we also provide the finetuning configs.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text', 'text': 'Note', 'text_level': 1, 'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': '1. For instance segmentation datasets, MMDetection only supports evaluating mask AP of dataset in COCO format for now.  \\n2. It is recommended to convert the data offline before training, thus you can still use CocoDataset and only need to modify the path of annotations and the training classes.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': '9.1.2 Reorganize new data format to middle format',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': 'It is also fine if you do not want to convert the annotation format to COCO or PASCAL format. Actually, we define a simple annotation format and all existing datasets are processed to be compatible with it, either online or offline.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': 'The annotation of a dataset is a list of dict, each dict corresponds to an image. There are 3 field filename (relative path), width, height for testing, and an additional field ann for training. ann is also a dict containing at least 2 fields: bboxes and labels, both of which are numpy arrays. Some datasets may provide annotations like crowd/difficult/ignored bboxes, we use bboxes_ignore and labels_ignore to cover them.',\n",
       "  'page_idx': 65},\n",
       " {'type': 'text', 'text': 'Here is an example.', 'page_idx': 65},\n",
       " {'type': 'text',\n",
       "  'text': \"[ { 'filename': 'a.jpg', 'width': 1280, 'height': 720, 'ann': [ 'bboxes': <np.ndarray, float32> (n, 4), 'labels': <np.ndarray, int64> (n, ), 'bboxes_ignore': <np.ndarray, float32> (k, 4), 'labels_ignore': <np.ndarray, int64> (k, ) (optional field) }\",\n",
       "  'page_idx': 65},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/20a012b0d697295b57eefc0c33354c563af7ba33d3735ad7cf80752c0da351cb.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>3,</td></tr><tr><td>...</td></tr><tr><td>]</td></tr></table>',\n",
       "  'page_idx': 66,\n",
       "  'outline': [67, 83, 543, 126]},\n",
       " {'type': 'text',\n",
       "  'text': 'There are two ways to work with custom datasets.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'text', 'text': 'online conversion', 'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': 'You can write a new Dataset class inherited from CustomDataset, and overwrite two methods load_annotations(self, ann_file) and get_ann_info(self, idx), like CocoDataset and VOCDataset.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'text', 'text': 'offline conversion', 'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': 'You can convert the annotation format to the expected format above and save it to a pickle or json file, like pascal_voc.py. Then you can simply use CustomDataset.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': '9.1.3 An example of customized dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume the annotation is in a new format in text files. The bounding boxes annotations are stored in text file annotation.txt as the following',\n",
       "  'page_idx': 66},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c8e6dbace0eeb3b576a6cc13798520020ca329c9f262f85c69a257d10a5f1c0d.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>#</td></tr><tr><td>0000001.jpg</td></tr><tr><td>1280 720</td></tr><tr><td>2</td></tr><tr><td>10 20 40 60 1</td></tr><tr><td>20 40 50 60 2</td></tr><tr><td>#</td></tr><tr><td>0000002.jpg</td></tr><tr><td>1280 720</td></tr><tr><td>3</td></tr><tr><td>50 20 40 60 2</td></tr><tr><td>20 40 30 45 2</td></tr><tr><td>30 40 50 60 3</td></tr></table>',\n",
       "  'page_idx': 66,\n",
       "  'outline': [69, 331, 543, 493]},\n",
       " {'type': 'text',\n",
       "  'text': 'We can create a new dataset in mmdet/datasets/my_dataset.py to load the data.',\n",
       "  'page_idx': 66},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/5c3debd4d34e51fe0dddd8ca00bcc2030d5c9762986b1c36db6c506b661613e9.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': \"<table><tr><td>import mmcv\\nimport numpy as np</td></tr></table>\\n\\nfrom .builder import DATASETS\\nfrom .custom import CustomDataset\\n\\n@DATASETS.register_module()\\nclass MyDataset(CustomDataset):\\n    CLASSES = ('person', 'bicycle', 'car', 'motorcycle')\\n    def load_annotations(self, ann_file):\\n        ann_list = mmcv.list_from_file(ann_file)<nl>\",\n",
       "  'page_idx': 66,\n",
       "  'outline': [69, 521, 544, 705]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 66},\n",
       " {'type': 'text',\n",
       "  'text': \"data_infos = [] for i, ann_line in enumerate(ann_list): if ann_line  $! =$  '#': continue img_shape  $=$  ann_list[i  $^+$  2].split(' ') width  $=$  int(img_shape[0]) height  $=$  int(img_shape[1]) bbox_number  $=$  int(ann_list[i + 3]) anns  $=$  ann_line.split(' ') bboxes  $=$  [] labels  $=$  [] for anns in ann_list[i  $^+$  4:i  $^+$  4+ bbox_number]: bboxes.append([float(ann) for ann in anns[:4]]) labels.append(int(anns[4])) data_infos.append( dict( filename  $=$  ann_list[i + 1], width  $\\\\equiv$  width, height  $=$  height, ann  $\\\\equiv$  dict( bboxes  $=$  np.array(bboxes).astype(np.float32), labels  $=$  np.array(labels).astype(np.int64)) ) return data_infos def get_ann_info(self, idx): return self.data_infos[idx]['ann']\",\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': 'Then in the config, to use MyDataset you can modify the config as the following',\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset_A_train = dict( type='MyDataset', ann_file = 'image_list.txt', pipeline=train_pipeline)\",\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': '9.2 Customize datasets by dataset wrappers',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection also supports many dataset wrappers to mix the dataset or modify the dataset distribution for training. Currently it supports to three dataset wrappers as below:',\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': '- RepeatDataset: simply repeat the whole dataset.- ClassBalancedDataset: repeat dataset in a class balanced manner.- ConcatDataset: concat datasets.',\n",
       "  'page_idx': 67},\n",
       " {'type': 'text',\n",
       "  'text': '9.2.1 Repeat dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"We use RepeatDataset as wrapper to repeat the dataset. For example, suppose the original dataset is Dataset_A, to repeat it, the config looks like the following dataset_A_train  $=$  dict( type  $=$  'RepeatDataset', times  $= \\\\mathbb{N}$  dataset=dict( # This is the original config of Dataset_A type  $=$  'Dataset_A', pipeline  $=$  train_pipeline ) )\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text', 'text': '', 'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '9.2.2 Class balanced dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"We use ClassBalancedDataset as wrapper to repeat the dataset based on category frequency. The dataset to repeat needs to instantiate function self. get_cat_ids(idx) to support ClassBalancedDataset. For example, to repeat Dataset_A with oversample_thr  $= 1e - 3$  , the config looks like the following dataset_A_train  $=$  dict( type  $=$  'ClassBalancedDataset', oversample_thr  $= 1e - 3$  dataset=dict( # This is the original config of Dataset_A type  $=$  'Dataset_A', pipeline  $=$  train_pipeline ) )\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text', 'text': '', 'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'You may refer to source code for details.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '9.2.3 Concatenate dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'There are three ways to concatenate the dataset.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '1. If the datasets you want to concatenate are in the same type with different annotation files, you can concatenate the dataset config like the following.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset_A_train  $=$  dict( type  $=$  'Dataset_A', annn_file  $=$  ['anno_file_1', 'anno_file_2'], pipeline  $=$  train_pipeline )\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'If the concatenated dataset is used for test or evaluation, this manner supports to evaluate each dataset separately. To test the concatenated datasets as a whole, you can set separate_eval=False as below.',\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset_A_train  $=$  dict( type  $=$  'Dataset_A', annn_file  $=$  ['anno_file_1', 'anno_file_2'],\",\n",
       "  'page_idx': 68},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': '9.2. Customize datasets by dataset wrappers',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 68},\n",
       " {'type': 'text',\n",
       "  'text': 'separate_eval=False, pipeline=train_pipeline )',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': '2. In case the dataset you want to concatenate is different, you can concatenate the dataset configs like the following.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c1ab06f38869679405cab029e0c97b0fc5fb0ee74dcf3d39979523c7f5c655af.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>dataset_A_train = dict()\\ndataset_B_train = dict()\\ndata = dict(\\n  imgs_per_gpu=2,\\n  workers_per_gpu=2,\\n  train = [\\n    dataset_A_train,\\n    dataset_B_train\\n  ],\\n  val = dataset_A_val,\\n  test = dataset_A_test\\n)</td></tr></table>',\n",
       "  'page_idx': 69,\n",
       "  'outline': [93, 153, 543, 315]},\n",
       " {'type': 'text',\n",
       "  'text': 'If the concatenated dataset is used for test or evaluation, this manner also supports to evaluate each dataset separately.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': '3. We also support to define ConcatDataset explicitly as the following.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/bd4098ccd76c47a3caa96f65e58499483d3c0cc2a55a4119dbb4624d55c5f62b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>dataset_A_val = dict()\\ndataset_B_val = dict()\\ndata = dict(\\n  imgs_per_gpu=2,\\n  workers_per_gpu=2,\\n  train=dataset_A_train,\\n  val=dict(\\n    type=&#x27;ConcatDataset&#x27;,\\n    datasets=[dataset_A_val, dataset_B_val],\\n    separate_eval=False))</td></tr></table>',\n",
       "  'page_idx': 69,\n",
       "  'outline': [93, 373, 543, 510]},\n",
       " {'type': 'text',\n",
       "  'text': 'This manner allows users to evaluate all the datasets as a single one by setting separate_eval=False.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': '1. The option separate_eval=False assumes the datasets use self.data_infos during evaluation. Therefore, COCO datasets do not support this behavior since COCO datasets do not fully rely on self.data_infos for evaluation. Combining different types of datasets and evaluating them as a whole is not tested thus is not suggested. \\n2. Evaluating ClassBalancedDataset and RepeatDataset is not supported thus evaluating concatenated datasets of these types is also not supported.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': 'A more complex example that repeats Dataset_A and Dataset_B by N and M times, respectively, and then concatenates the repeated datasets is as the following.',\n",
       "  'page_idx': 69},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/61bc0ee014640ff134f49ebaeda0ffb16bae1904e1f10febfd2e7a59f0a7c173.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>dataset_A_train = dict(\\n  type=&#x27;RepeatDataset&#x27;,\\n  times=N,</td></tr></table>',\n",
       "  'page_idx': 69,\n",
       "  'outline': [69, 670, 543, 709]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 69},\n",
       " {'type': 'text',\n",
       "  'text': \"dataset=dict( type  $=$  'Dataset_A', pipeline  $=$  train_pipeline ) ) dataset_A_val  $=$  dict( pipeline  $=$  test_pipeline ) dataset_A_test  $=$  dict( pipeline  $=$  test_pipeline ) dataset_B_train  $=$  dict( type  $=$  'RepeatDataset', times  $= \\\\mathbb{M}$  dataset  $=$  dict( type  $=$  'Dataset_B', pipeline  $=$  train_pipeline ) ) data  $=$  dict( imgs_per_gpu  $= 2$  , workers_per_gpu  $= 2$  train  $=$  [ dataset_A_train, dataset_B_train ], val  $=$  dataset_A_val, test  $=$  dataset_A_test )\",\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': '9.3 Modify Dataset Classes',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': 'With existing dataset types, we can modify the class names of them to train subset of the annotations. For example, if you want to train only three classes of the current dataset, you can modify the classes of dataset. The dataset will filter out the ground truth boxes of other classes automatically.',\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': \"classes  $=$  ('person', 'bicycle', 'car') data  $=$  dict( train  $=$  dict(classes  $=$  classes), val  $=$  dict(classes  $=$  classes), test  $=$  dict(classes  $=$  classes))\",\n",
       "  'page_idx': 70},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection V2.0 also supports to read the classes from a file, which is common in real applications. For example, assume the classes.txt contains the name of classes as the following.',\n",
       "  'page_idx': 70},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/d9dc99487a7f725296447ae6d2e54f8fe9857ec99369cc3f139c754dcd03068a.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>person</td></tr><tr><td>bicycle</td></tr><tr><td>car</td></tr></table>',\n",
       "  'page_idx': 71,\n",
       "  'outline': [69, 76, 543, 118]},\n",
       " {'type': 'text',\n",
       "  'text': 'Users can set the classes as a file path, the dataset will load it and convert it to a list automatically.',\n",
       "  'page_idx': 71},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3bf05e44743abd0ee3239213cd3e835d3822260e00f7f006c6e2d1afda3f9e32.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>classes = &#x27;path/to/classes.txt&#x27;</td></tr><tr><td>data = dict(</td></tr><tr><td>train=dict(classes=classes),</td></tr><tr><td>val=dict(classes=classes),</td></tr><tr><td>test=dict(classes=classes))</td></tr></table>',\n",
       "  'page_idx': 71,\n",
       "  'outline': [69, 145, 543, 212]},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 71},\n",
       " {'type': 'text',\n",
       "  'text': 'Note:- Before MMDetection v2.5.0, the dataset will filter out the empty GT images automatically if the classes are set and there is no way to disable that through config. This is an undesirable behavior and introduces confusion because if the classes are not set, the dataset only filter the empty GT images when filter_empty_gt=True and test_mode=False. After MMDetection v2.5.0, we decouple the image filtering process and the classes modification, i.e., the dataset will only filter empty GT images when filter_empty_gt=True and test_mode=False, no matter whether the classes are set. Thus, setting the classes only influences the annotations of classes used for training and users could decide whether to filter empty GT images by themselves.- Since the middle format only has box labels and does not contain the class names, when using CustomDataset, users cannot filter out the empty GT images through configs but only do this offline.- Please remember to modify the num_classes in the head when specifying classes in dataset. We implemented NumClassCheckHook to check whether the numbers are consistent since v2.9.0(after PR#4508).- The features for setting dataset classes and dataset filtering will be refactored to be more user- friendly in the future (depends on the progress).',\n",
       "  'page_idx': 71},\n",
       " {'type': 'text',\n",
       "  'text': '9.4 COCO Panoptic Dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 71},\n",
       " {'type': 'text',\n",
       "  'text': 'Now we support COCO Panoptic Dataset, the format of panoptic annotations is different from COCO format. Both the foreground and the background will exist in the annotation file. The annotation json files in COCO Panoptic format has the following necessary keys:',\n",
       "  'page_idx': 71},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/e79b5d426f3ad37c701fc9542afd8493d99c956516fb2969eeb4fd997a1254db.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>&#x27;images&#x27;: [\\n    {\\n        &#x27;file_name&#x27;: &#x27;0000000001268.jpg&#x27;,\\n        &#x27;height&#x27;: 427,\\n        &#x27;width&#x27;: 640,\\n        &#x27;id&#x27;: 1268\\n    },\\n    ...</td></tr><tr><td>&#x27;]</td></tr><tr><td>&#x27;annotations&#x27;: [\\n    {\\n        &#x27;filename&#x27;: &#x27;0000000001268.jpg&#x27;,\\n        &#x27;image_id&#x27;: 1268,\\n        &#x27;segments_info&#x27;: [\\n            {\\n                &#x27;image_id&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;: 1268,\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image_height&#x27;: 1268,\\n                &#x27;image_width&#x27;:\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n                &#x27;image&#x27;: 1268,\\n            }\\n        &#x27;image_width&#x27;:\\n        &#x27;image_height&#x27;:\\n        &#x27;image_width&#x27;:\\n        &#x27;image_height&#x27;:\\n        &#x27;image_width&#x27;:\\n        &#x27;image_height&#x27;:\\n        &#x27;image_width&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:\\n        &#x27;image&#x27;:</td></tr></table>',\n",
       "  'page_idx': 71,\n",
       "  'outline': [67, 510, 544, 708]},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': \"'id':8345037, # One- to- one correspondence with the id in the annotation. map. 'category_id': 51, 'iscrowd': 0, 'bbox': (x1, y1, w, h), # The bbox of the background is the outer.  $\\\\rightarrow$  rectangle of its mask. 'area': 24315 }, 1 } 1 'categories': [ # including both foreground categories and background categories {'id': 0, 'name': 'person'}, 1\",\n",
       "  'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': 'Moreover, the seg_prefix must be set to the path of the panoptic annotation images.',\n",
       "  'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': \"data  $=$  dict( type  $\\\\coloneqq$  'CocoPanopticDataset', train  $=$  dict( seg_prefix  $=$  'path/to/your/train/panoptic/image_annotation_data' ), val  $=$  dict( seg_prefix  $=$  'path/to/your/train/panoptic/image_annotation_data' ) )\",\n",
       "  'page_idx': 72},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 9. Tutorial 2: Customize Datasets',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 73},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 3: CUSTOMIZE DATA PIPELINES',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': '10.1 Design of Data pipelines',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': \"Following typical conventions, we use Dataset and DataLoader for data loading with multiple workers. Dataset returns a dict of data items corresponding the arguments of models' forward method. Since the data in object detection may not be the same size (image size, gt bbox size, etc.), we introduce a new DataContainer type in MMCV to help collect and distribute data of different size. See here for more details.\",\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'The data preparation pipeline and the dataset is decomposed. Usually a dataset defines how to process the annotations and a data pipeline defines all the steps to prepare a data dict. A pipeline consists of a sequence of operations. Each operation takes a dict as input and also output a dict for the next transform.',\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'We present a classical pipeline in the following figure. The blue blocks are pipeline operations. With the pipeline going on, each operator can add new keys (marked as green) to the result dict or update the existing keys (marked as orange).',\n",
       "  'page_idx': 74},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cafc4b1c111689ef52c80e4e47a15a2cae0193513315cb0aec2168e66b29353b.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 74,\n",
       "  'outline': [70, 381, 574, 530]},\n",
       " {'type': 'text', 'text': 'figure', 'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'The operations are categorized into data loading, pre- processing, formatting and test- time augmentation.',\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': 'Here is a pipeline example for Faster R- CNN.',\n",
       "  'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': \"img_norm_cfg  $=$  dict(mean  $\\\\coloneqq$  [123.675, 116.28, 103.53], std  $\\\\equiv$  [58.395, 57.12, 57.375], to_rgb  $\\\\equiv$  True) train_pipeline  $=$  [ dict(type  $=$  'LoadImageFromFile'), dict(type  $=$  'LoadAnnotations', with_bbox  $\\\\equiv$  True), dict(type  $=$  'Resize', img_scale  $=$  (1333, 800), keep_ratio  $\\\\equiv$  True), dict(type  $=$  'RandomFlip', flip_ratio  $= 0$  .5), dict(type  $=$  'Normalize', \\\\*\\\\*img_norm_cfg), dict(type  $=$  'Pad', size_divisor  $= 32$  - dict(type  $=$  'DefaultFormatBundle'),\",\n",
       "  'page_idx': 74},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 74},\n",
       " {'type': 'text',\n",
       "  'text': \"dict(type  $=$  'Collect', keys  $=$  ['img', 'gt_bboxes', 'gt_labels']), ] test_pipeline  $=$  [ dict(type  $=$  'LoadImageFromFile'), dict( type  $=$  'MultiScaleFlipAug', img_scale  $=$  (1333, 800), flip  $\\\\equiv$  False, transforms  $=$  [ dict(type  $=$  'Resize', keep_ratio  $=$  True), dict(type  $=$  'RandomFlip'), dict(type  $=$  'Normalize', \\\\*\\\\*img_norm_cfg), dict(type  $=$  'Pad', size_divisor  $= 32$  1 dict(type  $=$  'ImageToTensor', keys  $=$  ['img']), dict(type  $=$  'Collect', keys  $=$  ['img']), ] ]\",\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': 'For each operation, we list the related dict fields that are added/updated/removed.',\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.1 Data loading',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 75},\n",
       " {'type': 'text', 'text': 'LoadImageFromFile', 'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '- add: img, img_shape, ori_shapeLoadAnnotations- add: gt_bboxes, gt_bboxes_ignore, gt_labels, gt_masks, gt_semks, gt_semantic_seg, bbox_fields, mask_fieldsLoadProposals- add: proposals',\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.2 Pre-processing',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 75},\n",
       " {'type': 'text', 'text': 'Resize', 'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '- add: scale, scale_idx, pad_shape, scale_factor, keep_ratio- update: img, img_shape, *bbox_fields, *mask_fields, *seg_fieldsRandomFlip- add: flip- update: img, *bbox_fields, *mask_fields, *seg_fieldsPad- add: pad_fixed_size, pad_size_divisor- update: img, pad_shape, *mask_fields, *seg_fieldsRandomCrop- update: img, pad_shape, gt_bboxes, gt_labels, gt_masks, *bbox_fieldsNormalize',\n",
       "  'page_idx': 75},\n",
       " {'type': 'text',\n",
       "  'text': '- add: img_norm_cfg- update: imgSegRescale- update: gt_semantic_segPhotoMetricDistortion- update: imgExpand- update: img, gt_bboxesMinIoURandomCrop- update: img, gt_bboxes, gt_labelsCorrupt- update: img',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.3 Formatting',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 76},\n",
       " {'type': 'text', 'text': 'ToTensor', 'text_level': 1, 'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '- update: specified by keys.ImageToTensor- update: specified by keys.',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text', 'text': 'Transpose', 'text_level': 1, 'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '- update: specified by keys.ToDataContainer- update: specified by fields.',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': 'DefaultFormatBundle',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '- update: img, proposals, gt_bboxes, gt_bboxes_ignore, gt_labels, gt_masks, gt_semantic_segCollect- add: img_meta (the keys of img_meta is specified by meta_keys)- remove: all other keys except for those specified by keys',\n",
       "  'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '10.1.4 Test time augmentation',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 76},\n",
       " {'type': 'text', 'text': 'MultiScaleFlipAug', 'page_idx': 76},\n",
       " {'type': 'text',\n",
       "  'text': '10.2 Extend and use custom pipelines',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': '1. Write a new pipeline in a file, e.g., in my_pipeline.py. It takes a dict as input and returns a dict.',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': 'import random from mmdet.datasets import PIPELINES @PIPELINES register_module() class MyTransform: \"Add your transform Args: p (float): Probability of shifts.Default 0.5. def__init__(self,  $\\\\scriptstyle{\\\\vec{p} = \\\\mathbb{0},5}$  : self.  $\\\\textbf{p} = \\\\textbf{p}$  def__call__(self, results): if random.random()  $\\\\gimel$  self.p: results[\\'dummy\\']  $=$  True return results',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': '2. Import and use the pipeline in your config file. Make sure the import is relative to where your train script is located.',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': \"custom_imports  $=$  dict imports  $\\\\coloneqq$  ['path.to_my_pipeline'], allow_failed_imports  $\\\\coloneqq$  False) img_norm_cfg  $=$  dict( mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb  $\\\\coloneqq$  True) train_pipeline  $=$  [ dict(type  $\\\\coloneqq$  'LoadImageFromFile'), dict(type  $\\\\coloneqq$  'LoadAnnotations', with_bbox  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'Resize', img_scale=(1333, 800), keep_ratio  $\\\\coloneqq$  True), dict(type  $\\\\coloneqq$  'RandomFlip', flip_ratio  $= 0.5$  - dict(type  $\\\\coloneqq$  'Normalize', \\\\*\\\\*img_norm_cfg), dict(type  $\\\\coloneqq$  'Pad', size_divisor  $= 32$  - dict(type  $\\\\coloneqq$  'MyTransform',  $\\\\scriptstyle{\\\\vec{p} = \\\\mathbb{0},2}$  - dict(type  $\\\\coloneqq$  'DefaultFormatBundle'), dict(type  $\\\\coloneqq$  'Collect', keys  $\\\\coloneqq$  ['img', 'gt_bboxes', 'gt_labels']), ]\",\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': '3. Visualize the output of your augmentation pipeline',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': 'To visualize the output of your augmentation pipeline, tools/misc/browse_dataset.py can help the user to browse a detection dataset (both images and bounding box annotations) visually, or save the image to a designated directory. More details can refer to useful_tools',\n",
       "  'page_idx': 77},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 4: CUSTOMIZE MODELS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'We basically categorize model components into 5 types.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'backbone: usually an FCN network to extract feature maps, e.g., ResNet, MobileNet. neck: the component between backbones and heads, e.g., FPN, PAPPN. head: the component for specific tasks, e.g., bbox prediction and mask prediction. roi extractor: the part for extracting RoI features from feature maps, e.g., RoI Align. loss: the component in head for calculating losses, e.g., FocalLoss, L1Loss, and GHMLoss.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '11.1 Develop new components',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.1 Add a new backbone',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'Here we show how to develop new components with an example of MobileNet.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define a new backbone (e.g. MobileNet)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'Create a new file mmdet/models/backbones/mobilenet. py.',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch.nn as nn from .builder import BACKBONES @BACKBONES.register_module() class MobileNet(nn.Module): def __init__(self, angl, arg2): pass def forward(self, x): # should return a tuple pass',\n",
       "  'page_idx': 78},\n",
       " {'type': 'text',\n",
       "  'text': '2. Import the module',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': 'You can either add the following line to mmdet/models/backbones/__init__.py',\n",
       "  'page_idx': 79},\n",
       " {'type': 'text', 'text': 'from .mobilenet import MobileNet', 'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': 'or alternatively add to the config file to avoid modifying the original code.',\n",
       "  'page_idx': 79},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/735bf766349aaa73c8e301984f80effdbc7e2b9657d3d5923408bcb422faf937.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_imports = dict(</td></tr><tr><td>imports=[&#x27;mmdet.models.backends.mobilenet&#x27;],\\nallow_failed_imports=False)</td></tr></table>',\n",
       "  'page_idx': 79,\n",
       "  'outline': [69, 162, 543, 204]},\n",
       " {'type': 'text', 'text': '', 'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': '3. Use the backbone in your config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/08968e25327e282c42608377e3eeb2c9e821af7339e9c484602d38078b933cb2.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>model = dict(</td></tr><tr><td>...</td></tr><tr><td>backbone=dict(</td></tr><tr><td>type=&#x27;MobileNet&#x27;,</td></tr><tr><td>arg1=xxx,</td></tr><tr><td>arg2=xxx),</td></tr><tr><td>...</td></tr></table>',\n",
       "  'page_idx': 79,\n",
       "  'outline': [69, 268, 543, 360]},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.2 Add new necks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define a neck (e.g. PAFPN)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 79},\n",
       " {'type': 'text',\n",
       "  'text': 'Create a new file mmdet/models/necks/pafpn.py.',\n",
       "  'page_idx': 79},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/63037358f3a66af07db17701f325523f5c4d134496c4a82cc0bc5829b4c48f00.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from .builder import NECKS</td></tr><tr><td>@NECKS.register_module()</td></tr><tr><td>class PAFPN(nn.Module):</td></tr><tr><td>def __init__(self, \\n        in_channels, \\n        out_channels, \\n        num_outs, \\n        start_level=0, \\n        end_level=-1, \\n        add_extra_conv=False):</td></tr><tr><td>pass</td></tr><tr><td>def forward(self, inputs):</td></tr><tr><td># implementation is ignored</td></tr><tr><td>pass</td></tr></table>',\n",
       "  'page_idx': 79,\n",
       "  'outline': [67, 452, 544, 662]},\n",
       " {'type': 'text',\n",
       "  'text': '2. Import the module',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'You can either add the following line to mmdet/models/necks/__init__.py,',\n",
       "  'page_idx': 80},\n",
       " {'type': 'text', 'text': 'from .pafpn import PAFPN', 'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'or alternatively add to the config file and avoid modifying the original code.',\n",
       "  'page_idx': 80},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/bc46fd6327d1b2f260225f5f109fc837fdcf2dba682883faec6e50cd25147bdc.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_imports = dict(</td></tr><tr><td>imports=[&#x27;mmdet.models.necks.pafpn.py&#x27;],\\nallow_failed_imports=False)</td></tr></table>',\n",
       "  'page_idx': 80,\n",
       "  'outline': [69, 162, 543, 202]},\n",
       " {'type': 'text', 'text': '', 'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': '3. Modify the config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': \"neck=dict( type  $\\\\coloneqq$  'PAFPN', in_channels  $=$  [256, 512, 1024, 2048], out_channels  $= 256$  num_outs  $= 5$\",\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.3 Add new heads',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'Here we show how to develop a new head with the example of Double Head R- CNN as the following.',\n",
       "  'page_idx': 80},\n",
       " {'type': 'text',\n",
       "  'text': 'First, add a new bbox head in mmdet/models/roi_heads/bbox_heads/double_bbox_head.py. Double Head RCNN implements a new bbox head for object detection. To implement a bbox head, basically we need to implement three functions of the new module as the following.',\n",
       "  'page_idx': 80},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/d646ec2ee72e1bbe5701a4a8abad0302ea0d728bf252e437216ac6a6b479e6e6.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from mmdet.models.builder import HEADS\\nfrom .bbox_head import BBoxHead</td></tr></table>\\n\\n@HEADS.register_module()\\nclass DoubleConvFCBBoxHead(BBoxHead):\\n    r\"\"\"BBox head used in Double-Head R-CNN\\n\\n    /-&gt; cls\\n    /-&gt; shared convs -&gt; \\n    /-&gt; reg\\n    roi features\\n    /-&gt; cls\\n    /-&gt; shared fc   -&gt; \\n    /-&gt; reg\\n    \"\"\" # noqa: W605\\n\\ndef __init__(self,\\n    num_conv=0,\\n    num_fcs=0,\\n    conv_out_channels=1024,\\n    fc_out_channels=1024,\\n    conv_cfg=None,<nl>',\n",
       "  'page_idx': 80,\n",
       "  'outline': [67, 444, 543, 712]},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 80},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/49e8f405080173e77228aa824c94842e3125ff2a10de6ac8df5381355461ead0.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>norm_cfg=dict(type=&#x27;BN&#x27;),\\n    **kwargs):\\nkwargs.setdefault(&#x27;with_avg_pool&#x27;, True)\\nsuper(DoubleConvFCBBoxHead, self).__init__(**kwargs)</td></tr><tr><td>def forward(self, x_cls, x_reg):</td></tr></table>',\n",
       "  'page_idx': 81,\n",
       "  'outline': [67, 78, 543, 175]},\n",
       " {'type': 'text',\n",
       "  'text': 'Second, implement a new RoI Head if it is necessary. We plan to inherit the new DoubleHeadRoIHead from StandardRoIHead. We can find that a StandardRoIHead already implements the following functions.',\n",
       "  'page_idx': 81},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/b5775a03a57e203b60b2cf278c77eaf33d25fe3970550f90925037cf1e41b6f3.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>import torch</td></tr><tr><td>from mmdet.core import bbox2result, bbox2roi, build_assigner, build sampler</td></tr><tr><td>from .builder import HEADS, build_head, build_roi_extractor</td></tr><tr><td>from .base_roi_head import BaseRoIHead</td></tr><tr><td>from .test_mixins import BBoxTestMixin, MaskTestMixin</td></tr><tr><td>@HEADS.register_module()</td></tr><tr><td>class StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\\n    &quot;&quot;&quot;Simplest base roi head including one bbox head and one mask head.\\n    &quot;&quot;&quot;\\n    def init_assigner_sampler(self):\\n    def init_bbox_head(self, bbox_roi_extractor, bbox_head):\\n    def init_mask_head(self, mask_roi_extractor, mask_head):\\n    def forward_dummy(self, x, proposals):\\n    def forward_train(self,\\n        x,\\n        img_metas,\\n        proposal_list,\\n        gt_bboxes,\\n        gt_labels,\\n        gt_bboxes_ignore=None,\\n        gt_masks=None):\\n    def _bbox_forward(self, x, rois):\\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\\n        img_metas):\\n    def _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\\n        img_metas):\\n    def _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None):</td></tr></table>',\n",
       "  'page_idx': 81,\n",
       "  'outline': [69, 213, 543, 709]},\n",
       " {'type': 'text',\n",
       "  'text': 'def simple_test(self, x, proposal_list, img_metas, proposals=None, rescale=False):    \"\"\"Test without augmentation.\"\"\"',\n",
       "  'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': \"Double Head's modification is mainly in the bbox_forward logic, and it inherits other logics from the StandardRoIHead. In the mmdet/models/roi_heads/double_roi_head.py, we implement the new RoI Head as the following:\",\n",
       "  'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': 'from ..builder import HEADS from .standard_roi_head import StandardRoIHead @HEADS.register_module() class DoubleHeadRoIHead(StandardRoIHead): \"\"\"RoI head for Double Head RCNN https://arxiv.ora/abs/1904.06493 def __init__(self, req_roi_scale_factor, **kwargs): super(DoubleHeadRoIHead, self).__init__(**kwargs) self.reg_roi_scale_factor = req_roi_scale_factor def _bbox_forward(self, x, rois): bbox_cls_feats = self.bbox_roi_extractor(x[:self.bbox_roi_extractor.num_inputs], rois) bbox_reg_feats = self.bbox_roi_extractor.num_inputs], x[:self.bbox_roi_extractor.num_inputs], rois, roi_scale_factor=self.reg_roi_scale_factor) if self.with_shared_head: bbox_cls_feats = self.shared_head(bbox_cls_feats) bbox_reg_feats = self.shared_head(bbox_reg_feats) cls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats) bbox_results = dict( cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_cls_feats) return bbox_results',\n",
       "  'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': 'Last, the users need to add the module in mmdet/models/bbox_heads/__init__.py and mmdet/models/roi_heads/__init__.py thus the corresponding registry could find and load them.',\n",
       "  'page_idx': 82},\n",
       " {'type': 'text', 'text': 'Alternatively, the users can add', 'page_idx': 82},\n",
       " {'type': 'text',\n",
       "  'text': \"custom_imports=dict(    imports=['mmdet.models.roi_heads.double_roi_head', 'mmdet.models.bbox_heads.double_  $\\\\rightarrow$  bbox_head'])\",\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'to the config file and achieve the same goal.',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'The config file of Double Head R- CNN is as the following',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': \"base_  $=$  '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py' model  $=$  dict( roi_head  $=$  dict( type  $=$  'DoubleHeadRoIHead', reg_roi_scale_factor  $= 1,3$  bbox_head  $=$  dict( delete  $=$  True, type  $=$  'DoubleConvFCBBoxHead', num_conv  $= 4$  num_fcs  $= 2$  in_channels  $= 256$  conv_out_channels  $= 1024$  fc_out_channels  $= 1024$  roi_feat_size  $= 7$  num_classes  $= 80$  bboxcoder  $=$  dict( type  $=$  'DeltaXYwHBBoxCoder', target_means  $=$  [0. , 0. , 0. ., target_std  $=$  [0.1, 0.1, 0.2, 0.2]), reg_class_agnostic  $=$  False, loss_cls  $=$  dict( type  $=$  'CrossEntropyLoss', use_sigmoid  $=$  False, loss_weight  $= 2.0$  loss bbox  $=$  dict(type  $=$  'SmoothLLoss', beta  $= 1.0$  , loss_weight  $= 2.0$  )\",\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'Since MMDetection 2.0, the config system supports to inherit configs such that the users can focus on the modification. The Double Head R- CNN mainly uses a new DoubleHeadRoIHead and a new DoubleConvFCBBoxHead, the arguments are set according to the __init__ function of each module.',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': '11.1.4 Add new loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume you want to add a new loss as MyLoss, for bounding box regression. To add a new loss function, the users need implement it in mmdet/models/losses/my_loss.py. The decorator weighted_loss enable the loss to be weighted for each element.',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch import torch.nn as nn from ..builder import LOSSES from .utils import weighted_loss @weighted_loss def my_loss(pred, target): assert pred_size()  $= =$  target.size() and target.numel() > 0 loss  $=$  torch.abs(pred - target) return loss',\n",
       "  'page_idx': 83},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': '@LOSSES.register_module() class MyLoss(nn.Module):',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': \"def __init__(self, reduction  $\\\\coloneqq$  'mean', loss_weight  $\\\\coloneqq \\\\mathbb{1}$  .0): super(MyLoss, self).__init__() self.reduction  $=$  reduction self.loss_weight  $=$  loss_weight def forward(self, pred, target, weight  $\\\\coloneqq$  None, avg_factor  $\\\\coloneqq$  None, reduction\\toverride  $\\\\coloneqq$  None): assert reduction override in (None, 'none', 'mean', 'sum') reduction  $=$  ( reduction override if reduction override else self.reduction) loss_bbox  $=$  self.loss_weight \\\\* my_loss( pred, target, weight, reduction  $\\\\coloneqq$  reduction, avg_factor  $\\\\coloneqq$  avg_factor) return loss_bbox\",\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'Then the users need to add it in the mmdet/models/losses/__init__.py.',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'from .my_loss import MyLoss, my_loss',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': \"Alternatively, you can add custom_imports=dict( imports=['mmdet.models.losses.my_loss'])\",\n",
       "  'page_idx': 84},\n",
       " {'type': 'text', 'text': '', 'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'to the config file and achieve the same goal.',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'To use it, modify the loss_xxx field. Since MyLoss is for regression, you need to modify the loss_bbox field in the head.',\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': \"loss_bbox=dict(type='MyLoss', loss_weight=1.0))\",\n",
       "  'page_idx': 84},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 11. Tutorial 4: Customize Models',\n",
       "  'page_idx': 85},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '12.1 Customize optimization settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.1 Customize optimizer supported by Pytorch',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'We already support to use all the optimizers implemented by PyTorch, and the only modification is to change the optimizer field of config files. For example, if you want to use ADAM (note that the performance could drop a lot), the modification could be as the following.',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': \"optimizer  $=$  dict(type  $=$  'Adam',  $\\\\mathbf{lr} = \\\\mathbb{0}$  .0003, weight_decay  $= 0$  .0001)\",\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'To modify the learning rate of the model, the users only need to modify the lr in the config of optimizer. The users can directly set arguments following the API doc of PyTorch.',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.2 Customize self-implemented optimizer',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define a new optimizer',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'A customized optimizer could be defined as following.',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'Assume you want to add a optimizer named MyOptimizer, which has arguments a, b, and c. You need to create a new directory named mmdet/core/optimizer. And then implement the new optimizer in a file, e.g., in mmdet/core/ optimizer/my_optimizer.py:',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': 'from .registry import OPTIMIZERS from torch.optim import Optimizer @OPTIMIZERS.register_module() class MyOptimizer(Optimizer): def init__self,a,b,c)',\n",
       "  'page_idx': 86},\n",
       " {'type': 'text',\n",
       "  'text': '2. Add the optimizer to registry',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'To find the above module defined above, this module should be imported into the main namespace at first. There are two options to achieve it.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Modify mmdet/core/optimizer/__init__.py to import it.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'The newly defined module should be imported in mmdet/core/optimizer/__init__.py so that the registry will find the new module and add it:',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'from .my_optimizer import MyOptimizer',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': \"Use custom_imports in the config to manually import it custom_imports  $=$  dict imports  $=$  ['mmdet.core.optimizer.my_optimizer'], allow_failed_  $\\\\rightarrow$  imports  $=$  False)\",\n",
       "  'page_idx': 87},\n",
       " {'type': 'text', 'text': '', 'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'The module mmdet.core.optimizer.my_optimizer will be imported at the beginning of the program and the class MyOptimizer is then automatically registered. Note that only the package containing the class MyOptimizer should be imported. mmdet.core.optimizer.my_optimizer.MyOptimizer cannot be imported directly.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Actually users can use a totally different file directory structure using this importing method, as long as the module root can be located in PYTHONPATH.',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': '3. Specify the optimizer in the config file',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Then you can use MyOptimizer in optimizer field of config files. In the configs, the optimizers are defined by the field optimizer like the following:',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': \"optimizer  $=$  dict(type  $=$  'SGD',  $\\\\mathtt{lr} = \\\\mathtt{0}$  .02, momentum  $= 0$  .9, weight_decay  $= 0$  .00001)\",\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': \"To use your own optimizer, the field can be changed to optimizer  $=$  dict(type  $=$  'MyOptimizer',  $\\\\exists = \\\\exists$  value, b=b_value,  $\\\\mathtt{c} = \\\\mathtt{c}$  value)\",\n",
       "  'page_idx': 87},\n",
       " {'type': 'text', 'text': '', 'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.3 Customize optimizer constructor',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'Some models may have some parameter- specific settings for optimization, e.g. weight decay for BatchNorm layers. The users can do those fine- grained parameter tuning through customizing optimizer constructor:',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'from mmcv.utils import build_from_cfg from mmcv.runner.optimizer import OPTIMIZER_BUILDERS, OPTIMIZERS from mmdet.utils import get_root_logger from .my_optimizer import MyOptimizer @OPTIMIZER_BUILDERS.register_module() class MyOptimizerConstructor(object): def _init__(self, optimizer_cfg, paramwise_cfg=None): def _call__(self, model):',\n",
       "  'page_idx': 87},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 87},\n",
       " {'type': 'text',\n",
       "  'text': 'The default optimizer constructor is implemented here, which could also serve as a template for new optimizer constructor.',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': '12.1.4 Additional settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'Tricks not implemented by the optimizer should be implemented through optimizer constructor (e.g., set parameterwise learning rates) or hooks. We list some common settings that could stabilize the training or accelerate the training. Feel free to create PR, issue for more settings.',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': '- Use gradient clip to stabilize training: Some models need gradient clip to clip the gradients to stabilize the training process. An example is as below:',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'If your config inherits the base config which already sets the optimizer_config, you might need _delete_=True to override the unnecessary settings. See the config documentation for more details.',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': \"- Use momentum schedule to accelerate model convergence: We support momentum scheduler to modify model's momentum according to learning rate, which could make the model converge in a faster way. Momentum scheduler is usually used with LR scheduler, for example, the following config is used in 3D detection to accelerate convergence. For more details, please refer to the implementation of CyclicLRUpdater and CyclicMomentumUpdater.\",\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': \"lr_config = dict(policy='cyclic', target_ratio=(10, 1e- 4), cyclic_times=1, step_ratio_up=0.4, ) momentum_config = dict(policy='cyclic', target_ratio=(0.85 / 0.95, 1), cyclic_times=1, step_ratio_up=0.4, )\",\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': '12.2 Customize training schedules',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': 'By default we use step learning rate with 1x schedule, this calls StepLRHook in MMCV. We support many other learning rate schedule here, such as CosineAnnealing and Poly schedule. Here are some examples',\n",
       "  'page_idx': 88},\n",
       " {'type': 'text',\n",
       "  'text': \"- Poly schedule: lr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=False)\",\n",
       "  'page_idx': 88},\n",
       " {'type': 'text', 'text': '- ConsineAnnealing schedule:', 'page_idx': 88},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/08bca5394a4d7bdfd1fd80e2ce829e9617df705a9959633d810386f7694a9456.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>lr_config = dict(\\n  policy=&#x27;CosineAnnealing&#x27;,\\n  warmup=&#x27;linear&#x27;,\\n  warmup_liters=10000,\\n  warmup_ratio=1.0 / 10,\\n  min_lr_ratio=1e-5)</td></tr></table>',\n",
       "  'page_idx': 89,\n",
       "  'outline': [93, 76, 543, 154]},\n",
       " {'type': 'text',\n",
       "  'text': '12.3 Customize workflow',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': \"Workflow is a list of (phase, epochs) to specify the running order and epochs. By default it is set to be workflow  $=$  [('train', 1)]\",\n",
       "  'page_idx': 89},\n",
       " {'type': 'text', 'text': '', 'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': 'which means running 1 epoch for training. Sometimes user may want to check some metrics (e.g. loss, accuracy) about the model on the validate set. In such case, we can set the workflow as so that 1 epoch for training and 1 epoch for validation will be run iteratively.',\n",
       "  'page_idx': 89},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/65545831fee3f3a119b54b34777c6e869236b51b4d25552966a81f0039509dfc.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>[&#x27;train&#x27;, &#x27;1&#x27;, &#x27;val&#x27;, &#x27;1&#x27;]</td></tr></table>',\n",
       "  'page_idx': 89,\n",
       "  'outline': [69, 289, 543, 306]},\n",
       " {'type': 'text', 'text': '', 'page_idx': 89},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': \"1. The parameters of model will not be updated during val epoch. \\n2. Keyword total_epochs in the config only controls the number of training epochs and will not affect the validation workflow. \\n3. Workflows [('train', 1), ('val', 1)] and [('train', 1)] will not change the behavior of EvalHook because EvalHook is called by after_train_epoch and validation workflow only affect hooks that are called through after_val_epoch. Therefore, the only difference between [('train', 1), ('val', 1)] and [('train', 1)] is that the runner will calculate losses on validation set after each training epoch.\",\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': '12.4 Customize hooks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': '12.4.1 Customize self-implemented hooks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': '1. Implement a new hook',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 89},\n",
       " {'type': 'text',\n",
       "  'text': 'There are some occasions when the users might need to implement a new hook. MMDetection supports customized hooks in training (#3395) since v2.3.0. Thus the users could implement a hook directly in mmdet or their mmdet- based codebases and use the hook by only modifying the config in training. Before v2.3.0, the users need to modify the code to get the hook registered before training starts. Here we give an example of creating a new hook in mmdet and using it in training.',\n",
       "  'page_idx': 89},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/200a5e00bf236d9a4e7b8b5b51b704ca7747e13333ede4cc5d83223dedccfd0b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>from mmcv.runner import HOOKS, Hook</td></tr><tr><td>@HOOKS.register_module()</td></tr><tr><td>class MyHook(Hook):</td></tr><tr><td>def __init__(self, a, b):</td></tr></table>',\n",
       "  'page_idx': 89,\n",
       "  'outline': [69, 622, 543, 722]},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/a7be00f85e243bc5af3bb0e7e7ac9921d1ab45ca96a2acc587e2767d5564b11e.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>pass</td></tr><tr><td>def before_run(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def after_run(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def before_epoch(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def after_epoch(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def before_iter(self, runner):</td></tr><tr><td>pass</td></tr><tr><td>def after_iter(self, runner):</td></tr><tr><td>pass</td></tr></table>',\n",
       "  'page_idx': 90,\n",
       "  'outline': [69, 82, 543, 318]},\n",
       " {'type': 'text',\n",
       "  'text': 'Depending on the functionality of the hook, the users need to specify what the hook will do at each stage of the training in before_run, after_run, before_epoch, after_epoch, before_iter, and after_iter.',\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '2. Register the new hook',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': 'Then we need to make MyHook imported. Assuming the file is in mmdet/core/utils/my_hook.py there are two ways to do that:',\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '- Modify mmdet/core/utils/__init__.py to import it. The newly defined module should be imported in mmdet/core/utils/__init__.py so that the registry will find the new module and add it:',\n",
       "  'page_idx': 90},\n",
       " {'type': 'text', 'text': 'from .my_hook import MyHook', 'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': \"- Use custom_imports in the config to manually import itcustom_imports = dict(imports=['mmdet.core.utils.my_hook'], allow_failed_imports=False)\",\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '3. Modify the config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 90},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3ce5ea53df6a33bde776da8129d3d10415c3f5e1016809c578bc969636f6cede.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_hooks = [</td></tr><tr><td>dict(type=&#x27;MyHook&#x27;, a=a_value, b=b_value)</td></tr></table>',\n",
       "  'page_idx': 90,\n",
       "  'outline': [69, 583, 543, 628]},\n",
       " {'type': 'text',\n",
       "  'text': \"You can also set the priority of the hook by adding key priority to 'NORMAL' or 'HIGHEST' as below\",\n",
       "  'page_idx': 90},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/5e4d33181ff9a26991b0beed3e852a4dd2c75b040010487c406fbf687353d4ab.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>custom_hooks = [</td></tr><tr><td>dict(type=&#x27;MyHook&#x27;, a=a_value, b=b_value, priority=&#x27;NORMAL&#x27;)</td></tr></table>',\n",
       "  'page_idx': 90,\n",
       "  'outline': [69, 654, 543, 696]},\n",
       " {'type': 'text',\n",
       "  'text': \"By default the hook's priority is set as NORMAL during registration.\",\n",
       "  'page_idx': 90},\n",
       " {'type': 'text',\n",
       "  'text': '12.4.2 Use hooks implemented in MMCV',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'If the hook is already implemented in MMCV, you can directly modify the config to use the hook as below',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': '4. Example: NumClassCheckHook',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'We implement a customized hook named NumClassCheckHook to check whether the num_classes in head matches the length of CLASSES in dataset.We set it in default_runtime.py.',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text', 'text': 'We set it in default_runtime.py.', 'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': \"custom_hooks  $=$  [dict(type  $\\\\coloneqq$  'NumClassCheckHook')]\",\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': '12.4.3 Modify default runtime hooks',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'There are some common hooks that are not registered through custom_hooks, they are',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'log_config checkpoint_config evaluation lr_config optimizer_config momentum_config',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': \"In those hooks, only the logger hook has the VERY_LOW priority, others' priority are NORMAL. The above- mentioned tutorials already covers how to modify optimizer_config, momentum_config, and lr_config. Here we reveals how what we can do with log_config, checkpoint_config, and evaluation.\",\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'Checkpoint config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'The MMCV runner will use checkpoint_config to initialize CheckpointHook.',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'checkpoint_config  $=$  dict(interval  $= 1$',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'The users could set max_keep_ckpt s to only save only small number of checkpoints or decide whether to store state dict of optimizer by save_optimizer. More details of the arguments are here',\n",
       "  'page_idx': 91},\n",
       " {'type': 'text', 'text': 'Log config', 'text_level': 1, 'page_idx': 91},\n",
       " {'type': 'text',\n",
       "  'text': 'The log_config wraps multiple logger hooks and enables to set intervals. Now MMCV supports WandbLoggerHook, MlflowLoggerHook, and TensorboardLoggerHook. The detail usages can be found in the doc.',\n",
       "  'page_idx': 91},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/664f26d0798238c78d0ddb3f720b3bdcdb7375c2e00189cf6e93f11a10e5de6c.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>log_config = dict(\\n    interval=50,\\n    hooks=[\\n        dict(type=&#x27;TextLoggerHook&#x27;),\\n        dict(type=&#x27;TensorboardLoggerHook&#x27;)\\n    ]</td></tr></table>',\n",
       "  'page_idx': 91,\n",
       "  'outline': [69, 619, 543, 697]},\n",
       " {'type': 'text',\n",
       "  'text': 'Evaluation config',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 92},\n",
       " {'type': 'text',\n",
       "  'text': 'The config of evaluation will be used to initialize the EvalHook. Except the key interval, other arguments such as metric will be passed to the dataset.evaluate()',\n",
       "  'page_idx': 92},\n",
       " {'type': 'text',\n",
       "  'text': \"evaluation = dict(interval=1, metric='bbox')\",\n",
       "  'page_idx': 92},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 12. Tutorial 5: Customize Runtime Settings',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 93},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 6: CUSTOMIZE LOSSES',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection provides users with different loss functions. But the default configuration may be not applicable for different datasets or models, so users may want to modify a specific loss to adapt the new situation.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'This tutorial first elaborate the computation pipeline of losses, then give some instructions about how to modify each step. The modification can be categorized as tweaking and weighting.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '13.1 Computation pipeline of a loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'Given the input prediction and target, as well as the weights, a loss function maps the input tensor to the final loss scalar. The mapping can be divided into four steps:',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '1. Set the sampling method to sample positive and negative samples.  \\n2. Get element-wise or sample-wise loss by the loss kernel function.  \\n3. Weighting the loss with a weight tensor element-wise.  \\n4. Reduce the loss tensor to a scalar.  \\n5. Weighting the loss with a scalar.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '13.2 Set sampling method (step 1)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': 'For some loss functions, sampling strategies are needed to avoid imbalance between positive and negative samples. For example, when using CrossEntropyLoss in RPN head, we need to set RandomSampler in train_cfg',\n",
       "  'page_idx': 94},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3db1ae3fe201591b6d8d5a318edc1bda97e95cfe4b1432f857cd594f2643750a.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>train_cfg=dict(</td></tr><tr><td>rpn=dict(</td></tr><tr><td>sampler=dict(</td></tr><tr><td>type=&#x27;RandomSampler&#x27;,</td></tr><tr><td>num=256,</td></tr><tr><td>pos_fraction=0.5,</td></tr><tr><td>neg_pos_ub=-1,</td></tr><tr><td>add_gt_as_proposals=False))</td></tr></table>',\n",
       "  'page_idx': 94,\n",
       "  'outline': [69, 541, 543, 644]},\n",
       " {'type': 'text',\n",
       "  'text': 'For some other losses which have positive and negative sample balance mechanism such as Focal Loss, GHMC, and QualityFocalLoss, the sampler is no more necessary.',\n",
       "  'page_idx': 94},\n",
       " {'type': 'text',\n",
       "  'text': '13.3 Tweaking loss',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 95},\n",
       " {'type': 'text',\n",
       "  'text': '13.3 Tweaking lossTweaking a loss is more related with step 2, 4, 5, and most modifications can be specified in the config. Here we take Focal Loss (FL) as an example. The following code sniper are the construction method and config of FL respectively, they are actually one to one correspondence.',\n",
       "  'page_idx': 95},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/e24c185d69f9fa4cb7137aff1692ea19753c48d0636a1d7ec0546cf09ee66c98.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>@LOSSES.register_module()\\nclass FocalLoss(nn.Module):\\n    def __init__(self,\\n        use sigmoid=True,\\n        gamma=2.0,\\n        alpha=0.25,\\n        reduction=&#x27;mean&#x27;,\\n        loss_weight=1.0)</td></tr><tr><td>loss_cls=dict(\\n    type=&#x27;FocalLoss&#x27;,\\n    use sigmoid=True,\\n    gamma=2.0,\\n    alpha=0.25,\\n    loss_weight=1.0)</td></tr></table>',\n",
       "  'page_idx': 95,\n",
       "  'outline': [69, 147, 543, 348]},\n",
       " {'type': 'text',\n",
       "  'text': '13.3.1 Tweaking hyper-parameters (step 2)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 95},\n",
       " {'type': 'text',\n",
       "  'text': 'gamma and beta are two hyper- parameters in the Focal Loss. Say if we want to change the value of gamma to be 1.5 and alpha to be 0.5, then we can specify them in the config as follows:',\n",
       "  'page_idx': 95},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/1636e9c2008d82d3d2c43045335babf9aae52bbb98e2a46272ab17ccba23952e.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>loss_cls=dict(</td></tr><tr><td>type=&#x27;FocalLoss&#x27;,</td></tr><tr><td>use sigmoid=True,</td></tr><tr><td>gamma=1.5,</td></tr><tr><td>alpha=0.5,</td></tr><tr><td>loss_weight=1.0)</td></tr></table>',\n",
       "  'page_idx': 95,\n",
       "  'outline': [69, 427, 543, 506]},\n",
       " {'type': 'text',\n",
       "  'text': '13.3.2 Tweaking the way of reduction (step 3)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 95},\n",
       " {'type': 'text',\n",
       "  'text': 'The default way of reduction is mean for FL. Say if we want to change the reduction from mean to sum, we can specify it in the config as follows:',\n",
       "  'page_idx': 95},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/28f7f4ff0747401c4d8dad437d3d67da599b72410db105ceb56e2f2980dca03b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>loss_cls=dict(</td></tr><tr><td>type=&#x27;FocalLoss&#x27;,</td></tr><tr><td>use sigmoid=True,</td></tr><tr><td>gamma=2.0,</td></tr><tr><td>alpha=0.25,</td></tr><tr><td>loss_weight=1.0,</td></tr><tr><td>reduction=&#x27;sum&#x27;)</td></tr></table>',\n",
       "  'page_idx': 95,\n",
       "  'outline': [69, 585, 543, 676]},\n",
       " {'type': 'text',\n",
       "  'text': '13.3.3 Tweaking loss weight (step 5)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': 'The loss weight here is a scalar which controls the weight of different losses in multi- task learning, e.g. classification loss and regression loss. Say if we want to change to loss weight of classification loss to be 0.5, we can specify it in the config as follows:',\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': \"loss_cls=dict( type='FocalLoss', use sigmoid=True, gamma=2.0, alpha=0.25, loss_weight=0.5)\",\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': '13.4 Weighting loss (step 3)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': 'Weighting loss means we re- weight the loss element- wisely. To be more specific, we multiply the loss tensor with a weight tensor which has the same shape. As a result, different entries of the loss can be scaled differently, and so called element- wisely. The loss weight varies across different models and highly context related, but overall there are two kinds of loss weights, label_weights for classification loss and bbox_weights for bbox regression loss. You can find them in the get_target method of the corresponding head. Here we take ATSSHead as an example, which inherit AnchorHead but overwrite its get_targets method which yields different label_weights and bbox_weights.',\n",
       "  'page_idx': 96},\n",
       " {'type': 'text', 'text': 'class ATSSHead(AnchorHead):', 'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': '```pythondef get_targets(self, anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None, gt_labels_list=None, label_channels=1, unmap_outputs=True):```',\n",
       "  'page_idx': 96},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 13. Tutorial 6: Customize Losses',\n",
       "  'page_idx': 97},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 7: FINETUNING MODELS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'Detectors pre- trained on the COCO dataset can serve as a good pre- trained model for other datasets, e.g., CityScapes and KITTI Dataset. This tutorial provides instruction for users to use the models provided in the Model Zoo for other datasets to obtain better performance.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'There are two steps to finetune a model on a new dataset.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': '- Add support for the new dataset following Tutorial 2: Customize Datasets.- Modify the configs as will be discussed in this tutorial.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'Take the finetuning process on Cityscapes Dataset as an example, the users need to modify five parts in the config.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': '14.1 Inherit base configs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'To release the burden and reduce bugs in writing the whole configs, MMDetection V2.0 support inheriting configs from multiple existing configs. To finetune a Mask RCNN model, the new config needs to inherit _base_/models/mask_rcnn_r50_fpn.py to build the basic structure of the model. To use the Cityscapes Dataset, the new config can also simply inherit _base_/datasets/cityscapes_instance.py. For runtime settings such as training schedules, the new config needs to inherit _base_/default_runtime.py. This configs are in the configs directory and the users can also choose to write the whole contents rather than use inheritance.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': \"```python- _base_ = [    .../__base_/models/mask_rcnn_r50_fpn.py',    .../__base_/datasets/cityscapes_instance.py', .../__base_/default_runtime.py']```\",\n",
       "  'page_idx': 98},\n",
       " {'type': 'text', 'text': '14.2 Modify head', 'text_level': 1, 'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': 'Then the new config needs to modify the head according to the class numbers of the new datasets. By only changing num_classes in the roi_head, the weights of the pre- trained models are mostly reused except the final prediction head.',\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': \"model  $=$  dict( pretrained  $=$  None, roi_head  $=$  dict( bbox_head  $=$  dict( type  $=$  'Shared2FCBBoxHead', in_channels  $= 256$  fc_out_channels  $= 1024$  roi_feat_size  $= 7$\",\n",
       "  'page_idx': 98},\n",
       " {'type': 'text',\n",
       "  'text': \"num_classes  $= 8$  bbox_coder  $=$  dict( type  $=$  'DeltaXYwHBBoxCoder', target_means  $=$  [0. , 0. , 0. , 0. ], target_std  $\\\\mathbf{\\\\dot{s}} =$  [0.1, 0.1, 0.2, 0.2]), reg_class_agnostic  $=$  False, loss_cls  $=$  dict( type  $=$  'CrossEntropyLoss', use_sigmoid  $=$  False, loss_weight  $= 1.0$  ), loss_bbox  $=$  dict(type  $=$  'SmoothLLoss', beta  $= 1.0$  , loss_weight  $= 1.0$  )), mask_head  $=$  dict( type  $=$  'FCNMaskHead', num_conv  $s = 4$  in_channels  $= 256$  conv_out_channels  $= 256$  num_classes  $= 8$  loss_mask  $=$  dict( type  $=$  'CrossEntropyLoss', use_mask  $=$  True, loss_weight  $= 1.0$  ))\",\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': '14.3 Modify dataset',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': 'The users may also need to prepare the dataset and write the configs about dataset. MMDetection V2.0 already support VOC, WIDER FACE, COCO and Cityscapes Dataset.',\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': '14.4 Modify training schedule',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': 'The finetuning hyperparameters vary from the default schedule. It usually requires smaller learning rate and less training epochs',\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': \"optimizer # lr is set for a batch size of 8 optimizer  $=$  dict(type  $=$  'SGD',  $\\\\mathtt{lr} = \\\\mathtt{0}$  .01, momentum  $= 0$  .9, weight_decay  $= 0$  .0001) optimizer_config  $=$  dict(grad clip  $\\\\coloneqq$  None) # learning policy lr_config  $=$  dict( policy  $=$  'step', warmup  $=$  'linear', warmup_iter  $s = 500$  warmup_ratio  $= 0$  .001, step=[7]) # the max_epochs and step in lr_config need specifically tuned for the customized dataset runner  $=$  dict(max_epochs  $= 8$  log_config  $=$  dict(interval  $= 100$\",\n",
       "  'page_idx': 99},\n",
       " {'type': 'text',\n",
       "  'text': '14.5 Use pre-trained model',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 100},\n",
       " {'type': 'text',\n",
       "  'text': 'To use the pre- trained model, the new config add the link of pre- trained models in the load_from. The users might need to download the model weights before training to avoid the download time during training.',\n",
       "  'page_idx': 100},\n",
       " {'type': 'text',\n",
       "  'text': 'Chapter 14. Tutorial 7: Finetuning Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 101},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '- Tutorial 8: Pytorch to ONNX (Experimental)  \\n- How to convert models from Pytorch to ONNX      * Prerequisite      * Usage      * Description of all arguments  \\n- How to evaluate the exported models      * Prerequisite      * Usage      * Description of all arguments      * Results and Models  \\n- List of supported models exportable to ONNX  \\n- The Parameters of Non-Maximum Suppression in ONNX Export      - Reminders      - FAQs',\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '15.1 How to convert models from Pytorch to ONNX',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '15.1.1 Prerequisite',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 102},\n",
       " {'type': 'text',\n",
       "  'text': '1. Install the prerequisites following get_started.md/Prepare environment.  \\n2. Build custom operators for ONNX Runtime and install MMCV manually following How to build custom operators for ONNX Runtime  \\n3. Install MMdetection manually following steps 2-3 in get_started.md/Install MMdetection.',\n",
       "  'page_idx': 102},\n",
       " {'type': 'text', 'text': '15.1.2 Usage', 'text_level': 1, 'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/deployment/pytorch2onnx.py \\\\${CONFIG_FILE} \\\\${CHECKPOINT_FILE} \\\\- - output- file \\\\ ${OUTPUT_FILE} \\\\- - input - img}$ {INPUT_IMAGE_PATH} \\\\- - shape \\\\ ${{IMAGE_SHAPE} \\\\- - test - img}$ {TEST_IMAGE_PATH} \\\\- - opset - version \\\\\\\\){OPSET_VERSION} \\\\- - cfg - options \\\\\\\\({CFG_OPTIONS} \\\\- - dynamic - export \\\\- - show \\\\- - verify \\\\- - simplify} \\\\',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': '15.1.3 Description of all arguments',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': 'config : The path of a model config file.',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': '15.1.3 Description of all arguments- config: The path of a model config file.- checkpoint: The path of a model checkpoint file.- - output- file: The path of output ONNX model. If not specified, it will be set to tmp. onnx.- - input- img: The path of an input image for tracing and conversion. By default, it will be set to tests/data/color.jpg.- - shape: The height and width of input tensor to the model. If not specified, it will be set to 8000 1216. - - test- img: The path of an image to verify the exported ONNX model. By default, it will be set to None, meaning it will use - - input- img for verification.- - opset- version: The opset version of ONNX. If not specified, it will be set to 11. - - dynamic- export: Determines whether to export ONNX model with dynamic input and output shapes. If not specified, it will be set to False.- - show: Determines whether to print the architecture of the exported model and whether to show detection outputs when - - verify is set to True. If not specified, it will be set to False.- - verify: Determines whether to verify the correctness of an exported model. If not specified, it will be set to False.- - simplify: Determines whether to simplify the exported ONNX model. If not specified, it will be set to False.- - cfg- options: Override some settings in the used config file, the key- value pair in xxx=yyy format will be merged into config file.- - skip- post- process: Determines whether export model without post process. If not specified, it will be set to False. Notice: This is an experimental option. Only work for some single stage models. Users need to implement the post- process by themselves. We do not guarantee the correctness of the exported model.',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text', 'text': 'Example:', 'page_idx': 103},\n",
       " {'type': 'text',\n",
       "  'text': 'Example:python tools/deployment/pytorch2onnx.py \\\\  - - configs/yolo/yolov3_d53_mstrain- 608_273e_coco.py \\\\  - - checkpoints/yolo/yolov3_d53_mstrain- 608_273e_coco.pth \\\\',\n",
       "  'page_idx': 103},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 103},\n",
       " {'type': 'text', 'text': '(continued from previous page)', 'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '- -output-file checkpoints/yolo/yolov3_d51_mstrain-608_273e_coco.onnx \\n--input-img demo/demo.jpg \\n--test-img tests/data/color.jpg \\n--shape 608 608 \\n--show \\n--verify \\n--dynamic-export \\n--cfg-options \\\\model.test_cfg.deploy_nms_pre=-1 \\\\',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '15.2 How to evaluate the exported models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'We prepare a tool tools/deployment/test.py to evaluate ONNX models with ONNXRuntime and TensorRT.',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '15.2.1 Prerequisite',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Install onnx and onnxruntime (CPU version)',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'pip install onnx onnxruntime  $\\\\coloneqq = 1$  .5.1',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'If you want to run the model on GPU, please remove the CPU version before using the GPU version.',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'pip uninstall onnxruntime pip install onnxruntime- gpu',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Note: onnxruntime- gpu is version- dependent on CUDA and CUDNN, please ensure that your environment meets the requirements.',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Build custom operators for ONNX Runtime following How to build custom operators for ONNX Runtime',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'Install TensorRT by referring to How to build TensorRT plugins in MMCV (optional)',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text', 'text': '15.2.2 Usage', 'text_level': 1, 'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/deployment/test.py \\\\ ${CONFIG_FILE} \\\\$ {MODEL_FILE} \\\\- - out}\\\\{OUTPUT_FILE\\\\} \\\\- - backend}\\\\{BACKEND\\\\} \\\\- - format- only}\\\\{FORMAT_ONLY\\\\} \\\\- - eval}\\\\{EVALUATION_METRICS\\\\} \\\\- - show- dir}\\\\{SHOW_DIRECTORY\\\\} \\\\- - - show- score- thr}\\\\{SHOW_SCORE_THRESHOLD\\\\} \\\\- - - cfg- options}\\\\{CFG_OPTIONS\\\\} \\\\- - - eval- options}\\\\{EVALUATION_OPTIONS\\\\} \\\\',\n",
       "  'page_idx': 104},\n",
       " {'type': 'text',\n",
       "  'text': '15.2.3 Description of all arguments',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'config: The path of a model config file. model: The path of an input model file. - - out: The path of output result file in pickle format. - - backend: Backend for input model to run and should be onnxruntime or tensorrt. - - format- only : Format the output results without perform evaluation. It is useful when you want to format the result to a specific format and submit it to the test server. If not specified, it will be set to False. - - eval: Evaluation metrics, which depends on the dataset, e.g., \"bbox\", \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC. - - show- dir: Directory where painted images will be saved - - show- score- thr: Score threshold. Default is set to 0.3. - - cfg- options: Override some settings in the used config file, the key- value pair in xxx=yyy format will be merged into config file. - - eval- options: Custom options for evaluation, the key- value pair in xxx=yyy format will be kwargs for dataset.evvaluate() function',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'If the deployed backend platform is TensorRT, please add environment variables before running the file:',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'export ONNX_BACKEND  $\\\\equiv$  MMCVTensorRT',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'If you want to use the - - dynamic - export parameter in the TensorRT backend to export ONNX, please remove the - - simplify parameter, and vice versa.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': '15.2.4 Results and Models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 105},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'All ONNX models are evaluated with dynamic shape on coco dataset and images are preprocessed according to the original config file. Note that CornerNet is evaluated without test- time flip, since currently only single- scale evaluation is supported with ONNX Runtime.- Mask AP of Mask R- CNN drops by  $1\\\\%$  for ONNX Runtime. The main reason is that the predicted masks are directly interpolated to original image in PyTorch, while they are at first interpolated to the preprocessed input image of the model and then to original image in other backend.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': '15.3 List of supported models exportable to ONNX',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'The table below lists the models that are guaranteed to be exportable to ONNX and runnable in ONNX Runtime.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'Minimum required version of MMCV is 1.3.5- All models above are tested with Pytorch  $= = 1.6.0$  and onnxruntime  $= = 1.5.1$  , except for CornerNet. For more details about the torch version when exporting CornerNet to ONNX, which involves mmcv: : cummax, please refer to the Known Issues in mmcv.',\n",
       "  'page_idx': 105},\n",
       " {'type': 'text',\n",
       "  'text': 'Though supported, it is not recommended to use batch inference in onnxruntime for DETER, because there is huge performance gap between ONNX and torch model (e.g. 33.5 vs  $39.9\\\\mathrm{mAP}$  on COCO for onnxruntime and torch respectively, with a batch size 2). The main reason for the gap is that these is non- negligible effect on the predicted regressions during batch inference for ONNX, since the predicted coordinates is normalized by img_shape (without padding) and should be converted to absolute format, but img_shape is not dynamically traceable thus the padded img_shape_for_onnx is used.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'Currently only single- scale evaluation is supported with ONNX Runtime, also mmcv: : SoftNonMaxSuppression is only supported for single image by now.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': '15.4 The Parameters of Non-Maximum Suppression in ONNX Export',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'In the process of exporting the ONNX model, we set some parameters for the NMS op to control the number of output bounding boxes. The following will introduce the parameter setting of the NMS op in the supported models. You can set these parameters through - - cfg- options.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'nms_pre: The number of boxes before NMS. The default setting is 1000. deploy_nms_pre: The number of boxes before NMS when exporting to ONNX model. The default setting is 0. max_per_img: The number of boxes to be kept after NMS. The default setting is 100. max_output_bones_per_class: Maximum number of output boxes per class of NMS. The default setting is 200.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text', 'text': '15.5 Reminders', 'text_level': 1, 'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'When the input model has custom op such as RoIA1. ign and if you want to verify the exported ONNX model, you may have to build mmcv with ONNXRuntime from source. mmcv.onnx. simplify feature is based on onnx- simplifier. If you want to try it, please refer to onnx in mmcv and onnxruntime op in mmcv for more information. If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, please try to dig a little deeper and debug a little bit more and hopefully solve them by yourself. Because this feature is experimental and may change fast, please always try with the latest mmcv and mmdetection.',\n",
       "  'page_idx': 106},\n",
       " {'type': 'text', 'text': '15.6 FAQs', 'text_level': 1, 'page_idx': 106},\n",
       " {'type': 'text', 'text': '- None', 'page_idx': 106},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 107},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '- Tutorial 9: ONNX to TensorRT (Experimental)  \\n- How to convert models from ONNX to TensorRT      * Prerequisite      * Usage  \\n- How to evaluate the exported models  \\n- List of supported models convertible to TensorRT  \\n- Reminders  \\n- FAQs',\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '16.1 How to convert models from ONNX to TensorRT',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '16.1.1 Prerequisite',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': '1. Please refer to get_started.md for installation of MMCV and MMDetection from source.  \\n2. Please refer to ONNXRuntime in mmcv and TensorRT plugin in mmcv to install mmcv-full with ONNXRuntime custom ops and TensorRT plugins.  \\n3. Use our tool pytorch2onnx to convert the model from PyTorch to ONNX.',\n",
       "  'page_idx': 108},\n",
       " {'type': 'text', 'text': '16.1.2 Usage', 'text_level': 1, 'page_idx': 108},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/150533eca9b6416a051deda1ef63e6edf959d7e5188376e3ed17bb8b18595aed.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>python tools/deployment/onnx2tensorrt.py</td><td>$ {CONFIG} \\\\\\\\\\n$ {MODEL} \\\\\\\\\\n--trt-file {{TRT_FILE}} \\\\\\\\\\n--input-img {{INPUT_IMAGE_PATH}} \\\\\\\\\\n--shape {{INPUT_IMAGE_SHAPE}} \\\\\\\\\\n--min-shape {{MIN_IMAGE_SHAPE}} \\\\\\\\\\n--max-shape {{MAX_IMAGE_SHAPE}} \\\\\\\\\\n--workspace-size {{WORKSPACE_SIZE}} \\\\\\\\\\n--show \\\\\\\\\\n--verify</td></tr></table>',\n",
       "  'page_idx': 108,\n",
       "  'outline': [67, 553, 543, 693]},\n",
       " {'type': 'text', 'text': 'Description of all arguments:', 'page_idx': 108},\n",
       " {'type': 'text',\n",
       "  'text': \"- config: The path of a model config file.- model: The path of an ONNX model file.- \\n--trt-file: The Path of output TensorRT engine file. If not specified, it will be set to tmp.trt.- \\n--input-img: The path of an input image for tracing and conversion. By default, it will be set to demo/demo.jpg.- \\n--shape: The height and width of model input. If not specified, it will be set to 400 600.- \\n--min-shape: The minimum height and width of model input. If not specified, it will be set to the same as \\n--shape.- \\n--max-shape: The maximum height and width of model input. If not specified, it will be set to the same as \\n--shape.- \\n--workspace-size: The required GPU workspace size in GiB to build TensorRT engine. If not specified, it will be set to 1 GiB.- \\n--show: Determines whether to show the outputs of the model. If not specified, it will be set to False.- \\n--verify: Determines whether to verify the correctness of models between ONNXRuntime and TensorRT. If not specified, it will be set to False.- \\n--verbose: Determines whether to print logging messages. It's useful for debugging. If not specified, it will be set to False.\",\n",
       "  'page_idx': 109},\n",
       " {'type': 'text', 'text': 'Example:', 'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': 'python tools/deployment/onnx2tensorrt.py \\\\ configs/retinanet/retinanet_r50_fpn_1x_coco.py \\\\ checkpoints/retinanet_r50_fpn_1x_coco.onnx \\\\ - - trt- file checkpoints/retinanet_r50_fpn_1x_coco.trt \\\\ - - input- img demo/demo.jpg \\\\ - - shape 400 600 \\\\ - - show \\\\ - - verify \\\\',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': '16.2 How to evaluate the exported models',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': 'We prepare a tool tools/dep1opyment/test.py to evaluate TensorRT models.',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to following links for more information.',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': '- how-to-evaluate-the-exported-models- results-and-models',\n",
       "  'page_idx': 109},\n",
       " {'type': 'text',\n",
       "  'text': '16.3 List of supported models convertible to TensorRT',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'The table below lists the models that are guaranteed to be convertible to TensorRT.',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text', 'text': 'Notes:', 'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'Notes:- All models above are tested with Pytorch==1.6.0, onma==1.7.0 and TensorRT- 7.2.1.6. Ubuntu- 16.04. x86_64- gnu.cuda- 10.2. cudnn8.0',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text', 'text': '16.4 Reminders', 'text_level': 1, 'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, we may not provide much help here due to the limited resources. Please try to dig a little deeper and debug by yourself.',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'Because this feature is experimental and may change fast, please always try with the latest mmcv and mmdetecion.',\n",
       "  'page_idx': 110},\n",
       " {'type': 'text', 'text': '16.5 FAQs', 'text_level': 1, 'page_idx': 110},\n",
       " {'type': 'text', 'text': 'None', 'page_idx': 110},\n",
       " {'type': 'text',\n",
       "  'text': 'MMDetection, Release 2.18.0',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 111},\n",
       " {'type': 'text',\n",
       "  'text': 'TUTORIAL 10: WEIGHT INITIALIZATION',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'During training, a proper initialization strategy is beneficial to speeding up the training or obtaining a higher performance. MMCV provide some commonly used methods for initializing modules like nn. Conv2d. Model initialization in MMdetection mainly uses init_cfg. Users can initialize models with following two steps:',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': '1. Define init_cfg for a model or its components in model_cfg, but init_cfg of children components have higher priority and will override init_cfg of parents modules. \\n2. Build model as usual, but call model.init_weights() method explicitly, and model parameters will be initialized as configuration.',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'The high- level workflow of initialization in MMdetection is :',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': \"model_cfg(init_cfg) - > build_from_cfg - > model - > init_weight() - > initialize(self, self.init_cfg) - > children's init_weight()\",\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': '17.1 Description',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'It is dict or list[dict], and contains the following keys and values:',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': \"type (str), containing the initializer name in INITIALIZERS, and followed by arguments of the initializer. layer (str or list[str]), containing the names of basi layers in Pytorch or MMCV with learnable parameters that will be initialized, e.g. 'Conv2d','DeformConv2d'. override (dict or list[dict]), containing the sub- modules that not inherit from BaseModule and whose initialization configuration is different from other layers' which are in 'layer' key. Initializer defined in type will work for all layers defined in 1ayer, so if sub- modules are not derived Classes of BaseModule but can be initialized as same ways of layers in 1ayer, it does not need to use override. override contains:\",\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'type followed by arguments of initializer; name to indicate sub- module which will be initialized.',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': '17.2 Initialize parameters',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'Inherit a new model from mmcv. runner. BaseModule or mmdet. models Here we show an example of FooModel.',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch.nn as nn from mmcv. runner import BaseModule class FooModel(BaseModule) def init__self,',\n",
       "  'page_idx': 112},\n",
       " {'type': 'text', 'text': '(continues on next page)', 'page_idx': 112},\n",
       " {'type': 'text',\n",
       "  'text': 'arg1, arg2, init_cfg=None): super(FooModel, self).__init__(init_cfg)',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'Initialize model by using init_cfg directly in code',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'import torch.nn as nn from mcmc:runter import BaseModule # or directly inherit mmdet models class FooModel(BaseModule) def __init__(self, arg1, arg2, init_cfg  $\\\\equiv$  XXX): super(FooModel, self).__init__(init_cfg)',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'Initialize model by using init_cfg directly in mcmc. Sequential or mcmc. ModuleList code',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'from mcmc:runter import BaseModule, ModuleList class FooModel(BaseModule) def __init__(self, arg1, arg2, init_cfg  $\\\\equiv$  None): super(FooModel, self).__init__(init_cfg) self.conv1  $=$  ModuleList(init_cfg  $\\\\equiv$  XXX)',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': 'Initialize model by using init_cfg in config file',\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': \"model  $=$  dict( model  $=$  dict( type  $\\\\equiv$  'FooModel', arg1  $\\\\equiv$  XXX, arg2  $\\\\equiv$  XXX, init_cfg  $\\\\equiv$  XXX),\",\n",
       "  'page_idx': 113},\n",
       " {'type': 'text',\n",
       "  'text': '17.3 Usage of init_cfg',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 114},\n",
       " {'type': 'text', 'text': '1. Initialize model by layer key', 'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'If we only define layer, it just initialize the layer in layer key.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'NOTE: Value of layer key is the class name with attributes weights and bias of Pytorch, (so such as MultiheadAttention layer is not supported).',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'Define layer key for initializing module with same configuration.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"init_cfg  $=$  dict(type  $=$  'Constant', layer  $=$  ['Conv1d', 'Conv2d', 'Linear'], val  $= 1$  # initialize whole module with same configuration\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'Define layer key for initializing layer with different configurations.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"init_cfg  $=$  [dict(type  $=$  'Constant', layer  $=$  'Conv1d', val  $= 1$  - dict(type  $=$  'Constant', layer  $=$  'Conv2d', val  $= 2$  - dict(type  $=$  'Constant', layer  $=$  'Linear', val  $= 3$  ] # nn.Conv1d will be initialized with dict(type  $=$  'Constant', val  $= 1$  # nn.Conv2d will be initialized with dict(type  $=$  'Constant', val  $= 2$  # nn.Linear will be initialized with dict(type  $=$  'Constant', val  $= 3$\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': '1. Initialize model by override key',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'When initializing some specific part with its attribute name, we can use override key, and the value in override will ignore the value in init_cfg.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"layers # self.feat  $=$  nn.Conv1d(3, 1, 3) # self.reg  $=$  nn.Conv2d(3, 3, 3) # self.cls  $=$  nn.Linear(1,2) init_cfg  $=$  dict(type  $=$  'Constant', layer  $=$  ['Conv1d','Conv2d'], val  $= 1$  , bias  $= 2$  override  $=$  dict(type  $=$  'Constant', name  $=$  'reg', val  $= 3$  , bias  $= 4$  )) # self.feat and self.cls will be initialized with dict(type  $=$  'Constant', val  $= 1$ $\\\\rightarrow$  bias  $= 2$  # The module called 'reg' will be initialized with dict(type  $=$  'Constant', val  $= 3$  , bias  $= 4$\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': 'If layer is None in init_cfg, only sub- module with the name in override will be initialized, and type and other args in override can be omitted.',\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"layers # self.feat  $=$  nn.Conv1d(3, 1, 3) # self.reg  $=$  nn.Conv2d(3, 3, 3) # self.cls  $=$  nn.Linear(1,2) init_cfg  $=$  dict(type  $=$  'Constant', val  $= 1$  , bias  $= 2$  override  $=$  dict(name  $=$  'reg')) # self.feat and self.cls will be initialized by Pytorch # The module called 'reg' will be initialized with dict(type  $=$  Constant', val  $= 1$  , bias  $= 2$\",\n",
       "  'page_idx': 114},\n",
       " {'type': 'text',\n",
       "  'text': \"If we don't define layer key or override key, it will not initialize anything. Invalid usage\",\n",
       "  'page_idx': 114},\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed21f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMDetectionRelease 2.18.0\n",
      "MMDetection Authors\n",
      "1 Prerequisites 1\n",
      "2 Installation 3\n",
      "3 Verification 7\n",
      "4 Benchmark and Model Zoo 9\n",
      "5 1: Inference and train with existing models and standard datasets 17\n",
      "6 2: Train with customized datasets 29\n",
      "7 3: Train with customized models and standard datasets 35\n",
      "8 Tutorial 1: Learn about Configs 41\n",
      "9 Tutorial 2: Customize Datasets 55\n",
      "10 Tutorial 3: Customize Data Pipelines 67\n",
      "11 Tutorial 4: Customize Models 71\n",
      "12 Tutorial 5: Customize Runtime Settings 79\n",
      "13 Tutorial 6: Customize Losses 87\n",
      "14 Tutorial 7: Finetuning Models 91\n",
      "15 Tutorial 8: Pytorch to ONNX (Experimental) 95\n",
      "16 Tutorial 9: ONNX to TensorRT (Experimental) 101\n",
      "17 Tutorial 10: Weight initialization 105\n",
      "32 Changelog 141\n",
      "33 Frequently Asked Questions 173\n",
      "34 English 177\n",
      "35 179\n",
      "36 mmdet.apis 181\n",
      "37 mmdet.core 183\n",
      "PREREQUISITES\n",
      "MMDetection, Release 2.18.0\n",
      "INSTALLATION\n",
      "2.1 Prepare environment\n",
      "2.2 Install MMDetection\n",
      "Note:\n",
      "2.3 Install without GPU support\n",
      "2.4 Another option: Docker Image\n",
      "2.5 A from-scratch setup script\n",
      "2.6 Developing with multiple MMDetection versions\n",
      "VERIFICATION\n",
      "MMDetection, Release 2.18.0\n",
      "BENCHMARK AND MODEL ZOO\n",
      "4.1 Mirror sites\n",
      "4.2 Common settings\n",
      "4.3 ImageNet Pretrained Models\n",
      "4.4 Baselines\n",
      "4.4.1 RPN\n",
      "4.4.2 Faster R-CNN\n",
      "4.4.3 Mask R-CNN\n",
      "4.4.4 Fast R-CNN (with pre-computed proposals)\n",
      "4.4.5 RetinaNet\n",
      "4.4.6 Cascade R-CNN and Cascade Mask R-CNN\n",
      "4.4.7 Hybrid Task Cascade (HTC)\n",
      "4.4.8 SSD\n",
      "4.4.9 Group Normalization (GN)\n",
      "4.4.10 Weight Standardization\n",
      "4.4.11 Deformable Convolution v2\n",
      "4.4.12 CARAFE: Content-Aware ReAssembly of FEatures\n",
      "4.4.13 Instaboost\n",
      "4.4.14 Libra R-CNN\n",
      "4.4.15 Guided Anchoring\n",
      "4.4.16 FCOS\n",
      "4.4.17 FoveaBox\n",
      "4.4.18 RepPoints\n",
      "4.4.19 FreeAnchor\n",
      "4.4.20 Grid R-CNN (plus)\n",
      "4.4.21 GHM\n",
      "4.4.22 GCNet\n",
      "4.4.23 HRNet\n",
      "4.4.24 Mask Scoring R-CNN\n",
      "4.4.25 Train from Scratch\n",
      "4.4.26 NAS-FPN\n",
      "4.4.27 ATSS\n",
      "4.4.28 FSAF\n",
      "4.4.29 RegNetX\n",
      "4.4.30 Res2Net\n",
      "4.4.31 GRoIE\n",
      "4.4.32 Dynamic R-CNN\n",
      "4.4.33 PointRend\n",
      "4.4.34 DetectorRS\n",
      "4.4.35 Generalized Focal Loss\n",
      "4.4.36 CornerNet\n",
      "4.4.37 YOLOv3\n",
      "4.4.38 PAA\n",
      "4.4.39 SABL\n",
      "4.4.40 CentripetalNet\n",
      "4.4.41 ResNeSt\n",
      "4.4.42 DETR\n",
      "4.4.43 Deformable DETR\n",
      "4.4.44 AutoAssign\n",
      "4.4.45 YOLOF\n",
      "4.4.46 Seesaw Loss\n",
      "4.4.47 CenterNet\n",
      "4.4.48 YOLOX\n",
      "4.4.49 PVT\n",
      "4.4.50 SOLO\n",
      "4.4.51 QueryInst\n",
      "4.4.52 Other datasets\n",
      "4.4.53 Pre-trained Models\n",
      "4.5 Speed benchmark\n",
      "4.5.1 Training Speed benchmark\n",
      "4.5.2 Inference Speed Benchmark\n",
      "4.6 Comparison with Detector2\n",
      "4.6.1 Hardware\n",
      "4.6.2 Software environment\n",
      "4.6.3 Performance\n",
      "4.6.4 Training Speed\n",
      "4.6.5 Inference Speed\n",
      "4.6.6 Training memory\n",
      "1: INFERENCE AND TRAIN WITH EXISTING MODELS AND STANDARD DATASETS\n",
      "5.1 Inference with existing models\n",
      "5.1.1 High-level APIs for inference\n",
      "5.1.2 Asynchronous interface - supported for Python 3.7+\n",
      "5.1.3 Demos\n",
      "Image demo\n",
      "Webcam demo\n",
      "Video demo\n",
      "5.2 Test existing models on standard datasets\n",
      "5.2.1 Prepare datasets\n",
      "5.2.2 Test existing models\n",
      "5.2.3 Examples\n",
      "5.2. Test existing models on standard datasets\n",
      "5.2.4 Test without Ground Truth Annotations\n",
      "5.2.5 Batch Inference\n",
      "5.2.6 Deprecated ImageToTensor\n",
      "5.3 Train predefined models on standard datasets\n",
      "5.3.1 Prepare datasets\n",
      "5.3.2 Training on a single GPU\n",
      "Note:\n",
      "5.3.3 Training on multiple GPUs\n",
      "Launch multiple jobs simultaneously\n",
      "5.3.4 Training on multiple nodes\n",
      "5.3.5 Manage jobs with Slurm\n",
      "2: TRAIN WITH CUSTOMIZED DATASETS\n",
      "6.1 Prepare the customized dataset\n",
      "6.1.1 COCO annotation format\n",
      "6.2 Prepare a config\n",
      "6.3 Train a new model\n",
      "6.4 Test and inference\n",
      "3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS\n",
      "7.1 Prepare the standard dataset\n",
      "7.2 Prepare your own customized model\n",
      "7.2.1 1. Define a new neck (e.g. AugFPN)\n",
      "7.2.2 2. Import the module\n",
      "7.2.3 3. Modify the config file\n",
      "7.3 Prepare a config\n",
      "7.4 Train a new model\n",
      "7.5 Test and inference\n",
      "TUTORIAL 1: LEARN ABOUT CONFIGS\n",
      "8.1 Modify config through script arguments\n",
      "8.2 Config File Structure\n",
      "8.3 Config Name Style\n",
      "8.4 Deprecated train_cfg/test_cfg\n",
      "8.5 An Example of Mask R-CNN\n",
      "8.6 FAQ\n",
      "8.6.1 Ignore some fields in the base configs\n",
      "8.6.2 Use intermediate variables in configs\n",
      "TUTORIAL 2: CUSTOMIZE DATASETS\n",
      "9.1 Support new data format\n",
      "9.1.1 Reorganize new data formats to existing format\n",
      "1. Modify the config file for using the customized dataset\n",
      "2. Check the annotations of the customized dataset\n",
      "Note\n",
      "9.1.2 Reorganize new data format to middle format\n",
      "9.1.3 An example of customized dataset\n",
      "9.2 Customize datasets by dataset wrappers\n",
      "9.2.1 Repeat dataset\n",
      "9.2.2 Class balanced dataset\n",
      "9.2.3 Concatenate dataset\n",
      "9.2. Customize datasets by dataset wrappers\n",
      "Note:\n",
      "9.3 Modify Dataset Classes\n",
      "Note:\n",
      "9.4 COCO Panoptic Dataset\n",
      "Chapter 9. Tutorial 2: Customize Datasets\n",
      "TUTORIAL 3: CUSTOMIZE DATA PIPELINES\n",
      "10.1 Design of Data pipelines\n",
      "10.1.1 Data loading\n",
      "10.1.2 Pre-processing\n",
      "10.1.3 Formatting\n",
      "ToTensor\n",
      "Transpose\n",
      "DefaultFormatBundle\n",
      "10.1.4 Test time augmentation\n",
      "10.2 Extend and use custom pipelines\n",
      "TUTORIAL 4: CUSTOMIZE MODELS\n",
      "11.1 Develop new components\n",
      "11.1.1 Add a new backbone\n",
      "1. Define a new backbone (e.g. MobileNet)\n",
      "2. Import the module\n",
      "3. Use the backbone in your config file\n",
      "11.1.2 Add new necks\n",
      "1. Define a neck (e.g. PAFPN)\n",
      "2. Import the module\n",
      "3. Modify the config file\n",
      "11.1.3 Add new heads\n",
      "11.1.4 Add new loss\n",
      "TUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS\n",
      "12.1 Customize optimization settings\n",
      "12.1.1 Customize optimizer supported by Pytorch\n",
      "12.1.2 Customize self-implemented optimizer\n",
      "1. Define a new optimizer\n",
      "2. Add the optimizer to registry\n",
      "3. Specify the optimizer in the config file\n",
      "12.1.3 Customize optimizer constructor\n",
      "12.1.4 Additional settings\n",
      "12.2 Customize training schedules\n",
      "12.3 Customize workflow\n",
      "Note:\n",
      "12.4 Customize hooks\n",
      "12.4.1 Customize self-implemented hooks\n",
      "1. Implement a new hook\n",
      "2. Register the new hook\n",
      "3. Modify the config\n",
      "12.4.2 Use hooks implemented in MMCV\n",
      "4. Example: NumClassCheckHook\n",
      "12.4.3 Modify default runtime hooks\n",
      "Checkpoint config\n",
      "Log config\n",
      "Evaluation config\n",
      "Chapter 12. Tutorial 5: Customize Runtime Settings\n",
      "TUTORIAL 6: CUSTOMIZE LOSSES\n",
      "13.1 Computation pipeline of a loss\n",
      "13.2 Set sampling method (step 1)\n",
      "13.3 Tweaking loss\n",
      "13.3.1 Tweaking hyper-parameters (step 2)\n",
      "13.3.2 Tweaking the way of reduction (step 3)\n",
      "13.3.3 Tweaking loss weight (step 5)\n",
      "13.4 Weighting loss (step 3)\n",
      "TUTORIAL 7: FINETUNING MODELS\n",
      "14.1 Inherit base configs\n",
      "14.2 Modify head\n",
      "14.3 Modify dataset\n",
      "14.4 Modify training schedule\n",
      "14.5 Use pre-trained model\n",
      "Chapter 14. Tutorial 7: Finetuning Models\n",
      "TUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL)\n",
      "15.1 How to convert models from Pytorch to ONNX\n",
      "15.1.1 Prerequisite\n",
      "15.1.2 Usage\n",
      "15.1.3 Description of all arguments\n",
      "15.2 How to evaluate the exported models\n",
      "15.2.1 Prerequisite\n",
      "15.2.2 Usage\n",
      "15.2.3 Description of all arguments\n",
      "15.2.4 Results and Models\n",
      "15.3 List of supported models exportable to ONNX\n",
      "15.4 The Parameters of Non-Maximum Suppression in ONNX Export\n",
      "15.5 Reminders\n",
      "15.6 FAQs\n",
      "MMDetection, Release 2.18.0\n",
      "TUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL)\n",
      "16.1 How to convert models from ONNX to TensorRT\n",
      "16.1.1 Prerequisite\n",
      "16.1.2 Usage\n",
      "16.2 How to evaluate the exported models\n",
      "16.3 List of supported models convertible to TensorRT\n",
      "16.4 Reminders\n",
      "16.5 FAQs\n",
      "MMDetection, Release 2.18.0\n",
      "TUTORIAL 10: WEIGHT INITIALIZATION\n",
      "17.1 Description\n",
      "17.2 Initialize parameters\n",
      "17.3 Usage of init_cfg\n",
      "LOG ANALYSIS\n",
      "RESULT ANALYSIS\n",
      "Usage\n",
      "Examples:\n",
      "VISUALIZATION\n",
      "20.1 Visualize Datasets\n",
      "20.2 Visualize Models\n",
      "20.3 Visualize Predictions\n",
      "MMDetection, Release 2.18.0\n",
      "ERROR ANALYSIS\n",
      "MMDetection, Release 2.18.0\n",
      "MODEL SERVING\n",
      "22.1 1. Convert model from MMDetection to TorchServe\n",
      "22.2 2. Build mmdet - serve docker image\n",
      "22.3 3. Run mmdet - serve\n",
      "22.4 4. Test deployment\n",
      "MMDetection, Release 2.18.0\n",
      "MODEL COMPLEXITY\n",
      "MMDetection, Release 2.18.0\n",
      "MODEL CONVERSION\n",
      "24.1 MMDetection model to ONNX (experimental)\n",
      "24.2 MMDetection 1.x model to MMDetection 2.x\n",
      "24.3 RegNet model to MMDetection\n",
      "24.4  Detection ResNet to Pytorch\n",
      "24.5 Prepare a model for publishing\n",
      "DATASET CONVERSION\n",
      "MMDetection, Release 2.18.0\n",
      "BENCHMARK\n",
      "26.1 Robust Detection Benchmark\n",
      "26.2 FPS Benchmark\n",
      "MMDetection, Release 2.18.0\n",
      "MISCELLANEOUS\n",
      "27.1 Evaluating a metric\n",
      "27.2 Print the entire config\n",
      "MMDetection, Release 2.18.0\n",
      "HYPER-PARAMETER OPTIMIZATION\n",
      "28.1 YOLO Anchor Optimization\n",
      "CONVENTIONS\n",
      "29.1 Loss\n",
      "29.2 Empty Proposals\n",
      "29.3 Coco Panoptic Dataset\n",
      "COMPATIBILITY OF MMDETECTION 2.X\n",
      "30.1 MMDetection 2.18.0\n",
      "30.1.1 DllHead compatibility\n",
      "30.2 MMDetection 2.14.0\n",
      "30.2.1 MMCV Version\n",
      "30.2.2 SSD compatibility\n",
      "30.3 MMDetection 2.12.0\n",
      "30.3.1 MMCV Version\n",
      "30.3.2 Unified model initialization\n",
      "30.3.3 Unified model registry\n",
      "30.3.4 Mask AP evaluation\n",
      "30.4 Compatibility with MMDetection 1.x\n",
      "30.4.1 Coordinate System\n",
      "30.4.2 Codebase Conventions\n",
      "30.4.3 Training Hyperparameters\n",
      "30.4.4 Upgrade Models from 1.x to 2.0\n",
      "30.5 pycocotools compatibility\n",
      "PROJECTS BASED ON MMDETECTION\n",
      "31.1 Projects as an extension\n",
      "31.2 Projects of papers\n",
      "CHANGELOG\n",
      "32.1 v2.18.0 (27/10/2021)\n",
      "32.1.1 Highlights\n",
      "32.1.2 New Features\n",
      "32.1.3 Bug Fixes\n",
      "32.1.4 Improvements\n",
      "32.1.5 Refactors\n",
      "32.1.6 Contributors\n",
      "32.2 v2.17.0 (28/9/2021)\n",
      "32.2.1 Highlights\n",
      "32.2.2 New Features\n",
      "32.2.3 Bug Fixes\n",
      "32.2.4 Improvements\n",
      "32.2.5 Contributors\n",
      "32.3 v2.16.0 (30/8/2021)\n",
      "32.3.1 Highlights\n",
      "32.3.2 New Features\n",
      "32.3.3 Bug Fixes\n",
      "32.3.4 Improvements\n",
      "32.3.5 Contributors\n",
      "32.4 v2.15.1 (11/8/2021)\n",
      "32.4.1 Highlights\n",
      "32.4.2 New Features\n",
      "32.4.3 Bug Fixes\n",
      "32.4.4 Improvements\n",
      "32.4.5 Contributors\n",
      "32.5 v2.15.0 (02/8/2021)\n",
      "32.5.1 Highlights\n",
      "32.5.2 New Features\n",
      "32.5.3 Bug Fixes\n",
      "32.5.4 Improvements\n",
      "32.5.5 Contributors\n",
      "32.6 v2.14.0 (29/6/2021)\n",
      "32.6.1 Highlights\n",
      "32.6.2 New Features\n",
      "32.6.3 Bug Fixes\n",
      "32.6.4 Improvements\n",
      "32.7 v2.13.0 (01/6/2021)\n",
      "32.7.1 Highlights\n",
      "32.7.2 New Features\n",
      "32.7.3 Bug Fixes\n",
      "32.7.4 Improvements\n",
      "32.8 v2.12.0 (01/5/2021)\n",
      "32.8.1 Highlights\n",
      "32.8.2 Backwards Incompatible Changes\n",
      "32.8.3 New Features\n",
      "32.8.4 Improvements\n",
      "32.8.5 Bug Fixes\n",
      "32.9 v2.11.0 (01/4/2021)\n",
      "Highlights\n",
      "New Features\n",
      "Improvements\n",
      "Bug Fixes\n",
      "32.10 v2.10.0 (01/03/2021)\n",
      "32.10.1 Highlights\n",
      "32.10.2 New Features\n",
      "32.10.3 Bug Fixes\n",
      "32.10.4 Improvements\n",
      "32.11 v2.9.0 (01/02/2021)\n",
      "32.11.1 Highlights\n",
      "32.11.2 New Features\n",
      "32.11.3 Bug Fixes\n",
      "32.11.4 Improvements\n",
      "32.12 v2.8.0 (04/01/2021)\n",
      "32.12.1 Highlights\n",
      "32.12.2 New Features\n",
      "32.12.3 Bug Fixes\n",
      "32.12.4 Improvements\n",
      "32.13 v2.7.0 (30/11/2020)\n",
      "32.13.1 New Features\n",
      "32.13.2 Bug Fixes\n",
      "32.13.3 Improvements\n",
      "32.14 v2.6.0 (1/11/2020)\n",
      "32.14.1 New Features\n",
      "32.14.2 Bug Fixes\n",
      "32.14.3 Improvements\n",
      "32.15 v2.5.0 (5/10/2020)\n",
      "32.15.1 Highlights\n",
      "32.15.2 Backwards Incompatible Changes\n",
      "32.15.3 New Features\n",
      "32.15.4 Bug Fixes\n",
      "32.15.5 Improvements\n",
      "32.16 v2.4.0 (5/9/2020)\n",
      "Highlights\n",
      "Backwards Incompatible Changes\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.17 v2.3.0 (5/8/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.18 v2.2.0 (1/7/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.19 v2.1.0 (8/6/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "New Features\n",
      "Improvements\n",
      "32.20 v2.0.0 (6/5/2020)\n",
      "Improvements\n",
      "Bug Fixes\n",
      "New Features\n",
      "32.21 v1.1.0 (24/2/2020)\n",
      "Highlights\n",
      "Breaking Changes\n",
      "Bug Fixes\n",
      "Improvements\n",
      "New Features\n",
      "32.22 v1.0.0 (30/1/2020)\n",
      "Highlights\n",
      "Bug Fixes\n",
      "Improvements\n",
      "New Features\n",
      "32.23 v1.0rc1 (13/12/2019)\n",
      "Highlights\n",
      "Breaking Changes\n",
      "Bug Fixes\n",
      "Improvements\n",
      "New Features\n",
      "32.24 v1.0rc0 (27/07/2019)\n",
      "32.25 v0.6.0 (14/04/2019)\n",
      "32.26 v0.6rc0(06/02/2019)\n",
      "32.27 v0.5.7 (06/02/2019)\n",
      "32.28 v0.5.6 (17/01/2019)\n",
      "32.29 v0.5.5 (22/12/2018)\n",
      "32.30 v0.5.4 (27/11/2018)\n",
      "32.31 v0.5.3 (26/11/2018)\n",
      "32.32 v0.5.2 (21/10/2018)\n",
      "32.33 v0.5.1 (20/10/2018)\n",
      "FREQUENTLY ASKED QUESTIONS\n",
      "33.1 MMCV Installation\n",
      "33.2 PyTorch/CUDA Environment\n",
      "33.3 Training\n",
      "33.4 Evaluation\n",
      "Chapter 33. Frequently Asked Questions\n",
      "MMDetection, Release 2.18.0\n",
      "CHAPTERTHIRTYFIVE\n",
      "MMDetection, Release 2.18.0\n",
      "MMDET.APIS\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "MMDET.CORE\n",
      "37.1 anchor\n",
      "Parameters\n",
      "Examples\n",
      "gen_base_anchors()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property num_baseanchors\n",
      "property num_base_priors\n",
      "property num_levels\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Examples\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property num_base_priors\n",
      "property num_levels\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property num_levels\n",
      "Parameters\n",
      "Parameters\n",
      "Return type torch.Tensor\n",
      "Parameters\n",
      "Parameters\n",
      "37.2 bbox\n",
      "num_gts\n",
      "gt_inds\n",
      "max_overlap\n",
      "labels\n",
      "Example\n",
      "property info\n",
      "property num_preds\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "property bboxes\n",
      "Parameters\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "static random_choice(gallery, num)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "1) is_aligned is False\n",
      "Parameters\n",
      "Return type Tensor\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "37.3 export\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Examples\n",
      "37.4 mask\n",
      "Parameters\n",
      "Parameters\n",
      "Return type BaseInstanceMasks\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "property areas\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "property areas\n",
      "References\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "37.5 evaluation\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "37.6 post_processing\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "37.7 utils\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "MMDET.DATASETS\n",
      "38.1 datasets\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "get_ann_info(idx)\n",
      "get_cat_ids(idx)\n",
      "Parameters\n",
      "xyxy2xywh(bbox)\n",
      "Parameters\n",
      "get_ann_info(idx)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "get_cat_ids(idx)\n",
      "Parameters\n",
      "Parameters\n",
      "load_annotations(ann_file)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "get_ann_info(idx)\n",
      "get_cat_ids(idx)\n",
      "Parameters\n",
      "Examples\n",
      "Return type list\n",
      "Examples\n",
      "38.2 pipelines\n",
      "Parameters\n",
      "Examples\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "get_indexes(dataset)\n",
      "Parameters\n",
      "get_indexes(dataset)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Note:\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "static random_sample(img_scales)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "38.3 samplers\n",
      "Parameters\n",
      "Parameters\n",
      "set_epoch(epoch)\n",
      "Parameters\n",
      "set_epoch(epoch)\n",
      "38.4 api_wrappers\n",
      "MMDetection, Release 2.18.0\n",
      "MMDET.MODELS\n",
      "39.1 detectors\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type dict\n",
      "property with bbox\n",
      "property with mask\n",
      "property with neck\n",
      "property with shared_head\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_dummy(img)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Return type list[np.ndarray]\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "forward_dummy(img)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "39.2 backbones\n",
      "Parameters\n",
      "Example\n",
      "forward  $(x)$\n",
      "train (mode  $\\equiv$  True)\n",
      "Parameters\n",
      "Example\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "train (mode=True)\n",
      "Parameters\n",
      "forward(x)\n",
      "init_weights()\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "make_res_layer(\\*\\*kwargs)\n",
      "Parameters\n",
      "Example\n",
      "Examples\n",
      "Parameters\n",
      "property norm1\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "forward(x)\n",
      "init_weights()\n",
      "Parameters\n",
      "39.3 necks\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "forward(feature)\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Example\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Parameters\n",
      "tensor_add(a, b)\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Parameters\n",
      "Parameters\n",
      "forward(inputs)\n",
      "Note:\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "39.4 dense heads\n",
      "forward(fears)\n",
      "Returns\n",
      "Usually a tuple of classification scores and bbox prediction\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single  $(x)$\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "additional Returns: This function enables user-defined returns from\n",
      "Return type tuple\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Decoded output of CenterNetHead, containing\n",
      "forward (feats)\n",
      "Returns\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "init_weightsC\n",
      "Parameters\n",
      "Returns\n",
      "which has components below:\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "init_weights()\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "forward(feats)\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple[Tensor]\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type dict\n",
      "init_weights()\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "forward(fears)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "forward(feats)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_single  $(x)$\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Example\n",
      "Returns\n",
      "Usually a tuple of classification scores and bbox prediction\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Return type Tensor\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "centers_to_bboxes(point_list)\n",
      "forward(feats)\n",
      "Returns\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "forward_single(x)\n",
      "Returns\n",
      "forward(feats)\n",
      "Returns\n",
      "Usually a tuple of classification scores and bbox prediction\n",
      "init_weights()\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "forward(feats)\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "forward(feats)\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type tuple\n",
      "Parameters\n",
      "property numanchors\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "forward_single(x)\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "additional Returns: This function enables user-defined returns from\n",
      "Return type tuple\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Usually returns a tuple containing learning targets.\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "property num anchors\n",
      "property num_attrb\n",
      "Parameters\n",
      "Parameters\n",
      "forward(fears)\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "init_weights()\n",
      "Parameters\n",
      "39.5 roi heads\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "property num_inputs\n",
      "Parameters\n",
      "property with bbox\n",
      "Parameters\n",
      "init_assigner sampler()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward(x)\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type Tuple[Tensor]\n",
      "init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "update_hyperparameters()\n",
      "forward(x)\n",
      "Parameters\n",
      "Returns\n",
      "Example\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "forward(fears)\n",
      "Parameters\n",
      "Parameters\n",
      "calc_sub_regions()\n",
      "forward(x)\n",
      "Parameters\n",
      "Parameters\n",
      "property with semantic\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Return type Tensor\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "forward(x)\n",
      "train (mode  $\\equiv$  True)\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "property with_feat_relay\n",
      "property with_glbctx\n",
      "property with semantic\n",
      "Parameters\n",
      "Parameters\n",
      "Queries\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "39.6 losses\n",
      "Parameters\n",
      "Return type tuple[float]\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "Parameters\n",
      "Parameters\n",
      "Example\n",
      "39.7 utils\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "forward(mask)\n",
      "Returns\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "forward  $(x)$\n",
      "Parameters\n",
      "forward(mask)\n",
      "Returns\n",
      "Parameters\n",
      "Parameters\n",
      "Returns\n",
      "Init_weights()\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "Parameters\n",
      "MMDetection, Release 2.18.0\n",
      "INDICES AND TABLES\n",
      "MMDetection, Release 2.18.0\n",
      "m\n",
      "MMDetection, Release 2.18.0\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "N\n",
      "O\n",
      "P\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "for i in content_list:\n",
    "    if \"text_level\" in i.keys():\n",
    "        print(i[\"text\"])\n",
    "        k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe9994d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_content_list_files(root_dir):\n",
    "    matched_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('content_list.json'):\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "                matched_files.append(full_path)\n",
    "    return matched_files\n",
    "\n",
    "# \n",
    "root_directory = '/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/'  # \n",
    "files = find_content_list_files(root_directory)\n",
    "\n",
    "levels = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        file_contents = json.load(f)\n",
    "    cnt = 0\n",
    "    for i in file_contents:\n",
    "        if \"text_level\" in i.keys():\n",
    "            cnt += 1\n",
    "    levels.append(cnt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51ec1392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/AMAZON_2017_10K/58085999-29c2-4982-b4b5-a57f4d7c126b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/AMAZON_2017_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Campaign_038_Introducing_AC_Whitepaper_v5e/2e5a21db-5439-4ff1-9d0d-0b08f857e1b3_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Campaign_038_Introducing_AC_Whitepaper_v5e/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2018.11.19_algorithms_FINAL/e49f25d9-4974-4db8-900a-c8a867132147_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2018.11.19_algorithms_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.05.21_International-Cooperation-COVID_FINAL/2da63574-5d44-4f01-b832-d7dfa16d5a9b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.05.21_International-Cooperation-COVID_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8e7c4cb542ad160f80fb3d795ada35d8/c2d6620a-43e0-4533-89e6-1bcb21ac5a11_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8e7c4cb542ad160f80fb3d795ada35d8/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/User_Manual_1500S_Classic_EN/a7daf174-a1cf-41ed-b0b2-fdeff1df7e62_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/User_Manual_1500S_Classic_EN/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/caltraincapacitymountainview1-150701205750-lva1-app6891_95/55448177-0815-48b5-8397-fe9b5d470122_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/caltraincapacitymountainview1-150701205750-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8dfc21ec151fb9d3578fc32d5c4e5df9/6029ae2a-169b-45dd-8abb-868029936746_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/8dfc21ec151fb9d3578fc32d5c4e5df9/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0b85477387a9d0cc33fca0f4becaa0e5/b3fb76ef-a972-476d-9592-82ec584f2028_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0b85477387a9d0cc33fca0f4becaa0e5/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2019.01.17_Trump-economy_FINAL2/e6dccc4c-65a8-454d-bd2e-c22eb9876f1c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2019.01.17_Trump-economy_FINAL2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2005.12872v3/9377366f-07c5-4015-bf22-2aedc8d96984_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2005.12872v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PS_2018.01.09_STEM_FINAL/bff688ef-a07c-4781-8fe5-629be36288ba_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PS_2018.01.09_STEM_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/csewt7zsecmmbzjufbyx-signature-24d91a254426c21c3079384270e1f138dc43a271cfe15d6d520d68205855b2a3-poli-150306115347-conversion-gate01_95/a20b1251-0b0f-4847-a4bc-c6ce3b5b6a41_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/csewt7zsecmmbzjufbyx-signature-24d91a254426c21c3079384270e1f138dc43a271cfe15d6d520d68205855b2a3-poli-150306115347-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.10997v5/742531ca-2e7d-4195-ae36-228b275a6980_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.10997v5/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/52b3137455e7ca4df65021a200aef724/b6e174e0-de64-4f83-b13b-b6950e88000c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/52b3137455e7ca4df65021a200aef724/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/asdaaburson-marstellerarabyouthsurvey2014-140407100615-phpapp01_95/79f704e9-846e-43f7-a3ba-ca8a734bd86e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/asdaaburson-marstellerarabyouthsurvey2014-140407100615-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PRE_2022.09.29_NSL-politics_REPORT/0c6d39af-c6f8-416e-966a-d3ddd5dd5fb7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PRE_2022.09.29_NSL-politics_REPORT/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2309.17421v2/804f46ab-97e3-4f52-9567-8a6a73313434_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2309.17421v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/competitiveoutcomes-091006065143-phpapp01_95/eedd8a81-6eba-4d1c-9e88-57e4db0b4014_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/competitiveoutcomes-091006065143-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2024.ug.eprospectus/4ebae30b-a7a3-4fa0-bd31-bda013b6e912_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2024.ug.eprospectus/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e639029d16094ea71d964e2fb953952b/30eaa1bc-2695-470a-b6e2-de5c9938ee5b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e639029d16094ea71d964e2fb953952b/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/STEPBACK/37f3741e-1be1-4664-8c00-b3e55a4b841c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/STEPBACK/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/GPL-Graduate-Studies-Professional-Learning-Brochure-Jul-2021/e7b0e602-e500-40dd-b834-f7296b727401_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/GPL-Graduate-Studies-Professional-Learning-Brochure-Jul-2021/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PIP_Seniors-and-Tech-Use_040314/0da1d022-feba-4bf1-ab4c-86bf376cd03a_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PIP_Seniors-and-Tech-Use_040314/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/camry_ebrochure/64af210a-f6d7-47dd-8e30-1a8b2ec77c7f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/camry_ebrochure/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bariumswallowpresentation-090810084400-phpapp01_95/585d3cb5-0bb0-43bf-9ccc-c65da7e2d091_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bariumswallowpresentation-090810084400-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2307.09288v2/eb7c3bf2-bfd6-439b-98fe-df3a3fda2623_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2307.09288v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2021.03.04_US-Views-on-China_FINAL/8494bee5-9a7f-4c33-9839-3e4983a04acc_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2021.03.04_US-Views-on-China_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3276a5b991c49cf5f9a4af0f7d6fce67/315386f2-4c32-4e27-bd57-d03083326c2c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3276a5b991c49cf5f9a4af0f7d6fce67/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NETFLIX_2015_10K/5d63a4c0-c5ed-41be-a2c6-e6b39d6a2208_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NETFLIX_2015_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_20.07.30_U.S.-Views-China_final/65252ada-6ab7-4091-af0a-b10e4d8769ea_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_20.07.30_U.S.-Views-China_final/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/chapter8-geneticscompatibilitymode-141214140247-conversion-gate02_95/583aa212-63f7-48b2-8bba-c1ea4d0af92f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/chapter8-geneticscompatibilitymode-141214140247-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.05039v2/54111238-c55d-4722-9966-73cdacbf30d1_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.05039v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ISEP_student_handbook_2020/e97d657f-769d-4b03-b425-4e0dfcd86365_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ISEP_student_handbook_2020/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SnapNTell/62e5eea8-266e-4d16-b6be-5a4857bc7887_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SnapNTell/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2017.10.04_Automation_FINAL/f7b6fcd5-3d26-4657-b727-37b3c76b8ea6_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PI_2017.10.04_Automation_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NYU_graduate/eba5c529-2717-4867-b052-881813eb195f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NYU_graduate/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/amb-siteaudits-ds15-150204174043-conversion-gate01_95/d0e52b53-e531-4502-a8e6-5ecd5aa14a54_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/amb-siteaudits-ds15-150204174043-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Macbook_air/8929f4e0-cb45-4239-b21e-ec25c00b938c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Macbook_air/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/91521110100M_4K_UHD_Display_User_Manual_V1.1/c6d92953-b8e3-4054-b8ed-21649f5a53b9_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/91521110100M_4K_UHD_Display_User_Manual_V1.1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Guide-for-international-students-web/b4fd9169-8008-4ae4-8397-92a5a7a69d2c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Guide-for-international-students-web/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PH_2016.06.08_Economy-Final/422a2be9-22c5-4fea-ac2d-3ceafa6a2e02_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PH_2016.06.08_Economy-Final/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-Business-School-BBA-Brochure-2024/5c38912d-f849-4d4a-ac2c-09a423beb22f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-Business-School-BBA-Brochure-2024/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-FASS-Graduate-Guidebook-2021-small/de59cea2-a938-4a42-b56d-0ab38ff7d08d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NUS-FASS-Graduate-Guidebook-2021-small/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/formwork-150318073913-conversion-gate01_95/36504667-22ab-428c-b119-e1db9abb8859_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/formwork-150318073913-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0e94b4197b10096b1f4c699701570fbf/37a81f14-0e13-4b71-8756-3ca9b8df408c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/0e94b4197b10096b1f4c699701570fbf/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e79deb02a0c0e87511080836c5d4347b/fcad443f-7621-4d74-adb9-5bdbf688c617_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/e79deb02a0c0e87511080836c5d4347b/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/germanwingsdigitalcrisisanalysis-150403064828-conversion-gate01_95/9b50dc40-86fc-4685-80e6-4a3d101ca708_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/germanwingsdigitalcrisisanalysis-150403064828-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/stereo_headset/58312fd4-3d1c-4221-9e21-d2ced1b216f9_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/stereo_headset/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.13186v3/3c9bd192-8e4c-408e-99cf-67f0de94d847_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.13186v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2021-Apple-Catalog/0574e0f3-c4d1-4915-9e37-9d796f5ae6cf_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2021-Apple-Catalog/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/c31e6580d0175ab3f9d99d1ff0bfa000/dc70ed02-fb51-4a6a-be90-67ceadcf01c7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/c31e6580d0175ab3f9d99d1ff0bfa000/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/honor_watch_gs_pro/fe2437b6-63cd-4b26-96ad-23c194e19f52_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/honor_watch_gs_pro/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/reportq32015-151009093138-lva1-app6891_95/cef24e0b-595b-41ea-884f-d260855e7c5b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/reportq32015-151009093138-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f1f5242528411b262be447e61e2eb10f/263ee5dd-c342-4ec2-a93b-41957b88559e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f1f5242528411b262be447e61e2eb10f/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.05634v2/e2082d53-f2db-4b0c-8ad2-27609f324786_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.05634v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2020.08.06_COVID-19-Restrictions_FINAL-1/c5cf9f1f-c3d5-4a3a-9209-7d923e9bd35b_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2020.08.06_COVID-19-Restrictions_FINAL-1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efd88e41c5f2606c57929cac6c1c0605/9433f5e8-b300-4514-817a-12d3fc71530d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efd88e41c5f2606c57929cac6c1c0605/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/tacl_a_00660/77502d71-f2e5-4d8a-ab6b-49cd9192a444_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/tacl_a_00660/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BESTBUY_2023_10K/7d121574-9b24-43eb-a817-50b325143bcd_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BESTBUY_2023_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/avalaunchpresentationsthatkickasteriskv3copy-150318114804-conversion-gate01_95/d3b0e34d-6452-4473-bee6-f1b2420329a5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/avalaunchpresentationsthatkickasteriskv3copy-150318114804-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/DSA-278777/696e43d2-cee4-4c75-ba60-24d7789bdcfb_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/DSA-278777/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ddoseattle-150627210357-lva1-app6891_95/35e51de7-ae1d-4a89-91c5-0aa0d6568d44_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ddoseattle-150627210357-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/b3m5kaeqm2w8n4bwcesw-140602121350-phpapp02_95/f935003e-4934-4179-8a7f-df9df5176238_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/b3m5kaeqm2w8n4bwcesw-140602121350-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/RAR/31d98fe0-9e13-40a3-baea-6304df70092c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/RAR/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2401.18059v1/c26eabe0-e483-47ad-adc2-4fcd8589e27c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2401.18059v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Sinopolis-Chengdu/a791013f-2046-4c6b-ae37-ed8ce9355e19_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Sinopolis-Chengdu/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Bergen-Brochure-en-2022-23/d2c37a73-24ea-434e-8b6d-76212b766baf_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Bergen-Brochure-en-2022-23/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a4f3ced0696009fec3179f493e4f28c4/28275dac-4928-4be4-912e-1ee77ee0736d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a4f3ced0696009fec3179f493e4f28c4/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a5879805d70c854ea4361e43a84e3bb2/99c381de-e60d-4f54-a3a2-d59749b91ed6_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/a5879805d70c854ea4361e43a84e3bb2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SAO-StudentSupport_Guidebook-Content/9941e983-2314-43e9-bfbe-af3ea8b01319_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/SAO-StudentSupport_Guidebook-Content/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/watch_d/e4528add-49b6-45f5-a1e4-fbdd3c0f916c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/watch_d/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3M_2018_10K/5db1be9e-ec5f-4217-947d-392dc60d01ef_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/3M_2018_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fd76bbefe469561966e5387aa709c482/29145350-94b8-416d-a1c1-2791445d1996_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fd76bbefe469561966e5387aa709c482/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2311.16502v3/c42296f7-09ae-4e88-a6c7-54e99433401f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2311.16502v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.09158v1/ed408fac-1abf-42d0-9d59-74b2bee6e993_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.09158v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.acl-long.386/601af515-dbb3-473a-b8ca-e63b28eef1a7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.acl-long.386/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nielsen2015musicbizpresentation-final-150526143534-lva1-app6891_95/8ad100e9-0005-46c0-b7e1-67786dfb0878_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nielsen2015musicbizpresentation-final-150526143534-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ACTIVISIONBLIZZARD_2019_10K/b5aaecc2-7d18-42cd-8a69-ce936349b690_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ACTIVISIONBLIZZARD_2019_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/san-francisco-11-contents/066b86f3-d98c-44b2-95d6-7fc2b25e6dc5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/san-francisco-11-contents/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/obs-productdesc-en/0b2545a7-f535-42f9-afee-60d706f3f88d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/obs-productdesc-en/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.07609v1/84043656-0c89-445d-9f3b-dd4f1e919d4f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2310.07609v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/indonesiamobilemarketresearch-ag-150106055934-conversion-gate02_95/5ae2d295-cef0-47e5-b945-88c1c4259a55_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/indonesiamobilemarketresearch-ag-150106055934-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/afe620b9beac86c1027b96d31d396407/4256f2ea-811e-4dae-a9e6-3c09c716d762_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/afe620b9beac86c1027b96d31d396407/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2021.04.22_voting-access_REPORT/064f9c6d-c6e1-4fb2-95a8-1ebb9c27c56e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PP_2021.04.22_voting-access_REPORT/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/edb88a99670417f64a6b719646aed326/19ef3a03-8232-48aa-b74c-a83b25f5cdfd_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/edb88a99670417f64a6b719646aed326/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/guojixueshengshenghuozhinanyingwen9.1/e297d614-ac0d-417b-b282-f871e8eb6e08_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/guojixueshengshenghuozhinanyingwen9.1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BRO-GL-MMONEY/d2ce48ed-e8c8-4ad0-abe9-64114eda1ab5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/BRO-GL-MMONEY/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.08559v2/486b4b41-05f3-442e-af37-dfe0789c73b8_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2303.08559v2/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/379f44022bb27aa53efd5d322c7b57bf/9777fdaf-b74e-457b-a34f-ce7e3e4b9c13_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/379f44022bb27aa53efd5d322c7b57bf/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bigdatatrends-120723191058-phpapp02_95/bd1209cb-a372-48c5-b83c-3ae7a2ba0947_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bigdatatrends-120723191058-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/digitalmeasurementframework22feb2011v6novideo-110221233835-phpapp01_95/4f3d86b9-4b54-414a-af40-ce1c873eac5d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/digitalmeasurementframework22feb2011v6novideo-110221233835-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalmediafindingspdf-141228031149-conversion-gate02_95/9f34cb69-f10d-4d76-ad7f-5aaf5d75516f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalmediafindingspdf-141228031149-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2306.05425v1/bb7a03da-8c0a-4df9-894a-c23924d572b4_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2306.05425v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/11-21-16-Updated-Post-Election-Release/65a37edf-8108-4f93-984b-c8e884292cc3_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/11-21-16-Updated-Post-Election-Release/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/dr-vorapptchapter1emissionsources-121120210508-phpapp02_95/4f3d86b9-4b54-414a-af40-ce1c873eac5d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/dr-vorapptchapter1emissionsources-121120210508-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nova_y70/ef6248ae-cd3b-4680-927f-5c8cbce005fb_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/nova_y70/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/698bba535087fa9a7f9009e172a7f763/bb318d38-ec2b-4bb0-89f1-1858ff3f4068_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/698bba535087fa9a7f9009e172a7f763/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/welcome-to-nus/d9c91204-2622-44ff-9fc8-62ac47657a83_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/welcome-to-nus/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Independents-Report/4aa85629-e267-4190-a22a-c81f6f48d4ff_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Independents-Report/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efis-140411041451-phpapp01_95/9067c1e1-739c-46a0-9290-2066bedb2322_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/efis-140411041451-phpapp01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earlybird-110722143746-phpapp02_95/3ce1c5b3-7ac7-40cc-9e7d-ec53dd86fb98_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earlybird-110722143746-phpapp02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2210.02442v1/9b97ca7b-3b33-4586-9d63-04eed6757b8e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2210.02442v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalpresentationdeck-whatwhyhowofcertificationsocial-160324220748_95/341675c9-0e3f-43ef-904a-0dc606fbaf8e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/finalpresentationdeck-whatwhyhowofcertificationsocial-160324220748_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/q1-2023-bilibili-inc-investor-presentation/11dcd0d3-ff07-4217-93d3-d88503f847cf_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/q1-2023-bilibili-inc-investor-presentation/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fdac8d1e9ef56519371df7e6532df27d/284af9cb-e633-483c-a141-9e7462c4ba97_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/fdac8d1e9ef56519371df7e6532df27d/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2405.09818v1/1995dc42-9449-40bf-8a68-dec7ace2983d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2405.09818v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/StudentSupport_Guidebook/9941e983-2314-43e9-bfbe-af3ea8b01319_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/StudentSupport_Guidebook/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/disciplined-agile-business-analysis-160218012713_95/7aa01fea-a924-4801-b340-428edc048fa6_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/disciplined-agile-business-analysis-160218012713_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Pew-Research-Center_Hispanic-Identity-Report_12.20.2017/3db7305f-2aad-47ee-97bf-e17face4960a_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/Pew-Research-Center_Hispanic-Identity-Report_12.20.2017/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/12-15-15-ISIS-and-terrorism-release-final/4437ed95-de42-4bae-9747-6d782059a59d_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/12-15-15-ISIS-and-terrorism-release-final/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.04350v3/9dab9a49-2e73-4060-9c9e-481e7652944f_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.04350v3/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/936c0e2c2e6c8e0c07c51bfaf7fd0a83/6b2cdea8-32b3-4fb3-aded-66ab9eeb26c4_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/936c0e2c2e6c8e0c07c51bfaf7fd0a83/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/owners-manual-2170416/27ccdae4-15fe-4c15-9070-dffd429a6d11_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/owners-manual-2170416/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f86d073b0d735ac873a65d906ba82758/67f18b13-3c8c-4add-8368-589bbc384e3e_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f86d073b0d735ac873a65d906ba82758/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/87a24eb7-d010-4cc6-80a7-fb87a2ce7dff_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/mmdetection-readthedocs-io-en-v2.18.0/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/COSTCO_2021_10K/cf7d7462-07d7-4b6a-a190-b9463744be05_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/COSTCO_2021_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bdf54dxa/f4a08913-6f89-438e-9d8b-f20e3b46d3c5_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/bdf54dxa/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ecommerceopportunityindia-141124010546-conversion-gate01_95/022ffa06-6936-4b08-bc2b-a4f649628ece_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ecommerceopportunityindia-141124010546-conversion-gate01_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/measuringsuccessonfacebooktwitterlinkedin-160317142140_95/89a49661-2c08-4b3c-9370-8f657aa854e4_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/measuringsuccessonfacebooktwitterlinkedin-160317142140_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/catvsdogdlpycon15se-150512122612-lva1-app6891_95/711c9d6a-a58f-4c4e-8e4d-f7bd39b236bb_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/catvsdogdlpycon15se-150512122612-lva1-app6891_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ADOBE_2015_10K/799d62d8-716c-459a-b848-32b3b69ae43c_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/ADOBE_2015_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/05-03-18-political-release/690659f1-5167-48ea-9123-67b88df35de7_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/05-03-18-political-release/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/transform-software-delivery-with-valueedge-brochure/64ceba93-a59c-437b-adb2-7cbc25eab7bc_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/transform-software-delivery-with-valueedge-brochure/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f8d3a162ab9507e021d83dd109118b60/38ff2d85-afd7-47d3-986f-401c3c9d5034_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/f8d3a162ab9507e021d83dd109118b60/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.03.09_US-Germany_FINAL/11c0f4e4-b457-475a-891b-54c88da1e550_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PG_2020.03.09_US-Germany_FINAL/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earthlinkweb-150213112111-conversion-gate02_95/075cede7-3224-4bf8-9ca3-fba1aba1e030_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/earthlinkweb-150213112111-conversion-gate02_95/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.findings-emnlp.248/8606b0dc-dc7f-4eb3-b6f6-3354140a4330_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2023.findings-emnlp.248/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/7c3f6204b3241f142f0f8eb8e1fefe7a/7bc5bbe9-12f7-4e0e-9cd2-3188b43cbc48_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/7c3f6204b3241f142f0f8eb8e1fefe7a/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NIKE_2021_10K/02bb4fa5-bb66-423b-bb4a-a1deeabcacd8_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/NIKE_2021_10K/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.14160v4/37a440f1-f997-49b1-87ae-267a190ac0a8_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2305.14160v4/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.09390v1/d21a647f-6acf-4176-993e-4a07a9f2b4fd_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/2312.09390v1/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/t480_ug_en/c19e962d-aaf0-47b4-b56f-7d14e60107c0_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/t480_ug_en/content_list.json\n",
      " Renamed: /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PWC_opportunity_of_lifetime/f66d5194-207c-45c6-82ab-d77807801dbc_content_list.json -> /Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB/PWC_opportunity_of_lifetime/content_list.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def rename_content_list_files(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('_content_list.json'):\n",
    "                old_path = os.path.join(dirpath, filename)\n",
    "                new_path = os.path.join(dirpath, 'content_list.json')\n",
    "\n",
    "                #  content_list.json\n",
    "                if os.path.exists(new_path):\n",
    "                    print(f\"  Skipped (already exists): {new_path}\")\n",
    "                    continue\n",
    "\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\" Renamed: {old_path} -> {new_path}\")\n",
    "\n",
    "# \n",
    "root_directory = '/Users/ymxu/data/users/yiming/dtagent/MinerU_MMLB'  #  \n",
    "rename_content_list_files(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc61a561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('/data/users/yiming/dtagent/MinerU_MMLB/SAO-StudentSupport_Guidebook-Content/layout.json', 'r', encoding='utf-8') as f:\n",
    "    layout = json.load(f)\n",
    "\n",
    "with open('/data/users/yiming/dtagent/MinerU_MMLB/SAO-StudentSupport_Guidebook-Content/content_list.json', 'r', encoding='utf-8') as f:\n",
    "    content_list = json.load(f)\n",
    "\n",
    "layout_info = dict()\n",
    "def recursive_search(obj, path=\"\"):\n",
    "    \"\"\"Recursively search for image_path keys in nested structures\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            current_path = f\"{path}.{key}\" if path else key\n",
    "            if key == \"image_path\" and isinstance(value, str):\n",
    "                layout_info[value] = obj[\"bbox\"]\n",
    "                # print(f\"Found image_path at {current_path}: {value}\")\n",
    "            else:\n",
    "                recursive_search(value, current_path)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            current_path = f\"{path}[{i}]\"\n",
    "            recursive_search(item, current_path)\n",
    "\n",
    "recursive_search(layout['pdf_info'])\n",
    "\n",
    "for i in content_list:\n",
    "    if \"img_path\" in i.keys():\n",
    "        i[\"outline\"] = layout_info[i['img_path'].split('/')[1]]\n",
    "len(layout_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8940d367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'WELCOME TO NANYANG TECHNOLOGICAL UNIVERSITY (NTU)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 0},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/958556849a467f528ba6f8a6d9bad16d246d50020df21c6b91d893f5b873e965.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 0,\n",
       "  'outline': [40, 104, 491, 478]},\n",
       " {'type': 'text',\n",
       "  'text': 'We take this opportunity to welcome you to NTU and wish you a successful and happy stay here.',\n",
       "  'page_idx': 0},\n",
       " {'type': 'text',\n",
       "  'text': 'This guide has been compiled to help you through most of the formalities and procedures before and after your arrival. You will find information ranging from the important immigration regulations to student life at NTU. We have included in this guidebook the contact details of many services which are available to support you throughout your stay.',\n",
       "  'page_idx': 0},\n",
       " {'type': 'text',\n",
       "  'text': 'If you have any questions after reading through this guidebook, please contact Student Support at Student Affairs Office via email at SAOstudentsupport@ntu.edu.sg or call on us at Student Services Centre, Level 4.',\n",
       "  'page_idx': 0},\n",
       " {'type': 'text',\n",
       "  'text': 'MESSAGE FROM THE DIRECTOR OF STUDENT AFFAIRS',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 1},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a7933f887120a2f9a74ef8a56b0ea9c5cde7b14a6fbedc326f8104054db8ced4.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 1,\n",
       "  'outline': [51, 111, 248, 463]},\n",
       " {'type': 'text',\n",
       "  'text': 'Ms Tan Siok San  Director, Student Affairs Office  Student and Academic Services Department',\n",
       "  'page_idx': 1},\n",
       " {'type': 'text',\n",
       "  'text': 'Welcome to Nanyang Technological University (NTU).',\n",
       "  'page_idx': 1},\n",
       " {'type': 'text',\n",
       "  'text': 'Thank you for choosing us as your partner in this knowledge- driven journey. We believe that your student experience and campus life in NTU will be one of significant personal growth, discovery and learning.',\n",
       "  'page_idx': 1},\n",
       " {'type': 'text',\n",
       "  'text': 'NTU is a cosmopolitan residential campus. In this microcosm of the globe, you will experience meeting talented and dynamic students, professors and researchers representing over 100 nationalities around the world. You may even be engaging intellectual giants, from Nobel Laureates to National Geographic adventurers. Find time to explore the abundant wildlife in our beautiful campus. Immerse in new and exciting teaching, learning and leisure facilities. Take a stroll along our North Spine Plaza and discover the variety of food and services available to suit all tastes and pockets.',\n",
       "  'page_idx': 1},\n",
       " {'type': 'text',\n",
       "  'text': 'Helping you to complete this journey at NTU is the Student Affairs Office (SAO). Our welcome programmes will pave the way for you to adjust to the local environment, learn about local health issues, and forge friendships that open doors to new cultures, fresh ideas and perspectives. Our One- Stop Centre caters for most student support matters such as payment and submission of important documents. If you have a disability, come and talk to our officers at the Accessible Education Unit @SAO. There is always something that we can do to make your stay more enjoyable.',\n",
       "  'page_idx': 1},\n",
       " {'type': 'text',\n",
       "  'text': 'I wish you an enriching learning experience here with us at NTU!',\n",
       "  'page_idx': 1},\n",
       " {'type': 'text', 'text': 'CONTENTS', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'INTRODUCTION 4About Nanyang Technological University 4Travelling within Campus 5Student Services Section at Student Affairs Office 6',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'BEFORE LEAVING HOME 8',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text', 'text': 'ARRIVAL 10', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'ARRIVAL 10How to Get to NTU 11Check- In to Your Housing 12Register with SAO- Student Support 13Matriculation 13Bank Account 13Network and Office 365 EDU Accounts 14Update Your Particulars 14Mobile Phone 14Orientations and Welcome Events 14A Vibrant Student Life 14',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text', 'text': 'IMMIGRATION 15', 'text_level': 1, 'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': \"IMMIGRATION 15Student's Pass 16Medical Examination 17Overstaying 17Social Visit Pass (For Spouses and Children of Full- Time Graduate Students) 17Useful Contacts of Ministries 17\",\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'HEALTH, WELLNESS AND INSURANCE 18',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'General Medical Care 19Medical Emergencies 20Academic Counselling 21Pastoral Care 21Counselling 21Stay Healthy 22Insurance 22',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'EMERGENCY HOTLINES 24Who to Call 25',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'ACADEMIC CALENDAR AND UNIVERSITY HOLIDAYS 262016- 2017 Academic Calendar 27University Holidays 28Academic Services 28',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'YOUR LEARNING ENVIRONMENT 29',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'YOUR LEARNING ENVIRONMENT 29e- learning and Mobile Learning 30IT Services 30Computer Ownership Scheme 30Computing Facilities and Learning Spaces 30NTU Libraries 31',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'STUDENT LIFE @ NTU 32',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'STUDENT LIFE @ NTU 32Student Organisations 33Sports and Games 33Meals on Campus 34Postal, Banking and Retail 34',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'MANAGING FINANCE 36',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'MANAGING FINANCE 36Estimated Living Costs 36Financial Assistance 36Part- Time Employment 36',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'ABOUT SINGAPORE 37',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'ABOUT SINGAPORE 37Experience Singapore 39Travelling within Singapore 43',\n",
       "  'page_idx': 2},\n",
       " {'type': 'text',\n",
       "  'text': 'About Nanyang Technological University',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 3},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/e4c608d4a1246f52c3e50cfb82b5edd36d36c74494a64ea1989a71ca8eafa0df.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 3,\n",
       "  'outline': [46, 128, 497, 479]},\n",
       " {'type': 'text',\n",
       "  'text': \"Young and research- intensive, Nanyang Technological University (NTU Singapore) is ranked 13th globally. It is also placed First amongst the world's best young universities.\",\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': 'The university has colleges of Engineering, Business, Science, Humanities, Arts, & Social Sciences, and an Interdisciplinary Graduate School. It also has a medical school, Lee Kong Chian School Of Medicine, set up jointly with Imperial College London.',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': 'NTU is also home to world- class autonomous entities such as the National Institute Of Education,',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': 'S Rajaratnam School Of International Studies, Earth Observatory of Singapore, and Singapore Centre on Environmental Life Sciences Engineering.',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': 'NTU provides a high- quality global education to about 33,000 undergraduate and postgraduate students. The student body includes top scholars and international olympiad medallists from the region and beyond.',\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': \"Hailing from 80 countries, the university's 4,300- strong faculty and research staff bring dynamic international perspectives and years of solid industry experience.\",\n",
       "  'page_idx': 3},\n",
       " {'type': 'text',\n",
       "  'text': 'Nanyang Business School',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'College of Engineering',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'College of Engineering- School of Chemical and Biomedical Engineering- School of Computer Engineering- School of Civil and Environmental Engineering- School of Electrical and Electronic Engineering- School of Materials Science and Engineering- School of Mechanical and Aerospace Engineering',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'College of Humanities, Arts & Social Sciences',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'School of Art, Design and Media School of Humanities and Social Sciences Wee Kim Wee School of Communication & Information',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'College of Science',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'School of Biological Sciences School of Physical & Mathematical Sciences College of Professional and Continuing Education (PaCE College)',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Interdisciplinary Graduate School',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text', 'text': 'Lee Kong Chian School of Medicine', 'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Autonomous Institutes',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Autonomous Institutes- Earth Observatory of Singapore- National Institute of Education- S. Rajaratnam School of International Studies- Singapore Centre on Environmental Life Sciences Engineering',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Travelling within Campus',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Internal Shuttle Bus',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Get the internal shuttle bus arrival information at your fingertips at http://campusbus.ntu.edu.sg/ntubus. The movement of the buses on different routes can also be viewed real- time on the map. You may even view the number plate and speed of the individual bus. You can also download a free iPhone application Traversity that tracks internal shuttle bus services at NTU.',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Public Bus Service',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Public bus services 179, 179A and 199 ply the Yunnan Garden campus in addition to the NTU shuttle bus service. Visit http://www.ntu.edu.sg/has/Transportation/ Pages/GettingToNTU.aspx for more details.',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Interactive Campus map',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'The new interactive Campus Map enables you to locate lecture theatres, tutorial rooms, buildings and landmarks on campus, and find directions between places easily. You may also print the map or send a friend an email with information on a location or direction. Google street view and internal shuttle bus routes are also integrated in the map. Best of all, it works on your mobile device too! Visit http://maps.ntu. edu.sg/maps to get interactive directions now!',\n",
       "  'page_idx': 4},\n",
       " {'type': 'text',\n",
       "  'text': 'Student Services Section at Student Affairs Office',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'The Student Services Section in Student Affairs Office (SAO) comprises Student Support and the One Stop @ Student Activities Centre (an integrated student services centre).',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Services and Information about Student Support',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 5},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/6d2fe2131d561a9098055b1a6cbdad91c68d4ae7c45de3832e651bac78f6b3c1.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 5,\n",
       "  'outline': [47, 189, 153, 272]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/b959aaaa7330c190e995473c13a791d14034c958abc34bae69c1922457f07fc9.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 5,\n",
       "  'outline': [159, 189, 264, 272]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/76c2b0db89c84eebe465b89f5c1be87d3326e6a4527eaa5d09c02729c07386f1.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 5,\n",
       "  'outline': [275, 189, 496, 273]},\n",
       " {'type': 'text',\n",
       "  'text': 'We assist international students with:',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': \"Pre- arrival information Student's Pass application Immigration matters Insurance Advice on pastoral care Crisis support Part- time employment endorsement\",\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'We also organise programmes to promote crosscultural understanding and interaction.All NTU students are welcome to attend our events and activities, which include:',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Orientation Campus Tours Coffee Sessions Community Service Cultural Tours and Outings Festive Open House Host Family Programmes Luncheons Pre- Graduation Seminars Grow,Embrace and Learn (G.E.L) Programme',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Contact Student Support',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'You can contact Student Support via our general phone line, email or come personally to our office should you require any assistance.',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Telephone: (65) 6790 6823 (during office hours) (65) 6790 5200 (24- hour Campus Security Hotline) Email: SAOstudentsupport@ntu.edu.sg',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': 'Mailing Address', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Student Affairs Office (SAO) Student and Academic Services Department Nanyang Technological University 42, Nanyang Avenue #04- 02, Student Services Centre Singapore 639815',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': 'Office Hours', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Monday to Thursday : 8.30am to 5.45pm Friday : 8.30am to 5.15pm Eve of Public Holidays : 8.30am to 12noon Saturday, Sunday and Public Holidays : Closed',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text', 'text': 'Locate Us', 'text_level': 1, 'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'You can locate us at the 4th level of the Student Services Centre. Visit http://maps.ntu.edu.sg/maps#q:student%20services%20Centre',\n",
       "  'page_idx': 5},\n",
       " {'type': 'text',\n",
       "  'text': 'Services and Information about One Stop @ Student Activities Centre (SAC)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 6},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/009c994ceebc65fc5bbfe5f2420c8d34c717d3438ef602d2a106d816fa6fb8f3.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 6,\n",
       "  'outline': [41, 189, 490, 272]},\n",
       " {'type': 'text',\n",
       "  'text': 'The One Stop @ SAC is the nerve centre and information counter for visitors with general enquiries about the university. One Stop is a strategic initiative to deliver high- quality, innovative and integrated student services which include general enquiries, application, submission and collection of documents and payment for NTU services. Additionally, two IT service desk counters are included to assist students. It is important to note that One Stop @ SAC is a cashless office. Students may choose to opt for other modes of payment such as credit card, E- nets, EZ card, Nets flash pay, cheque or telegraphic transfer.',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'Contact One Stop @ SAC',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'You can contact One Stop @ SAC via our general hotline, email or come personally to our Centre should you require any assistance.',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'Telephone: (65) 6592 3626 (during office hours)  Email: ossac@ntu.edu.sg',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text', 'text': 'Mailing Address', 'text_level': 1, 'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'One Stop @ SAC  Nanyang Technological University  50 Nanyang Avenue  NS3- 01- 03, Academic Complex North  Singapore 639798',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'Counter Operating Hours',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'Monday to Thursday : 8.30am - 5.00pm  Friday : 8.30am - 4.45pm  Eve of Public Holidays : 8.30am - 12noon  Saturday, Sunday and Public Holidays : Closed',\n",
       "  'page_idx': 6},\n",
       " {'type': 'text', 'text': 'Locate Us', 'text_level': 1, 'page_idx': 6},\n",
       " {'type': 'text',\n",
       "  'text': 'You can locate us beside the Student Activities Centre at the North Spine Plaza. Visit http://maps.ntu.edu.sg/maps#q:one%20stop%20%40%20sac.',\n",
       "  'page_idx': 6},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/778fd0a0b9b3c408d37f5275576f0f1e010cd045c541a3746ec3d4622091e4bb.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 7,\n",
       "  'outline': [0, 70, 536, 680]},\n",
       " {'type': 'text',\n",
       "  'text': 'BEFORE LEAVING HOME',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Use this checklist to prepare for your stay in Singapore',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Accept the Offer of Admission by stipulated deadline',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Apply for an Entry Visa (if applicable) to enter Singapore (Please refer to www.ica.gov.sg)',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': \"Apply for a Student's Pass (Please refer to Student's Pass section)\",\n",
       "  'page_idx': 8},\n",
       " {'type': 'text', 'text': 'Make housing arrangement', 'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Make travel arrangement to Singapore',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text', 'text': 'Pack your belongings', 'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'A print- out of eForm16 and duly signed by you Colour passport- sized photographs, including digital copies Original letter of admission from NTU Original copies of educational certificates, examination results and other documents submitted in support of your application for admission (not applicable for exchange students) Cash of at least \\\\(\\\\) 3,000$ for initial expenses (excluding tuition fees and rental) Funds to place a deposit and pay housing rental upon arrival Prescribed medication Personal items such as toiletries, towel, bed linen, pillow cases, alarm clock, adapter, etc Clothes and footwear, including a light jacket for air- conditioned venues such as the Library Formal wear for interviews or important functions Notebook if you are not purchasing one in Singapore (Singapore is using 230V for electricity)',\n",
       "  'page_idx': 8},\n",
       " {'type': 'text',\n",
       "  'text': 'Make sure these items are with you for clearance at the Singapore Checkpoint Your passport with at least six- month validity from the commencement of your course Your plane, train or bus ticket A print- out of your In- Principle Approval (IPA) letter A print- out of your Letter of Admission',\n",
       "  'page_idx': 8},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a8b5ae1b8baccf5d6c9e18a097d14ec754578be41a3971d1e0ca79bbab134326.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 9,\n",
       "  'outline': [0, 89, 536, 680]},\n",
       " {'type': 'text', 'text': 'ARRIVAL', 'text_level': 1, 'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'You will be busy for the first few weeks at NTU! Use this checklist to settle down',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text', 'text': 'Know how to get to NTU', 'page_idx': 10},\n",
       " {'type': 'text', 'text': 'Check- in to your housing', 'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Register with SAO- Student Support',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text', 'text': 'Complete registration procedures', 'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': \"Be briefed on the completion of the Student's Pass formalities\",\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Undergo a medical examination at the Fullerton Healthcare@NTU for students on more than 6 months study programme (Please refer to Medical Examination section)',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': \"Complete the Student's Pass formalities and collect your Student's Pass (Please refer to Student's Pass section)\",\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Complete all matriculation procedures',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Open a bank account if your study duration is more than 6 months',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Purchase a Singapore mobile line (optional)',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Activate your network and Office 365 EDU accounts',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Update your particulars and contact details via StudentLink (undergraduate students), GSLink (graduate students), or Exchange Portal (exchange students)',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'Attend orientation, welcome events and SAO- Student Support activities',\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': \"Join a student club or society and plug into NTU's vibrant student life (optional)\",\n",
       "  'page_idx': 10},\n",
       " {'type': 'text',\n",
       "  'text': 'How to Get to NTU',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'Singapore has an efficient, safe and affordable public transport system. You can take a taxi, Mass Rapid Transit (MRT) train or bus to campus. If you are travelling to Singapore for the first time and have luggage with you, we recommend you to take a taxi.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'For more information on the public transportation in Singapore, please refer to the Travelling within Singapore section.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': 'Taxi', 'text_level': 1, 'page_idx': 11},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/403da85c550ed9fced0c7c1de7ff02e789b440925f0e02a93be0d312daf0d868.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 11,\n",
       "  'outline': [47, 255, 266, 393]},\n",
       " {'type': 'text',\n",
       "  'text': 'You can hail a taxi at the taxi bays outside the Arrival Hall at the airport, at taxi stand or anywhere safe along public roads in Singapore. Inform the driver the address of your destination. If you are travelling to NTU, request the driver to take the Pan Island Expressway (PIE). The journey will take about 45 minutes. Taxi fares in Singapore are charged by the taxi meter and are based on a flag down rate and distance travels. Please check with the driver or taxi company on the surcharge and ask for a receipt at the end of the trip. Comprehensive information on Singapore taxis is available at www. taxisingapore.com.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text', 'text': 'MRT and Bus', 'text_level': 1, 'page_idx': 11},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/f7a57c82de384e661f071689496782b5b164d088bdc0e497f39b404969257701.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 11,\n",
       "  'outline': [278, 153, 496, 293]},\n",
       " {'type': 'text',\n",
       "  'text': 'The nearest train stations to NTU are Boon Lay (EW27) and Pioneer (EW28) on the East West Line operated by SMRT Corporation.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'From Boon Lay station, make your way to the adjacent bus interchange. Services 179 &199 will take you into NTU.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'From Pioneer station, you can hop on the Campus Rider that stops in front of Blk 649A, Jurong West Street 61.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'If you are coming from Changi Airport and are travelling light, you can get on Changi Airport (CG2), transfer at Tanah Merah (EW4) and take the train all the way to Boon Lay or Pioneer station. The journey may take over an hour. Please note that trains and public buses do not operate between midnight and 5.30am daily.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'For a map of MRT stations in Singapore, please visit www.ntu.edu.sg/AboutNTU/visitingntu/Pages/location. aspx.',\n",
       "  'page_idx': 11},\n",
       " {'type': 'text',\n",
       "  'text': 'Check-In to Your Housing',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'If you have applied for and been offered a place in campus housing, please ensure that you have provided your arrival details online. Please refer to your offer email for information on the collection of your room key.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'For further enquiries on housing matters, please contact the Office of Housing and Auxiliary Services (HAS), the office administrating on- campus and off- campus housing, via email. You can also visit www.ntu.edu.sg/has for more information on campus and off- campus housing.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/732c52c367c84f0a7633dd8fb1b4f4d10525578eb465165aee1d8a35d556b7a5.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Category</td><td>Contact</td></tr><tr><td>Undergraduate Students</td><td>has-ug@ntu.edu.sg</td></tr><tr><td>Graduate Students</td><td>has-pg@ntu.edu.sg</td></tr><tr><td>Exchange Students</td><td>has-exch@ntu.edu.sg</td></tr></table>',\n",
       "  'page_idx': 12,\n",
       "  'outline': [40, 200, 491, 252]},\n",
       " {'type': 'text',\n",
       "  'text': 'Register with SAO-Student Support Matriculation',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': \"Please settle into your housing before registering with SAO- Student Support during office hours to complete the registration procedures and be briefed on the procedures to complete the Student's Pass formalities. Do bring along your passport, embarkation card, Letter of Admission/Enrolment, receipts for any NTU's Miscellaneous Fee payment.\",\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Matriculation refers to the process of registering as a student of the university. After you have completed all matriculation procedures, NTU will issue you with a matriculation card identifying you as its student.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text', 'text': 'Bank Account', 'text_level': 1, 'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'For students whose study period is 6 months or more, you may choose to open an account with the bank of your choice in Singapore. The banks offer a wide range of services and have different types of saving accounts.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'The OCBC bank has a branch on campus at the North Spine at Block N3. Other banks are located near NTU at Jurong Point Shopping Centre. Please contact the banks or visit their website to determine their requirement for opening and maintaining an account.',\n",
       "  'page_idx': 12},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/4ae5433aeb5bccbd3d050457420168ff23fcc53e7cf266fb109e8bd27485ba8c.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Name of Bank</td><td>Website</td><td>Local Telephone Number</td></tr><tr><td>Development Bank of Singapore (DBS)</td><td>www.dbs.com.sg</td><td>1800 111 1111</td></tr><tr><td>Overseas-Chinese Banking Corporation (OCBC)</td><td>www.ocbc.com</td><td>1800 438 3333</td></tr><tr><td>POSBank</td><td>www.dbs.com/posb</td><td>1800 339 6666</td></tr><tr><td>United Overseas Bank Ltd (UOB)</td><td>www.uob.com.sg</td><td>1800 222 2121</td></tr></table>',\n",
       "  'page_idx': 12,\n",
       "  'outline': [41, 454, 491, 541]},\n",
       " {'type': 'text',\n",
       "  'text': 'To open a bank account, you will need:',\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': \"- Your original passport- Your Student's Pass (IPA Letter subject to the bank's acceptance)- Your Singapore address (must be reflected on original official letters, e.g. phone bills, bank statement, utilities bills.)\",\n",
       "  'page_idx': 12},\n",
       " {'type': 'text',\n",
       "  'text': 'Network and Office 365 EDU Accounts',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Your network account enables you to access to the NTU computer network, Intranet portal iNTU (https:// intu.ntu.edu.sg), e- services (StudentLink, GSLink), e- learning (NTULearn), Library databases and other computer resources. You will receive the details upon registration.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Your Office 365 EDU account enables you to email access to your email and a suite of Office 365 services (such as online storage, office software, etc.) at http:// www.outlook.com/e.ntu.edu.sg.This account is a lifelong account to serve your needs as a NTU alumnus.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to http://www.ntu.edu.sg/cits/newusers/ newstudent/Pages/studentaccounts.aspx for more information on your computer accounts.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Prior to your use of the computer account, you must change your account password according to the instructions stated in the website.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Update Your Particulars',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Update Your ParticularsPlease access StudentLink (undergraduate students), GSLink (graduate students), or Exchange Portal (exchange students) to update your particulars and contact details.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text', 'text': 'Mobile Phone', 'text_level': 1, 'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Mobile PhoneYou can sign up for a mobile line at Jurong Point Shopping Centre near to NTU or convenience store. Singapore has 3 telecommunication companies. Please visit their website to know more about their plans and rates.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/2d08570de0ad177f8c0abdbeed3181b54cfacaee7afb60f34f0b1a76a622eb70.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>M1</td><td>www.m1.com.sg</td></tr><tr><td>SingTel</td><td>www.singtel.com.sg</td></tr><tr><td>StarHub</td><td>www.starhub.com</td></tr></table>',\n",
       "  'page_idx': 13,\n",
       "  'outline': [278, 189, 496, 235]},\n",
       " {'type': 'text',\n",
       "  'text': 'Orientations and Welcome Events',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'Orientations and Welcome EventsThe Freshmen Welcome Ceremonies, orientations, campus and laboratory tours, and welcome events organised by SAO- Student Support, schools and Halls of Residence provide new students with useful information on student services and campus life. They are also great occasions to interact with fellow students and widen your social network.',\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': 'A Vibrant Student Life',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 13},\n",
       " {'type': 'text',\n",
       "  'text': \"A Vibrant Student LifeImmerse into NTU's vibrant student life with more than 100 student organisations with diverse interests from astronomy to sports to music. Please visit www.ntu.edu.sg/campuslife/clubs for more details.\",\n",
       "  'page_idx': 13},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/852a822c2a708848cdbd940e7559ddfb404fdb66c38df030c79b026d5e16f6b0.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 14,\n",
       "  'outline': [0, 114, 536, 680]},\n",
       " {'type': 'text', 'text': \"Student's Pass\", 'text_level': 1, 'page_idx': 15},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/43de50962870cc271ec8987742eaae9916582b5c2cbde674dbd948482025b2b6.jpg',\n",
       "  'image_caption': [\"Back of Student's Pass\"],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 15,\n",
       "  'outline': [49, 131, 265, 268]},\n",
       " {'type': 'text',\n",
       "  'text': \"All international students who have been accepted by NTU as full- time matriculated or registered students are required to hold a valid Student's Pass issued by the Immigration & Checkpoints Authority (ICA) of Singapore.\",\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"Application for Student's Pass\",\n",
       "  'text_level': 1,\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"New application for Student's Pass must be submitted at least 1 month and not more than 2 months before the commencement of the course. The application must be submitted through the Student's Pass Online Application and Registration (SOLAR) system which is accessible through ICA website at www.ica.gov.sg.\",\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"International students who had pursued pre- university studies in Singapore need to produce their Student's Pass if these have not been returned to ICA.\",\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"You may wish to email SAO- Student Support should you have further enquiry on Student's Pass application.\",\n",
       "  'page_idx': 15},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/18f9d1ee748a5d62204d0767ba22aa4a16ae45e32a948f6c10c99443ebf9dba6.jpg',\n",
       "  'image_caption': [\"Front of Student's Pass\"],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 15,\n",
       "  'outline': [278, 129, 496, 267]},\n",
       " {'type': 'text',\n",
       "  'text': 'Application Fees and Charges',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': 'You can pay the fees payable to ICA online via the SOLAR system by credit/debit card or internet banking.',\n",
       "  'page_idx': 15},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/32d86e2d1f8344e45518cedbe83c9ea13ad0dcc4a97857b236d2721fc6b359a7.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Processing Fee (non-refundable)</td><td>$30</td></tr><tr><td>Issuance Fee</td><td>$60</td></tr><tr><td>Multiple-Entry Visa fee (for visa required nationals)</td><td>$30</td></tr></table>',\n",
       "  'page_idx': 15,\n",
       "  'outline': [277, 357, 496, 414]},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"- A replacement fee of  $\\\\) 100\\\\(will be imposed if the Student's Pass is lost or stolen. An additional$ \\\\ $30$  processing fee will be imposed for amendments made to eForm16 after submission.\",\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"Cancellation of Student's Pass\",\n",
       "  'text_level': 1,\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': \"You are required to surrender your Student's Pass to ICA for cancellation within 7 days from the date of cessation or termination of your full- time studies, go on leave of absence or convert to part- time study. Please visit www.ica.gov.sg for more information.\",\n",
       "  'page_idx': 15},\n",
       " {'type': 'text',\n",
       "  'text': 'Medical Examination',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': \"The offer of admission to NTU is conditional upon the issuance of a valid Student's Pass by ICA. You are required to undergo the medical examination should your period of study be more than 6 months. The examination will include the Human Immunodeficiency (HIV) and Tuberculosis (TB) medical tests. These tests must be done at the Fullerton Healthcare@NTU.\",\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': \"Upon satisfactory completion of the medical examination, the Fullerton Healthcare@NTU will issue you with a medical report which you need to submit with your Student's Pass application. You will be denied admission and must return home at your own expense should you fail any of the medical examination.\",\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Appointment for Medical Examination',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'All international students will receive instruction on how to complete the medical examination when they report to SAO- Student Support upon their arrival at NTU.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text', 'text': 'Overstaying', 'text_level': 1, 'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': \"Overstaying is a punishable offence under the Immigration Act. You need to note the expiry date of your Social Visit Pass or Student's Pass and apply for an extension at least a month before the expiry.\",\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Social Visit Pass (For Spouses and Children of Full-Time Graduate Students)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Applying for an Entry Visa',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'If your spouse and children require an Entry Visa to visit Singapore, they may apply accordingly at a Singapore embassy in your home country.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Extending the Social Visit Pass',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'You may approach SAO- Student Support for a letter of support to extend the Social Visit Pass of your spouse and children. Please visit www.ntu.edu.sg/SAO/Studentsupport for more details.',\n",
       "  'page_idx': 16},\n",
       " {'type': 'text',\n",
       "  'text': 'Useful Contacts of Ministries',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 16},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/cf7e0c7ed1bac20143f902b0f1f5719b26d8557120dc7d3277a6bb5ae7c9d24d.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td rowspan=\"4\">In-charge of Student&#x27;s Pass</td><td rowspan=\"4\">Immigration &amp;amp; Check-points Authority (ICA)</td><td>Address:</td><td>ICA Building\\n10 Kallang Road\\nSingapore 206718</td></tr><tr><td>Location:</td><td>Next to Lavender MRT station</td></tr><tr><td>Telephone Number:</td><td>(65) 6391 6100 (24-hour ICA call centre)</td></tr><tr><td>Website:</td><td>www.ica.gov.sg</td></tr><tr><td rowspan=\"4\">In-charge of Training Employment Pass and Work Holiday Pass</td><td rowspan=\"4\">Ministry of Manpower (MOM)</td><td>Address:</td><td>The Riverwalk\\n20 Upper Circular Road\\n#04-01/02\\nSingapore 058416</td></tr><tr><td>Location:</td><td>Nearest MRT station: Clarke Quay MRT station</td></tr><tr><td>Telephone Number:</td><td>(65) 6438 5122</td></tr><tr><td>Website:</td><td>www.mom.gov.sg</td></tr></table>',\n",
       "  'page_idx': 16,\n",
       "  'outline': [40, 443, 491, 646]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/5c3c54241390ec5233b91eef478cccedf7b6bb038d3900bdb3a4672ba4a1e0ba.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 17,\n",
       "  'outline': [45, 0, 430, 678]},\n",
       " {'type': 'text',\n",
       "  'text': 'HEALTH, WELLNESS AND INSURANCE',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'General Medical Care',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/aeee19747a27fd2427b292f02fc3d3f0f0df49188dc8ef4a660e16c19446aedb.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 18,\n",
       "  'outline': [40, 106, 491, 316]},\n",
       " {'type': 'text',\n",
       "  'text': 'Fullerton Healthcare@NTU',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'The Medical Service on campus is operated by Fullerton Healthcare Group. Health services provided include general outpatient medical and dental treatment, laboratory and x- ray investigation, as well as minor surgery. They also provide immunisation and travel medical advice.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/02b294ae53cee41851a3a463aefe8b07c15e7c9981a74eb38012b6a7532ff37f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td colspan=\"2\">Address</td></tr><tr><td colspan=\"2\">Fullerton Healthcare @ NTU\\nUniversity Health Service\\n36 Nanyang Avenue, #01-01\\nSingapore 639801</td></tr><tr><td colspan=\"2\">Telephone Number</td></tr><tr><td colspan=\"2\">Medical: (65) 6793 6828 / (65) 6793 6794\\nDental: (65) 6790 8331</td></tr><tr><td colspan=\"2\">Operating Hours</td></tr><tr><td>Monday to Friday</td><td>8.30am to 9.00pm\\n(last registration at 8.30pm)</td></tr><tr><td>Saturday:</td><td>9.30am to 12.00noon</td></tr><tr><td>Sunday and Public Holidays:</td><td>Closed</td></tr></table>',\n",
       "  'page_idx': 18,\n",
       "  'outline': [40, 426, 259, 652]},\n",
       " {'type': 'text',\n",
       "  'text': 'Clinics Near Campus',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'There are several private clinics, that are near NTU. You may wish to visit http://www.singhealth.com.sg/PatientCare/GP/Pages/Home.aspx for a comprehensive list of clinics in Singapore.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Please note that visits to private clinics are at your own expenses.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Pharmacies Near Campus',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'These are the pharmacies which are near NTU.',\n",
       "  'page_idx': 18},\n",
       " {'type': 'text',\n",
       "  'text': 'Guardian Pharmacy',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/fc3143c6add0ef3920001157e00d2c1bf89c15914a65643d6132f81933ff71d5.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Location</td></tr><tr><td>Jurong Point Shopping Mall</td></tr><tr><td>1 Jurong West Central 2 #B1-27/28 (JP2)</td></tr><tr><td>Singapore 648886</td></tr></table>',\n",
       "  'page_idx': 18,\n",
       "  'outline': [274, 497, 490, 549]},\n",
       " {'type': 'text',\n",
       "  'text': 'Watsons Your Personal Store',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 18},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/1bf951d2806b9bdacd69639b43dca3e3eeb3ed3829b856ab4189f290d3a7c1c8.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Location</td></tr><tr><td>Jurong Point Shopping Mall</td></tr><tr><td>1 Jurong West Central 2 #B1-12/13/14</td></tr><tr><td>Singapore 648886</td></tr></table>',\n",
       "  'page_idx': 18,\n",
       "  'outline': [274, 575, 490, 627]},\n",
       " {'type': 'text',\n",
       "  'text': 'Medical Emergencies',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': \"In a medical emergency where immediate specialist treatment is required, please proceed to the hospital's Emergency department. The nearest government hospital is Ng Teng Fong General Hospital and their contact details are as follows:\",\n",
       "  'page_idx': 19},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/f924ccc72e0fbf8bbb4208d2a0c3d036648155473f3d45683f5f637e4e610c50.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Telephone Number</td></tr><tr><td>(65) 6716 2000</td></tr><tr><td>Email Address</td></tr><tr><td>enquiries@juronghealth.com.sg</td></tr><tr><td>Website</td></tr><tr><td>www.ntfgh.com.sg</td></tr></table>',\n",
       "  'page_idx': 19,\n",
       "  'outline': [46, 193, 264, 282]},\n",
       " {'type': 'text',\n",
       "  'text': 'Seeking Reimbursement for Hospitalisation',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': \"Eligible students may seek a reimbursement under the Group Hospitalisation and Surgical Insurance (GHSI) scheme for the hospitalisation fee incurred in Singapore government/restructured hospitals. The insurance company will review and determine the reimbursed amount based on the scheme's terms and conditions. For more information on GHSI, please refer to Insurance section.\",\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Seeking Reimbursement for Outpatient Specialist Care',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Eligible students are covered up to \\\\(\\\\) 1000$ per policy year for treatment by a specialist (including diagnostic tests such as x- rays and scans) or A&E department (strictly for accidents and emergencies only) in Singapore government/restructured hospitals. The coverage is subject to the terms and conditions of the GHSI. For more information on GHSI, please refer to Insurance section.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'text',\n",
       "  'text': 'Important: Outpatient specialist care will only be reimbursed if the specialist whom you are seeking treatment from is referred by the Fullerton Healthcare@ NTU or the A&E department of a government/restructured hospital.',\n",
       "  'page_idx': 19},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/03ec2fd407fe0a1bb62c6a5412ed1418d2357486abcdfa8586706a559a9ee054.jpg',\n",
       "  'table_caption': ['For a list of Singapore Government/Restructured Hospitals and Specialist Clinics, please refer to the table below.'],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Singapore Government/Restructured Hospitals</td><td>Website</td></tr><tr><td>Alexandra Hospital</td><td>www.alexhosp.com.sg</td></tr><tr><td>Changi General Hospital</td><td>www.cgh.com.sg</td></tr><tr><td>Institute of Mental Health</td><td>www.imh.com.sg</td></tr><tr><td>Khoo Teck Puat Hospital</td><td>www.ktph.com.sg</td></tr><tr><td>KK Women&#x27;s and Children&#x27;s Hospital</td><td>www.kkh.com.sg</td></tr><tr><td>National University Hospital</td><td>www.nuh.com.sg</td></tr><tr><td>Ng Teng Fong General Hospital</td><td>www.ntfgh.com.sg</td></tr><tr><td>Singapore General Hospital</td><td>www.sgh.com.sg</td></tr><tr><td>Tan Tock Seng Hospital</td><td>www.ttsh.com.sg</td></tr></table>',\n",
       "  'page_idx': 19,\n",
       "  'outline': [46, 440, 496, 590]},\n",
       " {'type': 'text',\n",
       "  'text': 'HEALTH, WELLNESS AND INSURANCE',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Support for Students with Special Needs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'The Accessible Education Unit (AEU) offers professional guidance and advice to students with disabilities and special needs. Disabilities may include the following conditions:',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Physical or mobility disability Sensory impairment (hearing or vision loss) Learning disability (e.g. Dyslexia, Autism, ADHD) Neurological deficits (e.g. traumatic head injury)',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'If you have special needs and require support services, please email the Accessible Education Unit at aeu@ntu.edu.sg',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Please note that medical evidences or specialist reports are required for verification of disability or special needs.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Academic Counselling',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'If you are unable to cope with your studies, please seek help from faculty/staff, tutor or the Assistant Chair of Students in your school.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text', 'text': 'Pastoral Care', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'Being away from home can be an especially lonely experience when you fall ill or are hospitalised. Do contact SAO- Student Support should you need any assistance.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/019c10e85f531720a89cce1da291adb7f68c724beacd1f74f5a272f81e3aff7f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Locate SAO-Student Support</td></tr><tr><td>Visit us at our office located on level 4 of the Student Services Centre</td></tr><tr><td>Telephone Number</td></tr><tr><td>(65) 6790 6823 (during office hours)\\n(65) 6790 5200 (24-hour Campus Security Hotline)</td></tr><tr><td>Email</td></tr><tr><td>SAO-Studentsupport@ntu.edu.sg</td></tr></table>',\n",
       "  'page_idx': 20,\n",
       "  'outline': [40, 446, 259, 564]},\n",
       " {'type': 'text', 'text': 'Counselling', 'text_level': 1, 'page_idx': 20},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a82566e7d31546c1ca76f5091b7c6ca1e55a4167e086312f7c6a2777d8eece8f.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 20,\n",
       "  'outline': [272, 304, 490, 419]},\n",
       " {'type': 'text',\n",
       "  'text': 'Professional Counselling',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'The Student Wellbeing Centre is available to all students for professional counselling. A team of registered counsellors are experienced in helping students from various backgrounds and with a wide range of issues.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'If you are facing challenges which affect your health, relationships, daily activities, academic performance or eating and sleeping patterns, please seek professional counselling. You may also approach the Centre if you are interested in personal development or self- improvement.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'If you have troubled thoughts or behaviour such as self- harm or harming others, please seek professional counselling or medical help.',\n",
       "  'page_idx': 20},\n",
       " {'type': 'text',\n",
       "  'text': 'To speak to a professional Student Counsellor, please make an appointment at www.ntu.edu.sg/ studentwellbeing/appointment or call (65) 6790 4462 during office hours. The Centre is located at University Health Service, #02- 01, 36 Nanyang Avenue. Consultation is free of charge for students and held in strict confidence.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'After office hours, you may contact Campus Security at (65) 6790 5200 or approach your Hall Fellow if you live in a Hall of Residence if there is any emergency.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Peer Helping Programme',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': \"The Student Wellbeing Centre administers a peer support network for students on campus called the 'Peer Helping Programme'. Student volunteers in the programme are trained by the Centre's professional Student Counsellors to befriend and support students with emotional and/or psychological issues. If you wish to find out more about this programme, please call or email the Student Wellbeing Centre at studentwellbeing@ntu.edu.sg.\",\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Workshops and Self-Help Resources',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'The Student Wellbeing Centre further promotes student well- being through workshops and talks on topics such as strategies for better learning, and stress and relaxation techniques. Resources are also available for students to support them through various periods in the academic journey. Visit www.ntu.edu.sg/studentwellbeing/selfhelp/students or drop by the Centre for these resources.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': 'Stay Healthy', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Learn how you can adopt an active and healthy lifestyle. Check out www.ntu.edu.sg/has/SnR/Pages/index. aspx for the programmes of the Sports and Recreation Centre and the Healthy Lifestyle Unit.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text', 'text': 'Insurance', 'text_level': 1, 'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'NTU has two insurance schemes - Group Hospitalisation and Surgical Insurance and the Group Personal Accident Insurance - to help eligible students meet basic medical cost.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Group Hospitalisation and Surgical Insurance (GHSI)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'GHSI is compulsory for all full- time international students, including Singapore permanent residents.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Falling ill and being hospitalised in Singapore can be a financial drain on international students who are not entitled to medical subsidies. Further, hospitals require a deposit of the entire estimated cost upon admission. For example, if the estimated cost for a hospital stay is  $\\\\) 5,000\\\\(you must place a deposit of$ \\\\ $5,000$  with the hospital upon admission.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'For eligible students on the GHSI, the underwriter of GHSI will prepare a Letter of Guarantee (LOG), which you can present to the hospital in lieu of the cash deposit, subject to the terms and conditions of the insurance scheme. For more information, please refer to www.ntu- ghs.com.sg',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'Group Personal Accident Insurance (GPAI)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'The GPAI Scheme provides basic coverage for accidental death or permanent disablement as well as medical reimbursement for accidents for undergraduates and full- time graduate students (optional). Please see www.ntu.edu.sg/Students/Undergraduate/StudentServices/HealthAndCounselling/MedicalInsuranceSchemes/Pages/GPAI.aspx for details including conditions for eligibility.',\n",
       "  'page_idx': 21},\n",
       " {'type': 'text',\n",
       "  'text': 'HEALTH, WELLNESS AND INSURANCE',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'Additional Insurance Coverage for Overseas Travel',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'All students are strongly advised to have adequate insurance coverage when travelling overseas for internships, conferences, exchange, study programmes, etc. The GHSI scheme does not provide adequate coverage for overseas travels.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to the following table for a summary of the insurance scheme.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/0305b249f78b1e0e1efb573decfb7dfdc37d92e003ffe73078da224db1fc3163.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Insurance Scheme</td><td>Insurance Coverage</td><td>Administered By</td><td>Eligibility</td></tr><tr><td>Group Hospitalisation and Surgical Insurance\\nwww.ntu-ghs.com.sg</td><td>Accidental death, hospitalisation and surgery as a result of illness (including mental illness), and accidental bodily injury</td><td>SAO-Student Support at Student Affairs Office</td><td>Full-time International Students, Singapore Permanent Resident</td></tr><tr><td>Group Personal Accident Insurance\\nwww.ntu.edu.sg/Students/Undergraduate/StudentServices/HealthAndCounselling/MedicalInsuranceSchemes/Pages/GPAI.aspx</td><td>Death, permanent disability and medical costs arising from an accident</td><td>Student Affairs Office</td><td>Undergraduates\\nFull-time Graduate students may opt for this insurance</td></tr></table>',\n",
       "  'page_idx': 22,\n",
       "  'outline': [40, 201, 491, 422]},\n",
       " {'type': 'text', 'text': 'Note:', 'text_level': 1, 'page_idx': 22},\n",
       " {'type': 'text',\n",
       "  'text': '- Full-time international undergraduates who require hospitalisation and/or surgery as a result of an accident should first claim under the GPAI scheme.- Information on the GHSI and GPAI is subject to changes.',\n",
       "  'page_idx': 22},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/af8d8fe2a1a7ea4eeee7f6f1d5ab3fe4638a7c94c64ca3f2152cacbe70ddcfa2.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 23,\n",
       "  'outline': [0, 95, 536, 680]},\n",
       " {'type': 'text',\n",
       "  'text': 'Emergency Hotlines',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'Emergency HotlinesPlease make sure that you save these numbers in your mobile or smart phone. They will come in handy in an emergency. For more crisis helplines, please visit www.ntu.edu.sg/studentwellbeing.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/f3bbbd662105924ca18fc0d0d8c4d25b27a7a5002973f0ec1c6f74e2bb012ba9.jpg',\n",
       "  'table_caption': ['Who to Call'],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Emergency Party</td><td>Telephone Number</td></tr><tr><td>Police</td><td>999 (24-hour)</td></tr><tr><td>Ambulance</td><td>995 (24-hour)</td></tr><tr><td>NTU Student Wellbeing Centre</td><td>6790 4462</td></tr><tr><td>Fullerton Healthcare@NTU</td><td>6793 6828</td></tr><tr><td>NTU Campus Security</td><td>6790 5200 (24-hour)</td></tr><tr><td>NTU Fault Reporting Centre</td><td>6790 4777 (24-hour)</td></tr></table>',\n",
       "  'page_idx': 24,\n",
       "  'outline': [40, 167, 490, 261]},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/4de80eb6fcfa95df34ec160b82e2b88d49efabc022bc113f144e5f795497da1b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Scenarios</td><td>Procedures</td></tr><tr><td>Medical Emergencies</td><td>During office hours: Call Fullerton Healthcare@NTU\\nAfter office hours: Call Campus Security/Hall Fellow</td></tr><tr><td>Emotional Distress/Suicidal Tendencies</td><td>During office hours: Call Student Wellbeing Centre\\nAfter office hours: Call Campus Security/Hall Fellow</td></tr><tr><td>Road Accidents</td><td>Call Police, Ambulance and Campus Security</td></tr><tr><td>Crime</td><td>Call Police and Campus Security</td></tr><tr><td>Missing Persons</td><td>Call Campus Security</td></tr><tr><td>Fire</td><td>Call Campus Security</td></tr><tr><td>Rowdy Behaviour</td><td>Call Campus Security</td></tr><tr><td>Lift Breakdown/Power Blackouts/\\nBurst Pipes</td><td>Call NTU Fault Reporting Centre</td></tr></table>',\n",
       "  'page_idx': 24,\n",
       "  'outline': [40, 286, 490, 547]},\n",
       " {'type': 'text',\n",
       "  'text': '24-Hour Campus Security',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': 'Our campus is patrolled round- the- clock on a daily basis by a team of security officers. Complementing these patrols are CCTV and security access control systems. A low crime rate, however, does not mean that there is no crime so please guard your safety and belongings.',\n",
       "  'page_idx': 24},\n",
       " {'type': 'text',\n",
       "  'text': \"Fire drills and emergency evacuation procedures are in place for students' security and safety.\",\n",
       "  'page_idx': 24},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/d84d3a3f4ecdf1a5a560c3efa66833a6913d95df480718391097d017a66e2262.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td colspan=\"4\">Planner</td><td></td></tr><tr><td colspan=\"2\">SUNDAY</td><td>MONDAY</td><td>TUESDAY</td><td>WED</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>2</td><td></td><td>3</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>5</td></tr><tr><td>9</td><td></td><td>10</td><td>11</td><td>12</td></tr><tr><td>16</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td rowspan=\"4\">ACADEMIC CALENDAR AND UNIVERSITY HOLIDAY</td><td>18</td><td></td><td></td></tr><tr><td>3</td><td>24</td><td></td><td></td></tr><tr><td></td><td>25</td><td></td><td></td></tr><tr><td></td><td>26</td><td></td><td></td></tr></table>',\n",
       "  'page_idx': 25,\n",
       "  'outline': [0, 0, 537, 679]},\n",
       " {'type': 'text',\n",
       "  'text': 'ACADEMIC CALENDAR AND UNIVERSITY HOLIDAY',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 26},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/179a66d7be7f1c852a3c2049040501776582985fc138a18289f7341ee0e04313.jpg',\n",
       "  'table_caption': ['2016-2017 Academic Calendar'],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Academic Year 2016-17</td><td>From</td><td>To</td></tr><tr><td>Semester 1</td><td>23-Jul-16</td><td>02-Dec-16</td></tr><tr><td>Freshmen Orientation</td><td>23-Jul-16</td><td>05-Aug-16</td></tr><tr><td>Teaching Weeks</td><td>08-Aug-16</td><td>23-Sep-16</td></tr><tr><td>Recess Week</td><td>26-Sep-16</td><td>30-Sep-16</td></tr><tr><td>Teaching Weeks</td><td>03-Oct-16</td><td>11-Nov-16</td></tr><tr><td>Revision and Examination</td><td>14-Nov-16</td><td>02-Dec-16</td></tr><tr><td>Vacation</td><td>05-Dec-16</td><td>06-Jan-17</td></tr><tr><td>Semester 2</td><td>09-Jan-17</td><td>05-May-17</td></tr><tr><td>Teaching Weeks</td><td>09-Jan-17</td><td>24-Feb-17</td></tr><tr><td>Recess Week</td><td>27-Feb-17</td><td>03-Mar-17</td></tr><tr><td>Teaching Weeks</td><td>06-Mar-17</td><td>14-Apr-17</td></tr><tr><td>Revision and Examination</td><td>17-Apr-17</td><td>05-May-17</td></tr><tr><td>Vacation</td><td>08-May-17</td><td>04-Aug-17</td></tr><tr><td>Special Term I</td><td>08-May-17</td><td>16-Jun-17</td></tr><tr><td>Teaching Weeks</td><td>08-May-17</td><td>09-Jun-17</td></tr><tr><td>Revision &amp;amp; Examination</td><td>12-Jun-17</td><td>16-Jun-17</td></tr><tr><td>Special Term II</td><td>19-Jun-17</td><td>28-Jul-17</td></tr><tr><td>Teaching Weeks</td><td>19-Jun-17</td><td>21-Jul-17</td></tr><tr><td>Revision &amp;amp; Examination</td><td>24-Jul-17</td><td>28-Jul-17</td></tr></table>',\n",
       "  'page_idx': 26,\n",
       "  'outline': [39, 131, 490, 446]},\n",
       " {'type': 'text',\n",
       "  'text': 'Research students in postgraduate programmes are expected to work on their research projects throughout the period of their candidate, subject to the student terms, requirements and entitlements.',\n",
       "  'page_idx': 26},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/c2069e52fa37b2c4b086ed7ebfbcb66a6efd75efcc33a4ebb635ecf6bf239dbe.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': ['For the full calendar and details, please visit http://www.ntu.edu.sg/Students/Undergraduate/AcademicServices/AcademicCalendar/Pages/AY2016-17.aspx.'],\n",
       "  'table_body': '<table><tr><td>Events</td><td>Date</td></tr><tr><td>Convocation 2016</td><td>25 Jul to 1 Aug 16</td></tr><tr><td>Undergraduate Freshmen Orientation</td><td>23 Jul to 5 Aug 16</td></tr><tr><td>Undergraduate Qualifying English Test</td><td>2 Aug 16</td></tr><tr><td>Freshmen Welcome Ceremony</td><td>4 &amp;amp; 5 Aug 16</td></tr><tr><td>Students&#x27; Union Day / Academic Council Meeting</td><td>25 Aug 16\\nNo classes for UG programmes from 1030 to 1430 hours</td></tr><tr><td>Celebrate NTU!\\nCampus-wide celebratory event including the State of the University Address by President</td><td>7 Mar 2017 (TBC)</td></tr></table>',\n",
       "  'page_idx': 26,\n",
       "  'outline': [40, 482, 491, 619]},\n",
       " {'type': 'text',\n",
       "  'text': 'University Holidays',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'The university is closed during public holidays in Singapore. Classes will proceed as usual on the following Monday should the public holiday fall on a Saturday.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': '2016 and 2017 Public Holidays',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'For a list of public holidays in 2016 and 2017, please refer to the following table or to www.mom.gov.sg/newsroom/press- releases/2015/0512- ph- 2016.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/428bdc35d7eace037d189675b510f8301f4787ed8abce6488fd6a608e58f6355.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>National Day</td><td>9 Aug 2016</td><td>Tuesday</td></tr><tr><td>Hari Raya Haji</td><td>12 Sep 2016</td><td>Monday</td></tr><tr><td>Deepavali</td><td>29 Oct 2016</td><td>Saturday</td></tr><tr><td>Christmas Day</td><td>25 Dec 2016 *</td><td>Sunday</td></tr><tr><td>New Year&#x27;s Day</td><td>1 Jan 2017 *</td><td>Sunday</td></tr><tr><td>Chinese New Year</td><td>28 Jan 2017\\n29 Jan 2017 *</td><td>Saturday\\nSunday</td></tr><tr><td>Good Friday</td><td>14 Apr 2017</td><td>Friday</td></tr><tr><td>Labour Day</td><td>1 May 2017</td><td>Monday</td></tr><tr><td>Vesak Day</td><td>10 May 2017</td><td>Wednesday</td></tr><tr><td>Hari Raya Puasa</td><td>25 Jun 2017 *</td><td>Sunday</td></tr></table>',\n",
       "  'page_idx': 27,\n",
       "  'outline': [46, 206, 496, 408]},\n",
       " {'type': 'text',\n",
       "  'text': '* The following Monday will be a public holiday.',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'Academic Services',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'Please refer to the respective websites for information on:',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': '- The Academic Unit System- Curriculum Structure- Examinations- Course Registration- Other Academic Services',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'Undergraduate studentswww.ntu.edu.sg/Students/Undergraduate/AcademicServices/Pages/default.aspx',\n",
       "  'page_idx': 27},\n",
       " {'type': 'text',\n",
       "  'text': 'Graduate studentshttp://www.ntu.edu.sg/sasd/oas/ge/Pages/default.aspx',\n",
       "  'page_idx': 27},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/11935e0a2ceffadfc405802b50fccf6616ccdd1d613c5b46d021dfc030967512.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 28,\n",
       "  'outline': [0, 70, 536, 677]},\n",
       " {'type': 'text',\n",
       "  'text': 'e-learning and Mobile Learning',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"e- learning and mobile learning are all part of NTU's online learning environment.\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'NTULearn is NTU online learning platform that allows students to learn anytime and anywhere. Students can view interactive course content, watch online lecture videos, submit assignments, participate in online discussions and forums, and assess themselves via online tutorial and much more.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'BbLearn under the NTU mobile app is a learning mobile app that gives students on- the- go access to announcements, class roster, group discussions, grades and more on smart mobile devices. Students can also access i- Ngage tablet app to view ebooks, collaborate on online activities, share annotations and perform online quizzes.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"The Learning Solutions and Applications Section in the Centre for IT Services (CITS) manages NTU's e- learning environment. For more information on the different e- learning tools used in NTU, such as Clickers (Student Response System), LAMS (Learning Activity Management System), AcuStudio (Video Recording System), eUreka (Project Management System), Turnitin (Plagarism Detection System), BBConnect (SMS Notification System) and etc please visit http://ntulearn.ntu.edu.sg.\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text', 'text': 'IT Services', 'text_level': 1, 'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'NTU students enjoy state- of- the- art IT infrastructure and services. Each matriculated student is given a lifelong Office 365 EDU account and a network account for accessing to all these vast services:',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'a high- speed wired and wireless campus network, including remote access e- services such as course registration, examination matters and e- billing e- learning and library resources online storage and sharing of personal documents, blogs and web pages live video webcasts of interesting seminars and events on campus',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'the interactive campus map, internal shuttle bus information and virtual tour, via web or mobile platform subscription to NTU mailing lists and e- newsletters',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': \"The department managing NTU's IT and learning environment is the Centre for IT Services (CITS). Please visit www.ntu.edu.sg/cits to learn more.\",\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Computer Ownership Scheme',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Each new student is encouraged to have a personal computer for more effective learning. Students will find that laptops are useful tools for formal lessons as well as coursework anytime and anywhere.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Before you purchase a computer hardware or software, please check out the attractive prices that are offered by the vendors. Please see http://www3. ntu.edu.sg/ CITS2/computerdeals/compcategory.htm.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'To take up a computer loan for purchasing a PC/laptop, which is administered by the Office of Admissions and Financial Aid for full- time undergraduates, please visit the website to know more about the scheme: http:// admissions.ntu.edu.sg/UndergraduateAdmissions/ FinancialAssistance/Pages/PCLoan.aspx.',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Computing Facilities and Learning Spaces',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'Computing facilities and learning spaces are available on campus in schools, libraries and the Halls of Residence. One such state- of- the- art learning space is the Learning Pod @ South Spine. This creative learning space provides:',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text',\n",
       "  'text': 'discussion rooms with LCD screens and glass writing boards an open learning space with comfortable and configurable furniture tables with a flexible power outlet and security bar for laptop users pay- per- use printing service',\n",
       "  'page_idx': 29},\n",
       " {'type': 'text', 'text': 'NTU Libraries', 'text_level': 1, 'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'NTU LibrariesNTU libraries provide students with access to rich information resources, collaborative learning spaces and research assistance. There are 7 libraries, open to every student in the campus.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': '- Art, Design and Media Library (ART-01-03): Print and multimedia collections- Asian Communication Resource Centre (WKWSCI-01-18): Communication and Information Studies collections- Business Library (N2-B2b-07): Business collection and media services- Chinese Library (S3.2-85-01): Chinese collection- Humanities and Social Sciences Library (S4-B3c): Humanities and Social/Sciences collections- Lee Wee Nam Library (NS3-03-01): Engineering and Science collections- Wang Gungwu Library (CHC-02): Overseas Chinese collection',\n",
       "  'page_idx': 30},\n",
       " {'type': 'text',\n",
       "  'text': 'For more information and access to resources, visit www.ntu.edu.sg/library.',\n",
       "  'page_idx': 30},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a084862a43aa2ffe94d808354f652c4d762d81ac2a4f96cd7683a2333b416468.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 30,\n",
       "  'outline': [271, 119, 489, 263]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/6f3fbf52a7628dda3cbfda1f634bbd5c417d085befb5a563aef08b17bfe14dae.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 30,\n",
       "  'outline': [271, 287, 489, 431]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/034011a0c5790deee71dde5ef21a5caa8aa47a8954ab4de6a6a6e9b4938cd9c6.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 30,\n",
       "  'outline': [40, 454, 490, 599]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/5e7adeb733beaca5a27bca7f74b09953aeee178d28c809cc2f531416bcdeca07.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 31,\n",
       "  'outline': [0, 72, 522, 680]},\n",
       " {'type': 'text',\n",
       "  'text': 'Student Organisations',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Sports and Games', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Student OrganisationsCultural performances, sporting events and many recreational pursuits enliven student life throughout the year. With more than 100 student organisations dedicated to diverse interests from astronomy to ballroom dancing to current affairs and community work, there is bound to be one that will take your fancy. Search www.ntu.edu.sg/campuslife/clubs/Pages/Clubssocieties.aspx to know more.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Sports and GamesKeen on sports? Whether you are a recreational or a competitive athlete, there are programmes and activities that will suit you. You are welcome to make use of the sporting and recreational facilities at the Sports and Recreation Centre. Visit www.ntu.edu.sg/has/SnR/Pages/index.aspx to learn more.',\n",
       "  'page_idx': 32},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/fb3a50da6e619a18a50ed432be4264fcb6b3290183054587de63f845672b3bdf.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 32,\n",
       "  'outline': [40, 238, 492, 334]},\n",
       " {'type': 'text',\n",
       "  'text': 'Adventure/Water Sports',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Field Sports', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Racquet Games', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Canoeing Canoe Polo Climbing Dragon Boat Lifeguard Corps Outdoor Adventure Scuba Diving Swimming Wakeboarding Windsurfing Water Polo Yachting',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Cricket Football Rugby Touch Football',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Mind Games', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Contact Bridge International Chess Weiqi',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Martial Arts', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Badminton Squash Tennis Table Tennis',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Other Sports', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text', 'text': 'Ball Games', 'text_level': 1, 'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Basketball Floorball Handball Netball Volleyball Tchoukball',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'Aikido Judo Shitoryu Karate Silat Taekwondo',\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': \"Aquathlon Archery Bowling Cheerleading Fencing Golf Inline Skating Runners' Club Snooker and Pool Sport Shooting Track and Field Ultimate Frisbee\",\n",
       "  'page_idx': 32},\n",
       " {'type': 'text',\n",
       "  'text': 'STUDENT LIFE @ NTU',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'text', 'text': 'Meals on Campus', 'text_level': 1, 'page_idx': 33},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/e13c271483807451130903ec97416dc0a38f496692baa2eab7a1c2158e8deefd.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 33,\n",
       "  'outline': [46, 129, 497, 276]},\n",
       " {'type': 'text',\n",
       "  'text': 'The NTU canteens, cafes and restaurants offer a wide variety of food. A meal in the canteen costs an average of \\\\(\\\\) 4$ but is subject to your food preference. Please visit http://www.ntu.edu.sg/has/FnB/Pages/index.aspx for a list of the canteens, cafes and restaurants.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'text',\n",
       "  'text': 'Postal, Banking and Retail',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 33},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/fe98db861904426d83ff210ef5eac26a32dea4097a99c6966008f2d5b890dfa8.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 33,\n",
       "  'outline': [46, 361, 497, 508]},\n",
       " {'type': 'text',\n",
       "  'text': 'There are various amenities that can be found on- campus. For an updated list of facilities, please visit http://www.ntu.edu.sg/has/Pages/index.aspx.',\n",
       "  'page_idx': 33},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/ef88d547011efc69174c02df51e3ab32606ad4f61a7e2015644280b42ded5f0b.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Type of Facilities</td><td></td><td>Location</td></tr><tr><td rowspan=\"3\">Postal</td><td>SAM \\n(Self-Service Automated Machine)</td><td>North Spine Plaza, Level 2 \\nSouth Spine, Level B3</td></tr><tr><td>7-Eleven @ Hall 2</td><td>Near 7-Eleven, next to Canteen 2</td></tr><tr><td>POPstation</td><td>North Spine Plaza, Level 2 \\nSouth Spine \\nThe Quad</td></tr><tr><td rowspan=\"5\">Bank / ATM (Automated Teller Machine)</td><td>OCBC Bank (NTU Branch)</td><td>North Spine Plaza, Level 1</td></tr><tr><td>OCBC ATM</td><td>North Spine Plaza, Level 1 (near OCBC Bank) \\nSouth Spine, Level B3 \\nNear Canteen 2</td></tr><tr><td>POSB ATM</td><td>North Spine Plaza, Level 2 \\nSouth Spine, Level B3 \\nNear Canteen 2</td></tr><tr><td>State Bank of India ATM</td><td>North Spine Plaza, Level 2</td></tr><tr><td>UOB ATM</td><td>North Spine Plaza, Level 2</td></tr><tr><td rowspan=\"2\">Supermarkets</td><td>Giant Super</td><td>Hall 2</td></tr><tr><td>Prime Supermarket</td><td>North Spine Plaza, Level 1</td></tr><tr><td rowspan=\"2\">Convenience Stores</td><td>7-Eleven Convenience Store</td><td>Hall 2 \\nHall 14, next to canteen</td></tr><tr><td>Buzz</td><td>North Spine Plaza, Level 1</td></tr><tr><td rowspan=\"2\">Bookstores</td><td>Popular Bookstore</td><td>NIE 4-B1-10</td></tr><tr><td>Booklink Pte Ltd</td><td>Blk S4, Level B5</td></tr><tr><td>Travel</td><td>STA Travel</td><td>North Spine Plaza, Level 1</td></tr><tr><td rowspan=\"5\">Lifestyle</td><td>Campus Supplies</td><td>North Spine Plaza, Level 1</td></tr><tr><td>Flower Cottage by Noel</td><td>North Spine Plaza, Level 1</td></tr><tr><td>HerVelvetVase</td><td>North Spine Plaza, Level 1</td></tr><tr><td>Mini Toons &amp;amp; More</td><td>North Spine Plaza, Level 1</td></tr><tr><td>U-shop</td><td>North Spine Plaza, Level 1</td></tr><tr><td>Computer Store</td><td>Eight Flags Computer Systems &amp;amp; Supplies</td><td>South Spine, Level B3, near Canteen B</td></tr><tr><td rowspan=\"2\">Hair Care</td><td>Hair Therapy</td><td>Next to Canteen 2</td></tr><tr><td>K-Cuts</td><td>North Spine Plaza, Level 1</td></tr><tr><td rowspan=\"2\">Self-service Machines</td><td>AXS Machine</td><td>North Spine, Level 2 \\nSouth Spine, Level B3 \\nNear Canteen 2</td></tr><tr><td>Photo-Me (Instant photo taking)</td><td>North Spine, Level 2 \\nSouth Spine, Level B3, Canteen B</td></tr></table>',\n",
       "  'page_idx': 34,\n",
       "  'outline': [40, 104, 490, 646]},\n",
       " {'type': 'text',\n",
       "  'text': 'Work out your budget prior to your arrival in Singapore for a financially well- prepared stay in Singapore. Read through this section to know how you can manage.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Estimated Living Costs',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 35},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/929b73ead4e74c82097fa89f1f49dcca9ba1ba020feb9c07ab7d67402c8957af.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': ['Note: \\\\*Varies with room type and subject to revision #Varies according to lifestyle'],\n",
       "  'table_body': '<table><tr><td>Item</td><td>Estimated Monthly Expenses#</td></tr><tr><td>On-Campus Undergraduate Housing*\\nOn-Campus Graduate Housing*</td><td>Refer to www.ntu.edu.sg/has</td></tr><tr><td>Meals#</td><td>(300 - )400</td></tr><tr><td>Personal Expenses#</td><td>(200 - )400</td></tr><tr><td>Transport for Travel within Singapore</td><td>(100 - )200</td></tr><tr><td>Textbooks and Course Materials</td><td>(300 - )500 per academic year</td></tr></table>',\n",
       "  'page_idx': 35,\n",
       "  'outline': [47, 167, 496, 273]},\n",
       " {'type': 'text',\n",
       "  'text': 'Financial Assistance',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Need financial help? Loans are available for students in need. Learn more about financial assistance schemes from the information provided in the admissions package or from http://www.ntu.edu.sg/Freshmen/FreshmenGuide/FinancialMatters/Pages/FinancialAssistanceSchemes.aspx.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Part-Time Employment',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'The SAO- Student Support does not recommend that international students rely on part- time work as a source of funds. Doing so is very risky as part- time work may not be available when needed nor will it provide sufficient funds to sustain your stay.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'International students on exchange or in non- graduating programmes may not engage in any part- time work in Singapore. For more information, please visit http://www.ntu.edu.sg/SAO/studentsupport/ManagingFinances/Pages/ParttimeEmployment.aspx',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Full- time undergraduate and graduate students who wish to take up part- time work for the experience should note the following:',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Part-Time Jobs outside Campus',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '- Prior approval from the university is required for matriculated international students to work part-time. Students should contact the SAO-Student Support. A fresh application is required for each job. Scholarship holders require the additional approval of their sponsors.- Full-time matriculated international undergraduates and graduate students may work a maximum of 16 hours a week during term time and full-time during the vacation.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': 'Part-Time Jobs on Campus under Work Study Scheme (undergraduates only)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 35},\n",
       " {'type': 'text',\n",
       "  'text': '- Undergraduates working part-time on campus may work a maximum of 14 hours a week during term time and nine hours a day or 44 hours a week during the vacation. Prior approval from the university is not required.',\n",
       "  'page_idx': 35},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/3aa097a4850d5641e86c69acf30aa626b22014377aea4f67506b19330aeadc06.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 36,\n",
       "  'outline': [0, 70, 536, 680]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/ee40110a4b78c5482aa55876d97d00e77a863e17412cc9b1cf5b7a1dcf884bbe.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 37,\n",
       "  'outline': [46, 106, 496, 397]},\n",
       " {'type': 'text',\n",
       "  'text': \"Singapore, officially the Republic of Singapore, is a Southeast Asian city- state off the southern tip of the Malay Peninsula, 137 kilometres north of the equator. It is an island country made up of 63 islands, separated from Malaysia by the Straits of Johor to its north and from Indonesia's Riau Islands by the Singapore Strait to its south.\",\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'She has a land area of about 710 square kilometres, making her one of the smallest countries in the world and the smallest in the region - hence the moniker \"The Little Red Dot\". Although small in size, Singapore commands an enormous presence in the world today with its free trade economy and highly efficient workforce. With her strategic location in the region, it has enabled her to become a central sea port along major shipping routes.',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text', 'text': '', 'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'English is the main language of instruction. The mother tongue is used widely for each major ethnicity. There are 4 major races - namely, Chinese, Malay, Indian and Eurasian. Each community offers a different perspective of life in Singapore in terms of culture, religion, food and language.',\n",
       "  'page_idx': 37},\n",
       " {'type': 'text',\n",
       "  'text': 'Experience Singapore',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': \"Experience SingaporeBeing a multi- racial, multi- ethnic and multi- religious society, Singapore is as diverse as it is cohesive. Beyond the history, culture, people, shopping and food, there are many more facets to Singapore's thriving cityscape for you to discover. These can only be experienced as you immerse yourself in the exploration of this once fishing village turned cosmopolitan city. To read more about Singapore, visit www.yoursingapore.com.\",\n",
       "  'page_idx': 38},\n",
       " {'type': 'text', 'text': 'Old World Charm', 'text_level': 1, 'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': \"Old World CharmIf you are interested to discover the old world charm, you can explore and experience the island's key historical landmarks or memorials. You can embark on a heritage trail to enjoy the sights and sounds at various cultural precincts, such as,\",\n",
       "  'page_idx': 38},\n",
       " {'type': 'text', 'text': 'Chinatown', 'text_level': 1, 'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'ChinatownA visit to Chinatown promises fascinating encounters with local customs and lifestyles.',\n",
       "  'page_idx': 38},\n",
       " {'type': 'text', 'text': 'Helix Bridge', 'text_level': 1, 'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'Helix BridgeWalk across this architectural and engineering marvel and admire the Singapore skyline.',\n",
       "  'page_idx': 38},\n",
       " {'type': 'text', 'text': 'Kampong Glam', 'text_level': 1, 'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'Kampong GlamExplore the alleys of Kampong Glam for exotic treats and buys.',\n",
       "  'page_idx': 38},\n",
       " {'type': 'text', 'text': 'Little India', 'text_level': 1, 'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'Little IndiaLittle India fascinates with shops selling everything from spices to flower garlands.',\n",
       "  'page_idx': 38},\n",
       " {'type': 'text', 'text': 'Modern City', 'text_level': 1, 'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'Modern CityIf you prefer the bright city lights and being amidst the hustle and bustle, then you will be delighted to know that there are numerous shopping malls, museums, and dining and entertainment hotspots to choose from.',\n",
       "  'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'Orchard Road & Marina Bay Sands',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': \"Spend a day in Orchard Road and Marina Bay Sands. You will understand why Singapore is dubbed a shopper's paradise.\",\n",
       "  'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': 'Clarke Quay/Boat Quay & Marina Bay',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 38},\n",
       " {'type': 'text',\n",
       "  'text': \"Experience Singapore nightlife activities at the club in these locations. You can also find Avalon, one of Hollywood's hottest clubs at Marina Bay.\",\n",
       "  'page_idx': 38},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/41e272b6bb28706c62ac5f25c76608069d5f2266ab97eece638bcecf1ec946df.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 38,\n",
       "  'outline': [41, 466, 146, 555]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cde7a63e9562aecc83ea341124732a595a06ee53a457008c5939c04c76021e36.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 38,\n",
       "  'outline': [153, 466, 259, 555]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/20857fc7a0256bc4c9f3207c9267e00989b00a95ee687b74168d8c0087011404.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 38,\n",
       "  'outline': [267, 466, 374, 556]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/d1e4c078c5e08e464f685f5058fd00506e0d858cf38c3b6c1925edb11e7c5aae.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 38,\n",
       "  'outline': [270, 561, 491, 652]},\n",
       " {'type': 'text', 'text': 'Food', 'text_level': 1, 'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': \"The other thing that will strike you most about Singapore is its multifarious offering of food regardless day or night. With a range of dining options from Peranakan to Chinese, Indian to Malay, fusion and more, you'll be spoilt for choice.\",\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Chinatown Complex Market',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Blk 335 Smith Street S050335 Located in the heartland of Chinatown. Have a meal with the locals and explore the shops of Chinatown.',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Chomp Chomp Food Centre',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Chomp Chomp Food Centre20 Kensington Park Road S557269Located at Serangoon Garden, this hawker has a wide variety of food at a reasonable price.',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Golden Mile Food Centre 505 Beach Road S199583 Popular for its wide array of local fare',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Holland Village Market and Food Centre 1 Lorong Mambong S277700 Holland Village is popular for its al fresco cafe culture',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Maxwell Road Hawker Centre',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': '1 Kadayanallur Street S069184 This centrally- located food centre is packed to the brim during peak hours.',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Old Airport Road Food Centre Blk 51 Old Airport Road S390051',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text', 'text': 'Tekka Centre', 'text_level': 1, 'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Tekka CentreBlk 665 Buffalo Road S210665Located in the heart of Little India',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Clementi Market and Food Centre',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Blk 353 Clementi Avenue 2 #01- 108 S120353 Surrounded by HDB shops that offer household items at a bargain price',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'East Coast Lagoon Food Village',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': '1220 East Coast Parkway S468960Tuck into scrumptious local fare after cycling or rollerblading at East Coast Park',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Geylang Serai Market and Food Centre',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': '1 Geylang Serai S402001Sample traditional Malay fare at this market and the eateries in Geylang Serai',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Lau Pat Sat Festival Market',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': '18 Raffles Quay S048582Largest remaining Victorian filigree cast- iron structure in Southeast Asia built in 1894 and is now a food centre offering a wide variety of local food',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Newton Circus Food Centre 500 Clemenceau Avenue North S229495Popular with tourists and locals',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': \"People's Park Cooked Food Centre\",\n",
       "  'text_level': 1,\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Blk 32 New Market Road S050032A good destination for traditional local fare after shopping in Chinatown',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text',\n",
       "  'text': 'Tiong Bahru Market and Food Centre 30 Seng Poh Road S168898Popular with locals for its wide array of dishes',\n",
       "  'page_idx': 39},\n",
       " {'type': 'text', 'text': 'Arts and Culture', 'text_level': 1, 'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'Singapore thrives in arts and cultural events. Visit these places to understand and experience more.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'Esplanade -Theatres on the Bay',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': '1 Esplanade Drive S038981 Engage in the arts at this distinctive landmark.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'National Museum of Singapore',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': '93 Stamford Road S178897 Visit its Singapore History and Living Galleries to learn about Singapore culture and also its 11 National Treasures.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'ArtScience Museum at Marina Bay Sands 6 Bayfront Avenue S018974',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'Chinatown Heritage Centre 48 Pagoda Street S059207',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'Peranakan Museum 39 Armenian Street S179941',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'Community and Sporting Activities',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': \"The People's Association promotes social cohesion in Singapore through its network of grassroots organisations and community clubs. Visit www.pa.gov.sg to learn about the activities of the community club near you. For Singapore's sporting calendar, please visit www.ssc.gov.sg.\",\n",
       "  'page_idx': 40},\n",
       " {'type': 'text', 'text': 'Events', 'text_level': 1, 'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'Besides major holidays, you can look forward to a vibrant calendar of events in Singapore. These are some of the events that are held in Singapore.',\n",
       "  'page_idx': 40},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/5e1b64c7ba7876d8c5a144522781addf5f2f7e3bd31bd55e435845a8a3ad3147.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 40,\n",
       "  'outline': [38, 507, 145, 579]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/433017db987607fb24c853f5e8457cbe1cd8872223fede1157f4b3a2cb9cfcfa.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 40,\n",
       "  'outline': [151, 507, 259, 578]},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cdc9c21355d1ca7fb2c54ad78efe2b01941c4ae259ab5ca80fedc203317e29e2.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 40,\n",
       "  'outline': [269, 507, 490, 579]},\n",
       " {'type': 'text',\n",
       "  'text': 'Chingay Parade Singapore Dragon Boat Festival Fashion Steps Out  $$  Orchard F1 Singapore Grand Prix Great Singapore Sale Mosaic Music Festival',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text',\n",
       "  'text': 'National Day Parade and celebrations Singapore International Arts Festival Singapore Food Festival World Gourmet Summit ZoukOut',\n",
       "  'page_idx': 40},\n",
       " {'type': 'text', 'text': 'Nature', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'National Parks (NParks) oversees the development of Singapore as a Garden City. Visit www.nparks.gov.sg for information on the parks and nature reserves in Singapore as well as information on free nature appreciation walks.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Bukit Timah Nature Reserve',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': \"Discover the rich flora and fauna of this reserve as you climb Bukit Timah - Singapore's highest hill.\",\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Gardens By The Bay',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Spanning 101 hectare, it houses over a quarter of a million rare plants in huge domed conservatories.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'MacRitchie Reservoir Park',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Explore its scenic and tranquil nature trails and go on a treetop walk.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Labrador Nature Reserve',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'An oasis of tranquillity and natural wonder. A staircase built on the edge of the secondary forest offers a prime view of the cliff side vegetation coupled with a panoramic view of the sea.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Jurong Central Park',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'It is the first in Singapore to have life- sized board- game features. It is situated across Boon Lay MRT Station.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Mount Faber Park', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'From this park, you can take a cable car to Sentosa Island. A mural wall depicting scenes of local history can be seen at Upper Faber Point, the highest point in the park.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Youth Olympic Park',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'The Youth Olympic Park is named after the inaugural Youth Olympic Games held in Singapore in August 2010. A broadwalk connects the Park to Marina Promenade.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'East Coast Park', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'A well- loved destination of Singaporeans for barbecues, cycling, rollerblading and water sports.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Jurong Bird Park', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'The largest bird park and a haven for 8,000 birds representing species. Visit www.birdpark.com.sg for details.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Night Safari', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'As dusk falls, observe over 1,000 nocturnal animals upclose.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Istana', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Fronting the main Istana Gate, Istana Park provides a vantage point for people keen to view the monthly changing of guards ceremony at the Istana.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Marina Barrage', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Catch the grandeur of the Singapore Flyer and the Central Business District skyline at this first reservoir in the heart of the city.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Hortpark', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'It is a 9- hectare park which is the first one- stop gardening lifestyle hub in Asia. The main highlights are the Forest Walk and Canopy Walk.',\n",
       "  'page_idx': 41},\n",
       " {'type': 'text', 'text': 'Sentosa', 'text_level': 1, 'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': \"Fancy a day island hopping? Sentosa has a well- mixed of activities ranging from the thrilling rides at Universal Studio and surf at 10- foot FlowBarrel wave, to learn about Singapore's past at several historical landmarks. Visit www.sentosa.com.sg to read more.\",\n",
       "  'page_idx': 41},\n",
       " {'type': 'text',\n",
       "  'text': 'Travelling within Singapore',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'Travelling within Singapore is a breeze with its advanced transportation system.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text', 'text': 'Taxi', 'text_level': 1, 'page_idx': 42},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/38b2a191491d044bdb9907d0600ab6a9236d9f6d78a8983e6bd55520f77905bb.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 42,\n",
       "  'outline': [41, 186, 258, 329]},\n",
       " {'type': 'text',\n",
       "  'text': 'Taxis can be flagged down at any time of the day at a taxi stand and along any public road outside of the Central Business District (CBD). Alternatively you can book a taxi with any of the taxi company via phone call where a booking fee is applicable. To know the booking amount, please check with the individual taxi company.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'All taxis in Singapore are metered. The fares must be charged according to the taxi meter based on the flag down rate, distance travelled and applicable surcharge (such as midnight surcharge, peak hour surcharge, location surcharge and Electronic Road Pricing charges). Please take note that each taxi company imposes different surcharge and flag down rate. You may need to check with the driver or taxi company on the surcharge before boarding the taxi. Should you wish to retain a receipt, you need to request for it at the end of the trip. Payment is usually by cash although some taxi companies do accept credit cards/NETs payment.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'This is a list of phone booking hotlines for the respective taxi companies.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/3f8540a356e1e7980b6fb03d68af6469c0a59145985dc30dd4fd694b01c3c80f.jpg',\n",
       "  'table_caption': [],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '<table><tr><td>Common Taxi Booking Number</td><td>(+65 6-DIAL-CAB) +65 6342-5222</td></tr><tr><td>Comfort &amp;amp; CityCab</td><td>+65 6552-1111</td></tr><tr><td>Premier Taxi</td><td>+65 6363-6888</td></tr><tr><td>Smart Cab</td><td>+65 6485-7777</td></tr><tr><td>SMRT Taxi</td><td>+65 6555-8888</td></tr><tr><td>Trans-Cab Services</td><td>+65 6555-3333</td></tr><tr><td>Prime Taxi</td><td>+65 6778-0808</td></tr></table>',\n",
       "  'page_idx': 42,\n",
       "  'outline': [272, 213, 491, 344]},\n",
       " {'type': 'text', 'text': 'Bus', 'text_level': 1, 'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'Public buses operate from 5.30am to midnight daily and the frequencies range from 5 minutes to 20 minutes. The information panels at the bus stops provide useful route and fare information for the bus services available at that stop.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'The buses pick up and drop off passengers only at designated bus- stops. You need to flag down the bus you wish to board at the bus- stop. For all bus trips, you can pay using exact cash or ez- link card, please refer to Contactless Smart (CEPAS- Compliant) Cards section in this guide. For more information, please visit www.sbstransit.com.sg or www.smrt.com.sg.',\n",
       "  'page_idx': 42},\n",
       " {'type': 'text',\n",
       "  'text': 'Mass Rapid Transit (MRT) & Light Rapid Transit (LRT)',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Commuters in Singapore enjoy a comprehensive public transport network similar to other developed cities. There are also LRT systems that link MRT stations within the HDB housing estates.Both MRT and LRT operate from 5.30am to midnight daily.The first and last train departure times vary between stations, as well as weekends and Public Holidays.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'For fare information, you may wish to use the easyto- use maps at the MRT ticketing machines. Please visit www.transitlink.com.sg or www.publictransport.sg should you wish to know more information on public transport fares and MRT map.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text', 'text': 'Queue System', 'text_level': 1, 'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'It is an unspoken rule that commuters must queue at bus- stops, taxi stands and train stations to board and alight from buses, taxis and trains. This applies to service and retail outlets throughout Singapore as well.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Contactless Smart (CEPAS-Compliant) Cards',\n",
       "  'text_level': 1,\n",
       "  'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'To ease the need of carrying sufficient cash for travelling, you may wish to purchase an ez- link card or a NETS FlashPay card. These are contactless tap- andgo Smart cards for public transit use on the MRT, LRT and buses.You can get them from the network of sales points within most MRT stations and 7- Eleven stores. Please visit www.ezlink.com.sg or www.nets.com.sg for more information on the Smart cards.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a20842d2eab190b1a8ca667f038dc4345dda2d4255535002c36ce2b98183aa9e.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 43,\n",
       "  'outline': [277, 106, 496, 296]},\n",
       " {'type': 'text', 'text': 'Changi Airport', 'text_level': 1, 'page_idx': 43},\n",
       " {'type': 'text',\n",
       "  'text': 'Changi Airport is the main airport in Singapore and a major aviation hub in Southeast Asia. Please visit www. changiairport.com to view the Airport interactive map and access the guide for arriving passengers.',\n",
       "  'page_idx': 43},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/35f754eb8062e1dac5c6c8e4c89f4538d8a6ba8e24f5f0b0ce4bdf5f11fda13d.jpg',\n",
       "  'image_caption': [],\n",
       "  'image_footnote': [],\n",
       "  'page_idx': 43,\n",
       "  'outline': [277, 402, 494, 524]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb687bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m         temp_file.close()\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m temp_file.name\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontent_list\u001b[49m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     32\u001b[39m         \u001b[38;5;28mprint\u001b[39m(i[\u001b[33m'\u001b[39m\u001b[33mpage_idx\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'content_list' is not defined"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "import tempfile\n",
    "def get_full_page_image_from_pdf(pdf_path: str, page_num: int) -> str:\n",
    "    \"\"\"\n",
    "    PDF\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: PDF\n",
    "        page_num: 0\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    with pymupdf.open(pdf_path) as pdf:\n",
    "        if page_num >= len(pdf):\n",
    "            raise ValueError(f\"Page {page_num} does not exist in PDF\")\n",
    "        \n",
    "        page = pdf[page_num]\n",
    "        \n",
    "        # \n",
    "        pix = page.get_pixmap(dpi=150)\n",
    "        \n",
    "        # \n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "        pix.save(temp_file.name)\n",
    "        temp_file.close()\n",
    "        \n",
    "        return temp_file.name\n",
    "\n",
    "for i in content_list:\n",
    "    if i['type'] == 'image':\n",
    "        print(i['page_idx'])\n",
    "        temp_image_path = get_full_page_image_from_pdf('/data/users/yiming/dtagent/MMLongBench/documents/SAO-StudentSupport_Guidebook-Content.pdf', i['page_idx'])\n",
    "        print(temp_image_path)\n",
    "        break\n",
    "\n",
    "from typing import List\n",
    "def save_cropped_image(pdf_path: str, page_num: int, outline: List[float]) -> str:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        pdf_path: PDF\n",
    "        page_num: 0\n",
    "        outline:  [x1, y1, x2, y2]\n",
    "        doc_name: \n",
    "        element_index: \n",
    "        base_output_dir: dom_output_path\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    with pymupdf.open(pdf_path) as pdf:\n",
    "        if page_num >= len(pdf):\n",
    "            raise ValueError(f\"Page {page_num} does not exist in PDF\")\n",
    "        \n",
    "        page = pdf[page_num]\n",
    "        # \n",
    "        rect = pymupdf.Rect(outline[0], outline[1], outline[2], outline[3])\n",
    "        \n",
    "        # \n",
    "        pix = page.get_pixmap(clip=rect, dpi=150)\n",
    "    \n",
    "    return pix\n",
    "\n",
    "import fitz  # pymupdf\n",
    "from IPython.display import Image, display\n",
    "from io import BytesIO\n",
    "\n",
    "for i in content_list:\n",
    "    if i['type'] == 'image':\n",
    "        print(i['page_idx'])\n",
    "        # temp_image_path = get_full_page_image_from_pdf('/data/users/yiming/dtagent/MMLongBench/documents/SAO-StudentSupport_Guidebook-Content.pdf', i['page_idx'])\n",
    "        pix = save_cropped_image('/data/users/yiming/dtagent/MMLongBench/documents/SAO-StudentSupport_Guidebook-Content.pdf', i['page_idx'],i['outline'])\n",
    "        #  PNG \n",
    "        img_bytes = BytesIO(pix.tobytes(\"png\"))\n",
    "\n",
    "        # \n",
    "        # display(Image(data=img_bytes.getvalue()))\n",
    "\n",
    "# with open('/data/users/yiming/dtagent/tmp/MMLongBench/tree_chatdoc/SAO-StudentSupport_Guidebook-Content.json', 'r', encoding='utf-8') as f:\n",
    "#     chatdoc = json.load(f)\n",
    "\n",
    "# for i in chatdoc['data']['elements']:\n",
    "#     if i['type'] == 'figure':\n",
    "#         print(i['page'])\n",
    "#         pix = save_cropped_image('/data/users/yiming/dtagent/MMLongBench/documents/SAO-StudentSupport_Guidebook-Content.pdf', i['page'],i['outline'])\n",
    "#         img_bytes = BytesIO(pix.tobytes(\"png\"))\n",
    "\n",
    "#         # \n",
    "#         display(Image(data=img_bytes.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69659b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eda02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14725682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
